{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLDisco MC_Maze pipeline\n",
    "\n",
    "This code uses the **NLDisco** (**N**eural **L**atent **Disco**very) pipeline on MC_Maze data.\n",
    "\n",
    "**Goal:** Discover interpretable latents (i.e., *features*) in high-dimensional neural data.\n",
    "\n",
    "**Terminology:**\n",
    "\n",
    "- *Neural / Neuronal:* Refers to biological neurons. Distinguished from *model neurons* (see below).\n",
    "- *Units:* Putative biological neurons -- the output from spikesorting extracellular electrophysiological data.\n",
    "- *Model neurons:* Neurons in a neural network model (aka *latents*)\n",
    "- *Features:* Interpretable latents (latent dimensions that align with meaningful behavioral or environmental variables)\n",
    "\n",
    "![](./figures/sae.svg)\n",
    "\n",
    "*(If the image above does not render, see ./figures/sae.svg)*\n",
    "\n",
    "Motivated by successful applications of sparse dictionary learning in AI mechanistic interpretability, NLDisco trains overcomplete sparse encoder-decoder (SED) models to reconstruct neural activity based on a set of sparsely active dictionary elements (i.e. latents), implemented as hidden layer neurons. In the figure above, this is illustrated as reconstructing target neural activity $z$ from input neural activity $y$ via $d$. Sparsity in the latent space encourages a monosemantic dictionary, where each hidden layer neuron corresponds to a single neural representation that can be judged for interpretability, making SEDs a simple but effective tool for neural latent discovery. \n",
    "\n",
    "These SEDs can be configured as autoencoders (SAEs) if the target for $z$ is $y$ (e.g. M1 activity based on M1 activity), or as transcoders if the target for $z$ is dependent on or related to $y$ (e.g. M1 activity based on M2 activity, or M1 activity on day 2 based on M1 activity on day 1). In this tutorial, we will work exclusively with SAEs.\n",
    "\n",
    "We consider a latent’s interpretability in two key aspects:\n",
    "\n",
    "1. its correspondence to a specific external variable – a \"natural\" behavioral or environmental feature\n",
    "\n",
    "2. its explicit composition from contributing neural activity\n",
    "\n",
    "**NLDisco pipeline:**\n",
    "\n",
    "1. Load and prepare data\n",
    "    - Neural data in the form of $[time \\times space]$, and in this tutorial specifically as binned spike counts of $[examples \\times units]$\n",
    "\n",
    "2. Train models \n",
    "    - Train models to reconstruct the neural data (in this tutorial, from the Churchland MC Maze dataset)\n",
    "    - Validate the quality of the models by looking at the sparsity of the latent activations and reconstruction quality of the neural data\n",
    "\n",
    "3. Save or load the model activations\n",
    "\n",
    "4. Decode a behavioural variable (in this case hand velocity)\n",
    "    - Using NLDisco latents\n",
    "    - Using CEBRA embeddings, as a comparison\n",
    "\n",
    "5. Find features\n",
    "    - Automatically generate promising mappings between model neurons (latents) and behavioral and/or environmental data\n",
    "    - Find meaningful features and their top contributing units using an interactive dashboard\n",
    "        - The mappings are a starting point to guide the search\n",
    "        - A user can also choose to look through model neurons manually\n",
    "\n",
    "6. Make paper plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Environment setup:**\n",
    "\n",
    "Prerequisite: an installed version of [pixi](https://pixi.sh/latest/)\n",
    "\n",
    "Steps:\n",
    "1. In the repo's root directory, run `pixi install --manifest-path ./pyproject.toml`. This will create an environment in a newly created `.pixi/envs` folder.\n",
    "2. Run `pixi run postinstall`\n",
    "\n",
    "**Data download:**\n",
    "\n",
    "Once your environment is set up, use it as a kernel for this notebook and run the cell below to automatically download and preprocess the [churchland_shenoy_neural_2012 dataset](https://brainsets.readthedocs.io/en/latest/glossary/brainsets.html#churchland-shenoy-neural-2012) (also known as MC_Maze). With the data downloaded, you can jump straight into the tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nldisco import mc_maze\n",
    "\n",
    "# # Directories for raw and processed data\n",
    "# raw_data_dir = \"../data/raw\"\n",
    "# processed_data_dir = \"../data/processed\"\n",
    "\n",
    "# # Subject name and number of their sessions to download\n",
    "# # Max 4 for jenkins and 3 for nitschke\n",
    "# # Be aware that these files are large (~2-7GB each)\n",
    "# subject_name = \"nitschke\"  # \"jenkins\" or \"nitschke\"\n",
    "# num_files = 1  \n",
    "\n",
    "# mc_maze.download_and_preprocess(raw_data_dir, processed_data_dir, subject_name, num_files=num_files)\n",
    "\n",
    "# # To load all the data, use:\n",
    "# # mc_maze.download_and_preprocess(raw_data_dir, processed_data_dir, \"jenkins\", num_files=4)\n",
    "# # mc_maze.download_and_preprocess(raw_data_dir, processed_data_dir, \"nitschke\", num_files=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the churchland_shenoy_neural_2012 dataset, the subjects are performing a center-out reaching task on a variety of different maze configurations. Each maze configuration comes in 3 versions:\n",
    "- 1 target and no barriers.\n",
    "- 1 target with barriers.\n",
    "- 3 targets, with barriers. But 2 of the targets are distractors and unaccessible given the barrier configuration.\n",
    "\n",
    "So maze conditions 1, 2, 3 are related; as are maze conditions 4, 5, 6; and so on.\n",
    "Neural activity was recorded from the dorsal premotor (PMd) and primary motor (M1) cortices. A variety of other data (monkey hand position, velocity and acceleration, gaze position...) is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set notebook settings.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import packages.\"\"\"\n",
    "\n",
    "# Standard library\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# IPython/Jupyter\n",
    "from IPython.display import display\n",
    "\n",
    "# Third-party\n",
    "import cebra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch as t\n",
    "from einops import asnumpy, reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Local project modules\n",
    "from nldisco import train as nt\n",
    "from nldisco import mc_maze, decode, pipeline, cebra_utils\n",
    "from nldisco.train_val_split import train_val_split_by_proportion, train_val_split_by_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare spike data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load session data and bin spikes.\"\"\"\n",
    "\n",
    "data_path = Path(r\"../data/processed\")\n",
    "subject_name = \"nitschke\"  # \"jenkins\" or \"nitschke\"\n",
    "sessions = mc_maze.load_sessions(data_path, subject_name)\n",
    "\n",
    "bin_size = 0.05  # in seconds\n",
    "spikes_df = mc_maze.bin_spike_data(sessions, bin_size)  # this can take several minutes\n",
    "\n",
    "display(spikes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Quick plots and stats to get a sense of the spike data.\"\"\"\n",
    "\n",
    "print(\"Firing rates distribution:\")\n",
    "# Compute mean firing rate (Hz) per unit\n",
    "duration_sec = len(spikes_df) * bin_size\n",
    "mean_firing_rates = spikes_df.sum(axis=0) / duration_sec  # spikes_arr/sec\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(mean_firing_rates, bins=30, edgecolor='black')\n",
    "plt.xlabel('Mean Firing Rate (Hz)')\n",
    "plt.ylabel('Number of Units')\n",
    "plt.title('Distribution of Mean Firing Rates per Unit')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print stats\n",
    "print(\"Mean: {:.2f} Hz\".format(mean_firing_rates.mean()))\n",
    "print(\"Range: {:.2f}–{:.2f} Hz\".format(mean_firing_rates.min(), mean_firing_rates.max()))\n",
    "\n",
    "print(\"\\n\\nSpike count distribution and sparsity stats:\")\n",
    "# Flatten spike counts\n",
    "flattened_spike_counts = spikes_df.values.flatten()\n",
    "# Define bins that align exactly to integer spike counts\n",
    "max_count = flattened_spike_counts.max()\n",
    "bins = np.arange(0, max_count + 2) - 0.5  # centers bins on integers\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(flattened_spike_counts, bins=bins, edgecolor='black')\n",
    "plt.title(\"Distribution of Spike Counts per Unit per Time Bin\")\n",
    "plt.xlabel(\"Spike Count\")\n",
    "plt.ylabel(\"Number of Unit-Bin Combinations\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print stats\n",
    "frac_nonzero_bins = (spikes_df != 0).values.sum() / spikes_df.size\n",
    "frac_nonzero_examples = (spikes_df.sum(axis=1) > 0).mean()\n",
    "print(f\"Fraction of non-zero bins: {frac_nonzero_bins:.4f}\")\n",
    "print(f\"Fraction of non-zero examples: {frac_nonzero_examples:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare environment / behavior (meta)data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load and bin (meta)data to match spike bins.\"\"\"\n",
    "\n",
    "# Retrieve and collate metadata (hand/eye/events) across sessions\n",
    "metadata, trials_df = mc_maze.retrieve_metadata(sessions) # this can take a minute\n",
    "# Bin metadata to the match the binned spikes_df\n",
    "metadata_binned = mc_maze.bin_metadata(metadata, trials_df, bin_size, spikes_df.index)\n",
    "\n",
    "print(\"Metadata:\")\n",
    "display(metadata)\n",
    "print(\"Binned metadata:\")\n",
    "display(metadata_binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train/val split, smooth and normalize spikes.\"\"\"\n",
    "\n",
    "split_by_session = False # if False, will split by proportion\n",
    "\n",
    "if split_by_session:\n",
    "    train_trials, val_trials = train_val_split_by_session(\n",
    "        metadata_binned[\"trial_idx\"].to_numpy(),\n",
    "        metadata_binned[\"session\"].to_numpy(),\n",
    "        train_sessions=[1, 2],  # pick your training sessions\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "    )\n",
    "else:\n",
    "    train_trials, val_trials = train_val_split_by_proportion(\n",
    "        metadata_binned[\"trial_idx\"].values,\n",
    "        train_proportion=0.8,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "    )\n",
    "\n",
    "# Create boolean masks to split metadata and spikes into train/val sets\n",
    "train_mask = metadata_binned['trial_idx'].isin(train_trials)\n",
    "val_mask = metadata_binned['trial_idx'].isin(val_trials)\n",
    "\n",
    "# Split metadata\n",
    "metadata_binned_train = metadata_binned[train_mask].reset_index(drop=True)\n",
    "metadata_binned_val = metadata_binned[val_mask].reset_index(drop=True)\n",
    "\n",
    "# Split spikes \n",
    "spikes_arr = spikes_df.values.astype(np.float32)\n",
    "spikes_train_arr = spikes_arr[train_mask]\n",
    "spikes_val_arr = spikes_arr[val_mask]\n",
    "\n",
    "# Smooth spikes\n",
    "sigma = 0.05 / bin_size\n",
    "spikes_train_arr = gaussian_filter1d(spikes_train_arr, sigma=sigma, axis=0)\n",
    "spikes_val_arr = gaussian_filter1d(spikes_val_arr, sigma=sigma, axis=0)\n",
    "\n",
    "# Normalize spikes (fit normalization on training data only)\n",
    "train_max = spikes_train_arr.max()\n",
    "spikes_train_arr = spikes_train_arr / train_max\n",
    "spikes_val_arr = spikes_val_arr / train_max\n",
    "\n",
    "# Summary\n",
    "print(f\"Train set: {len(train_trials)} trials ({train_mask.sum()} time bins)\")\n",
    "print(f\"Val set: {len(val_trials)} trials ({val_mask.sum()} time bins)\")\n",
    "print(f\"Spike data shapes: train {spikes_train_arr.shape}, val {spikes_val_arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert spike data to torch tensors and move to torch device.\"\"\"\n",
    "\n",
    "# it's best to have a gpu for training!\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device=}\")\n",
    "\n",
    "spikes_train = t.from_numpy(spikes_train_arr).to(device).to(dtype=t.bfloat16)\n",
    "spikes_val = t.from_numpy(spikes_val_arr).to(device).to(dtype=t.bfloat16)\n",
    "\n",
    "display(spikes_train)\n",
    "display(spikes_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train models\n",
    "\n",
    "> If desired, you can choose to skip this section and load pre-saved SAE activations instead. A set of activations per subject (Jenkins and Nitschke) is provided - go to section \"3. Save/load SAE activations\" for more information. It is however still highly recommended to read the rest of this section to understand how the SAEs are trained.\n",
    "\n",
    "This code trains 2 SAEs with identical setups so that you can compare the different instances and ensure they both give similar results. For each time bin of neural data in the train and val sets, the SAEs' latent activations are calculated (i.e., the output values of their hidden layer neurons), and it is these activations that will be used to find a latent's correspondence with external variables (features). A particularity of the NLDisco pipeline below is that it trains *Matryoshka* SAEs.\n",
    "\n",
    "**Matryoshka architecture:**\n",
    "\n",
    "The Matryoshka architecture segments the latent space into multiple levels, each of which attempts a full reconstruction of the target neural activity. In the figure below, black boxes indicate the latents (model neurons) involved in a given level, while light-red boxes indicate additional latents recruited at lower levels. A top-$k$ selection is used to choose which latents to recruit for reconstruction at each level (yellow neuron within each light-red box -- $k=1$ for each level in this example).\n",
    "\n",
    "This nested arrangement is motivated by the idea that multi-scale feature learning can mitigate “feature absorption” (a common issue where a more specific feature subsumes a portion of a more general feature), allowing both coarse and detailed representations to emerge simultaneously.\n",
    "\n",
    "- Latents in the highest level ($L_1$) typically correspond to broad, high-level features (e.g., a round object), \n",
    "- Latents exclusive to the lowest level ($L_3$) often correspond to more specific, fine-grained features (e.g., a basketball)\n",
    "\n",
    "![](./figures/msae.svg)\n",
    "\n",
    "*(If the image above does not render, see ./figures/msae.svg)*\n",
    "\n",
    "**Key training parameters to play with:**\n",
    "\n",
    "`SedConfig` (model-level):\n",
    "- `dsed_topk_map`: how many top-k model neurons are kept active at each level (controls sparsity per level)\n",
    "- `dsed_loss_x_map`: relative weight of each Matryoshka level to the overall reconstruction loss\n",
    "\n",
    "`optimize()` (optimizer-level):\n",
    "- `n_steps`: total training steps\n",
    "- `batch_sz`: how many examples per batch\n",
    "- `lr` (set in optimizer): learning rate used by whatever optimizer you choose (e.g. Adam below) and you can optionally use a scheduler (`use_lr_sched=True`)\n",
    "- `dead_latent_window`: number of steps a latent (model neuron) can stay inactive before being flagged as “dead”  \n",
    "  - Dead latents are revived with an auxiliary loss: instead of reconstructing the full input, they try to reconstruct only the residual error (the part the active neurons failed to capture)\n",
    "  - This gives inactive latents a chance to become useful again, preventing them from staying permanently silent   \n",
    "- `loss_fn` – reconstruction objective: built-ins are mse and msle, or you can pass a custom callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set config.\"\"\"\n",
    "\n",
    "# total of 1024 model neurons in 3 nested levels: 0-256, 0-512 and 0-1024\n",
    "dsae_topk_map = {256: 8, 512: 16, 1024: 24}\n",
    "dsae_topk_map = dict(sorted(dsae_topk_map.items()))  # ensure sorted from smallest to largest\n",
    "dsae_loss_x_map = {256: 1, 512: 1.25, 1024: 1.5}\n",
    "dsae_loss_x_map = dict(sorted(dsae_loss_x_map.items()))\n",
    "dsae = max(dsae_topk_map.keys())\n",
    "n_inst = 2  # number of SAE instances to train in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train model.\"\"\"\n",
    "\n",
    "sae_cfg = nt.SedConfig(\n",
    "    n_input=spikes_train.shape[1],\n",
    "    dsed_topk_map=dsae_topk_map,\n",
    "    dsed_loss_x_map=dsae_loss_x_map,\n",
    "    n_instances=n_inst,\n",
    ")\n",
    "sae = nt.Sed(sae_cfg).to(device)\n",
    "loss_fn = nt.msle\n",
    "lr = 5e-3\n",
    "\n",
    "n_epochs = 20\n",
    "batch_sz = 1024\n",
    "n_steps = (spikes_train.shape[0] // batch_sz) * n_epochs\n",
    "log_freq = max(1, n_steps // n_epochs // 2)\n",
    "dead_latent_window = max(1, n_steps // n_epochs // 3)\n",
    "\n",
    "data_log = nt.optimize(  # train model\n",
    "    spk_cts=spikes_train,\n",
    "    sed=sae,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=t.optim.Adam(sae.parameters(), lr=lr),\n",
    "    use_lr_sched=True,\n",
    "    dead_latent_window=dead_latent_window,\n",
    "    n_steps=n_steps,\n",
    "    log_freq=log_freq,\n",
    "    batch_sz=batch_sz,\n",
    "    log_wandb=False,\n",
    "    plot_l0=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate SAEs\n",
    "\n",
    "To validate the SAEs, examine the following printed values and plots.\n",
    "\n",
    "1. **NaN check:**\n",
    "Confirms that no NaN values appear in the encoder/decoder weights.\n",
    "\n",
    "2. **Decoder weights:**\n",
    "The histograms show the distribution of decoder weights for each SAE instance. Both should look similar and roughly centered around zero. If one model has a very different distribution, it may not have trained properly.\n",
    "\n",
    "3. **L0 of latents:**\n",
    "Shows how many latents are active per example, measured at the final Matryoshka level (which contains all latents). You want the median to be around the top-$k$ setting you chose for this largest level.\n",
    "\n",
    "4. **Latent activity density:**\n",
    "Fraction of time each latent is active. Most should fire sparsely (low fractions). Many at 0 (dead) is common but ideally limited, while many at 1 (always-on) is undesirable as it breaks sparsity.\n",
    "\n",
    "5. **R² of reconstructions:**\n",
    "R² between reconstructions and true spike counts, shown per example and per unit.\n",
    "\n",
    "6. **Cosine similarity of reconstructions:**\n",
    "Cosine similarity between reconstructions and true spike counts, shown per example and per unit.\n",
    "\n",
    "7. **R² of summed spike counts:**\n",
    "Reports how well the reconstructions capture the total population activity (sum of all spikes per example).\n",
    "\n",
    "**What to look for:**\n",
    "- No NaNs in encoder/decoder weights.\n",
    "- All SAE instances producing broadly similar plots.\n",
    "- Decoder weights centered near zero.\n",
    "- Latents used sparsely but not all dead.\n",
    "- Reconstruction metrics high (R² and cosine similarity near 1).\n",
    "- High R² for summed spike counts (close to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check for nans in weights.\"\"\"\n",
    "\n",
    "sae.W_dec.isnan().sum(), sae.W_enc.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize weights.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for inst in range(n_inst):\n",
    "    W_dec_flat = asnumpy(sae.W_dec[inst].float()).ravel()\n",
    "    sns.histplot(W_dec_flat, bins=1000, stat=\"probability\", alpha=0.7, label=f\"SAE {inst}\")\n",
    "    \n",
    "ax.set_title(\"SAE decoder weights\")\n",
    "ax.set_xlabel(\"Weight value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize metrics over all examples and units.\"\"\"\n",
    "\n",
    "fig, topk_acts_4d_train, recon_spk_cts_train, r2_per_unit_train, _, cossim_per_unit_train, _ = (\n",
    "    nt.eval_model(spikes_train, sae, batch_sz=batch_sz)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate variance explained of summed spike counts.\"\"\"\n",
    "\n",
    "n_recon_examples = recon_spk_cts_train.shape[0]\n",
    "recon_summed_spk_cts = reduce(recon_spk_cts_train, \"example inst neuron -> example inst\", \"sum\")\n",
    "\n",
    "actual_summed_spk_cts = reduce(spikes_train, \"example neuron -> example\", \"sum\")\n",
    "actual_summed_spk_cts = actual_summed_spk_cts[:n_recon_examples]  # trim to match\n",
    "\n",
    "for inst in range(n_inst):\n",
    "    r2 = r2_score(\n",
    "        asnumpy(actual_summed_spk_cts.float()),\n",
    "        asnumpy(recon_summed_spk_cts[:, inst].float()),\n",
    "    )\n",
    "    print(f\"SAE instance {inst} R² (summed spike count over all units per example) = {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove bad units and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Remove bad units and retrain.\"\"\"\n",
    "\n",
    "# Set threshold for removing units\n",
    "r2_thresh = 0.1\n",
    "inst = 0\n",
    "r2_inst = r2_per_unit_train[:, inst]\n",
    "keep_mask = r2_inst > r2_thresh\n",
    "print(f\"frac units above {r2_thresh=}: {keep_mask.sum() / keep_mask.shape[0]:.2f}\")\n",
    "print(f\"Number to keep: {keep_mask.sum()} / {keep_mask.shape[0]}\")\n",
    "\n",
    "if keep_mask.all():\n",
    "    print(\"All units pass threshold — skipping retrain.\")\n",
    "    spikes_train_pruned = spikes_train\n",
    "    spikes_val_pruned = spikes_val\n",
    "else:\n",
    "    # Prune\n",
    "    spikes_train_pruned = spikes_train[:, keep_mask]\n",
    "    spikes_val_pruned = spikes_val[:, keep_mask]\n",
    "\n",
    "    # Retrain SAE on pruned train data\n",
    "    sae_cfg = nt.SedConfig(\n",
    "        n_input=spikes_train_pruned.shape[1],\n",
    "        dsed_topk_map=dsae_topk_map,\n",
    "        dsed_loss_x_map=dsae_loss_x_map,\n",
    "        seq_len=1,\n",
    "        n_instances=n_inst,\n",
    "    )\n",
    "    sae = nt.Sed(sae_cfg).to(device)\n",
    "    loss_fn = nt.msle\n",
    "    lr = 5e-3\n",
    "\n",
    "    n_epochs = 20\n",
    "    batch_sz = 1024\n",
    "    n_steps = (spikes_train_pruned.shape[0] // batch_sz) * n_epochs\n",
    "    log_freq = max(1, n_steps // n_epochs // 2)\n",
    "    dead_latent_window = max(1, n_steps // n_epochs // 3)\n",
    "\n",
    "    data_log = nt.optimize(\n",
    "        spk_cts=spikes_train_pruned,\n",
    "        sed=sae,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=t.optim.Adam(sae.parameters(), lr=lr),\n",
    "        use_lr_sched=True,\n",
    "        dead_latent_window=dead_latent_window,\n",
    "        n_steps=n_steps,\n",
    "        log_freq=log_freq,\n",
    "        batch_sz=batch_sz,\n",
    "        log_wandb=False,\n",
    "        plot_l0=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Re-visualize metrics over all examples and units.\"\"\"\n",
    "\n",
    "if keep_mask.all():\n",
    "    print(\"All units pass threshold — skipping re-visualization.\")\n",
    "else:\n",
    "    fig, topk_acts_4d_train, recon_spk_cts_train, r2_per_unit_train, _, cossim_per_unit_train, _ = (\n",
    "        nt.eval_model(spikes_train_pruned, sae, batch_sz=batch_sz)\n",
    ")\n",
    "    n_recon_examples_train = recon_spk_cts_train.shape[0]\n",
    "    recon_summed_train = reduce(recon_spk_cts_train, \"example inst unit -> example inst\", \"sum\")\n",
    "\n",
    "    actual_summed_train = reduce(spikes_train_pruned, \"example unit -> example\", \"sum\")\n",
    "    actual_summed_train = actual_summed_train[:n_recon_examples_train]\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        r2 = r2_score(\n",
    "            asnumpy(actual_summed_train.float()),\n",
    "            asnumpy(recon_summed_train[:, inst].float()),\n",
    "        )\n",
    "        print(f\"SAE instance {inst} R² (summed spike count per example) = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize metrics on validation data.\"\"\"\n",
    "\n",
    "if spikes_val_pruned.shape[0] == 0:\n",
    "    print(\"No validation data available.\")\n",
    "else:\n",
    "    print(\"Validation data metrics:\")\n",
    "\n",
    "    fig, topk_acts_4d_val, recon_spk_cts_val, r2_per_unit_val, _, cossim_per_unit_val, _ = (\n",
    "        nt.eval_model(spikes_val_pruned, sae, batch_sz=batch_sz)\n",
    ")\n",
    "    n_recon_examples_val = recon_spk_cts_val.shape[0]\n",
    "    recon_summed_val = reduce(recon_spk_cts_val, \"example inst unit -> example inst\", \"sum\")\n",
    "\n",
    "    actual_summed_val = reduce(spikes_val_pruned, \"example unit -> example\", \"sum\")\n",
    "    actual_summed_val = actual_summed_val[:n_recon_examples_val]\n",
    "\n",
    "    for inst in range(n_inst):\n",
    "        r2 = r2_score(\n",
    "            asnumpy(actual_summed_val.float()),\n",
    "            asnumpy(recon_summed_val[:, inst].float()),\n",
    "        )\n",
    "        print(f\"SAE instance {inst} R² (summed spike count per example) = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save/load SAE activations\n",
    "\n",
    "A set of activations per subject (Jenkins and Nitschke) is provided in the `saved_sae_latent_activations` folder. They were both generated using a split by proportion (80/20 train/val split). If you want to use them, set the `load_activations` variable below to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load saved activations if available; otherwise build acts_df and (optionally) save.\"\"\"\n",
    "\n",
    "save_path = Path(r\"../saved_sae_latent_activations\")\n",
    "load_activations = True\n",
    "save_activations = False\n",
    "activations_file_train = \"sae_activations_train.parquet\"\n",
    "activations_file_val = \"sae_activations_val.parquet\"\n",
    "mask_file_train = \"train_mask.parquet\"\n",
    "mask_file_val = \"val_mask.parquet\"\n",
    "\n",
    "# Build save path (same style as before)\n",
    "session_dates = []\n",
    "for session in sessions:\n",
    "    session_date = datetime.fromtimestamp(session.session.recording_date).strftime(\"%Y%m%d\")\n",
    "    session_dates.append(session_date)\n",
    "session_dates_str = \"_\".join(session_dates)\n",
    "\n",
    "data_identifier = f\"{subject_name}_{session_dates_str}\"\n",
    "activations_save_path = save_path / data_identifier / \"sae_activations\"\n",
    "activations_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if load_activations:\n",
    "    acts_df_train = pd.read_parquet(activations_save_path / activations_file_train)\n",
    "    acts_df_val = (\n",
    "        pd.read_parquet(activations_save_path / activations_file_val) \n",
    "        if (activations_save_path / activations_file_val).exists() else None\n",
    "    )\n",
    "    train_mask = pd.read_parquet(activations_save_path / mask_file_train)[\"mask\"]\n",
    "    val_mask = pd.read_parquet(activations_save_path / mask_file_val)[\"mask\"]\n",
    "    print(f\"Loaded activations from {activations_save_path}\")\n",
    "else:\n",
    "    # Train\n",
    "    arr_tr = asnumpy(topk_acts_4d_train)  # [example_idx, instance_idx, latent_idx, act_value]\n",
    "    # Sparse activations (tight dtypes on indices, fp32 values)\n",
    "    acts_df_train = pd.DataFrame({\n",
    "        \"example_idx\": arr_tr[:, 0].astype(int),\n",
    "        \"instance_idx\": arr_tr[:, 1].astype(int),\n",
    "        \"latent_idx\": arr_tr[:, 2].astype(int),\n",
    "        \"activation_value\": arr_tr[:, 3].astype(np.float32),\n",
    "    })\n",
    "\n",
    "    if spikes_val_pruned.shape[0] > 0:\n",
    "        # Val\n",
    "        arr_va = asnumpy(topk_acts_4d_val)\n",
    "        acts_df_val = pd.DataFrame({\n",
    "            \"example_idx\": arr_va[:, 0].astype(int),\n",
    "            \"instance_idx\": arr_va[:, 1].astype(int),\n",
    "            \"latent_idx\": arr_va[:, 2].astype(int),\n",
    "            \"activation_value\": arr_va[:, 3].astype(np.float32),\n",
    "        })\n",
    "    else:\n",
    "        acts_df_val = None\n",
    "\n",
    "    n_examples_train = (int(acts_df_train[\"example_idx\"].max()) + 1)\n",
    "    std_threshold = 1e-6\n",
    "\n",
    "    # Precompute squared values once, then sum both in one grouped pass\n",
    "    acts_df_train_with_sq = (\n",
    "        acts_df_train.assign(activation_value_sq=acts_df_train[\"activation_value\"] ** 2)\n",
    "    )\n",
    "\n",
    "    latent_stats = (\n",
    "        acts_df_train_with_sq.groupby([\"instance_idx\", \"latent_idx\"], as_index=False)\n",
    "        .agg(sum_val=(\"activation_value\", \"sum\"),\n",
    "            sum_sq=(\"activation_value_sq\", \"sum\"))\n",
    "    )\n",
    "    n_examples_train = int(acts_df_train[\"example_idx\"].max()) + 1\n",
    "    latent_stats[\"mean\"] = latent_stats[\"sum_val\"] / n_examples_train\n",
    "    latent_stats[\"var\"]  = (latent_stats[\"sum_sq\"] / n_examples_train) - latent_stats[\"mean\"]**2\n",
    "    latent_stats[\"std\"]  = np.sqrt(np.clip(latent_stats[\"var\"].to_numpy(), 0.0, None))\n",
    "\n",
    "    kept_latents = latent_stats.loc[latent_stats[\"std\"] > std_threshold, [\"instance_idx\", \"latent_idx\"]]\n",
    "    n_dropped = len(latent_stats) - len(kept_latents)\n",
    "\n",
    "    if n_dropped:\n",
    "        acts_df_train = acts_df_train.merge(kept_latents, on=[\"instance_idx\", \"latent_idx\"], how=\"inner\")\n",
    "        acts_df_val = (\n",
    "            acts_df_val.merge(kept_latents, on=[\"instance_idx\", \"latent_idx\"], how=\"inner\")\n",
    "            if (spikes_val_pruned.shape[0] > 0) else None\n",
    "        )\n",
    "        print(f\"Pruned {n_dropped} SAE latents (std ≤ {std_threshold}). Kept {len(kept_latents)}.\")\n",
    "\n",
    "    if save_activations:\n",
    "        acts_df_train.to_parquet(activations_save_path / activations_file_train, index=False)\n",
    "        (\n",
    "            acts_df_val.to_parquet(activations_save_path / activations_file_val, index=False) \n",
    "            if spikes_val_pruned.shape[0] > 0 else None\n",
    "        )\n",
    "        train_mask.to_frame(name=\"mask\").to_parquet(\n",
    "            activations_save_path / mask_file_train, index=True\n",
    "        )\n",
    "        val_mask.to_frame(name=\"mask\").to_parquet(\n",
    "            activations_save_path / mask_file_val, index=True\n",
    "        )\n",
    "        print(f\"Saved activations to {activations_save_path}\")\n",
    "\n",
    "print(f\"Activations: \\nTrain shape: {acts_df_train.shape}\" + (f\", Val shape: {acts_df_val.shape}\" if acts_df_val is not None else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decode\n",
    "\n",
    "Here we use our NLDisco latents to decode a behavioural variable (in this case hand velocity). We also provide code to train CEBRA models and use the resulting CEBRA embeddings to decode as a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define decoding target.\"\"\"\n",
    "\n",
    "# Target for decoding (can be replaced with other metadata e.g., accel, position, etc.)\n",
    "decoding_target = np.column_stack([\n",
    "    metadata_binned[\"vel_x\"].to_numpy(dtype=np.float32),\n",
    "    metadata_binned[\"vel_y\"].to_numpy(dtype=np.float32),\n",
    "])\n",
    "y_train = decoding_target[train_mask]\n",
    "y_val = decoding_target[val_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With NLDisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare the data for decoding.\"\"\"\n",
    "\n",
    "if acts_df_val is None:\n",
    "        raise ValueError(\"Validation split is required for this section.\")\n",
    "\n",
    "# Build CSR matrices\n",
    "n_examples_train = int(train_mask.sum())\n",
    "n_examples_val = int(val_mask.sum()) if (acts_df_val is not None) else 0\n",
    "feature_index = decode.build_feature_index(acts_df_train, acts_df_val)\n",
    "X_train = decode.df_to_csr(acts_df_train, feature_index, n_examples=n_examples_train)\n",
    "X_val = decode.df_to_csr(acts_df_val, feature_index, n_examples=n_examples_val) if (acts_df_val is not None) else None\n",
    "print(f\"X_train shape: {X_train.shape}\" + (f\", X_val shape: {X_val.shape}\" if X_val is not None else \"\")) # (examples, latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Decode using NLDisco latents.\"\"\"\n",
    "\n",
    "best = decode.decode_with_lag_sweep(\n",
    "    X_tr=X_train,\n",
    "    X_va=X_val,\n",
    "    y_tr=y_train,\n",
    "    y_va=y_val,\n",
    "    lags=range(0, 6), # bins to sweep\n",
    "    alpha=30.0 # ridge strength\n",
    ")\n",
    "\n",
    "print(f\"Best lag (bins): {best['lag']}\")\n",
    "print(f\"R² per dimension: {np.round(best['r2_per_dim'], 3)}\")\n",
    "print(f\"Mean R²: {best['r2_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With CEBRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train CEBRA models.\"\"\"\n",
    "\n",
    "if val_mask.sum() == 0:\n",
    "    raise ValueError(\"Validation split is required for this section.\")\n",
    "\n",
    "save_path = Path(r\"../saved_CEBRA_models\")\n",
    "cebra_save_path = save_path / data_identifier / \"CEBRA_models\"\n",
    "\n",
    "# Prepare data for CEBRA\n",
    "trial_ids_train = metadata_binned['trial_idx'][train_mask].values\n",
    "trial_ids_val = metadata_binned['trial_idx'][val_mask].values\n",
    "\n",
    "# Only train if there are not already models saved\n",
    "if any(cebra_save_path.glob(\"*.pt\")):\n",
    "    print(f\"CEBRA models already found in {cebra_save_path}, skipping training.\")\n",
    "else:\n",
    "    print(f\"Saving CEBRA models to {cebra_save_path}\")\n",
    "    cebra_save_path.mkdir(parents=True, exist_ok=True)\n",
    "    params_grid = dict(\n",
    "        output_dimension=[48],\n",
    "        time_offsets=[1, 2],  # in the paper for 20ms bins they use 1-2 so I think 1 here for 50ms is good? or [0,1]?\n",
    "        model_architecture='offset10-model',\n",
    "        temperature_mode='constant',\n",
    "        temperature=np.linspace(0.0001, 0.004, 10).tolist(),\n",
    "        max_iterations=[5000],\n",
    "        batch_size=[512],\n",
    "        device='cuda_if_available',\n",
    "        num_hidden_units=[[128, 256, 512]],\n",
    "        verbose=True)\n",
    "\n",
    "    # Run the grid search\n",
    "    grid_search = cebra.grid_search.GridSearch()\n",
    "    datasets = {data_identifier: (spikes_train, trial_ids_train)}\n",
    "    grid_search.fit_models(datasets, params=params_grid, models_dir=cebra_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Validate top CEBRA model and visualise embeddings.\"\"\"\n",
    "\n",
    "# Load top model\n",
    "df_results = grid_search.get_df_results(models_dir=cebra_save_path)\n",
    "dataset_name = df_results[\"dataset_name\"].iloc[0]\n",
    "best_model, best_model_name = grid_search.get_best_model(dataset_name=dataset_name, models_dir=cebra_save_path)\n",
    "print(\"The best model is:\", best_model_name)\n",
    "model_path = cebra_save_path / f\"{best_model_name}.pt\"\n",
    "top_model = cebra.CEBRA.load(model_path, weights_only=False)\n",
    "print(\"Training InfoNCE loss curve:\")\n",
    "ax = cebra.plot_loss(top_model)\n",
    "plt.show()\n",
    "print(f\"Final InfoNCE training loss: \", df_results[\"loss\"].min())\n",
    "\n",
    "# Transform\n",
    "top_train_embedding = top_model.transform(spikes_train_arr)\n",
    "top_val_embedding = top_model.transform(spikes_val_arr)\n",
    "\n",
    "# InfoNCE loss\n",
    "loss_train = cebra.sklearn.metrics.infonce_loss(top_model, spikes_train_arr, trial_ids_train, num_batches=200, correct_by_batchsize=True)\n",
    "loss_val = cebra.sklearn.metrics.infonce_loss(top_model, spikes_val_arr, trial_ids_val, num_batches=200, correct_by_batchsize=True)\n",
    "print(\"\\nInfoNCE loss recalculated:\")\n",
    "print(\"Train:\", loss_train)\n",
    "print(\"Validation: \", loss_val)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Plot embeddings\n",
    "# Decide which metadata variable to use for coloring\n",
    "feature_train = metadata_binned[\"vel_x\"].to_numpy(dtype=np.float32)[train_mask]\n",
    "feature_val = metadata_binned[\"vel_x\"].to_numpy(dtype=np.float32)[val_mask]\n",
    "# Create random samples for plotting\n",
    "n_plot_train = min(10_000, top_train_embedding.shape[0])\n",
    "train_sample = np.random.choice(top_train_embedding.shape[0], size=n_plot_train, replace=False)\n",
    "n_plot_val = min(10_000, top_val_embedding.shape[0])\n",
    "val_sample = np.random.choice(top_val_embedding.shape[0], size=n_plot_val, replace=False)\n",
    "# Sample embeddings and features\n",
    "top_train_embedding_sample = top_train_embedding[train_sample, :]\n",
    "feature_train_sample = feature_train[train_sample]\n",
    "top_val_embedding_sample = top_val_embedding[val_sample, :]\n",
    "feature_val_sample = feature_val[val_sample]\n",
    "# Plot\n",
    "fig = cebra.integrations.plotly.plot_embedding_interactive(\n",
    "    top_train_embedding_sample,\n",
    "    embedding_labels=feature_train_sample,\n",
    "    title=\"CEBRA-Time (train)\",\n",
    "    markersize=3,\n",
    "    cmap=\"rainbow\"\n",
    ")\n",
    "fig.show()\n",
    "fig = cebra.integrations.plotly.plot_embedding_interactive(\n",
    "    top_val_embedding_sample,\n",
    "    embedding_labels=feature_val_sample,\n",
    "    title=\"CEBRA-Time (validation)\",\n",
    "    markersize=3,\n",
    "    cmap=\"rainbow\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load CEBRA models and average embeddings.\"\"\"\n",
    "\n",
    "# Load all .pt files in folder\n",
    "pt_files = sorted(cebra_save_path.glob(\"*.pt\"))\n",
    "if not pt_files:\n",
    "    raise FileNotFoundError(f\"No .pt files found in {cebra_save_path}\")\n",
    "print(f\"Found {len(pt_files)} models.\")\n",
    "\n",
    "# Compute validation losses and store together with paths\n",
    "model_losses = []\n",
    "for p in pt_files:\n",
    "    model = cebra.CEBRA.load(p, weights_only=False)\n",
    "    loss_val = cebra.sklearn.metrics.infonce_loss(model, spikes_val_arr, trial_ids_val, num_batches=200, correct_by_batchsize=True)\n",
    "    model_losses.append((p, loss_val))\n",
    "\n",
    "# Sort by loss and use this to determine a loss threshold to use with the function below\n",
    "model_losses.sort(key=lambda x: x[1])\n",
    "print(\"Model losses (sorted):\")\n",
    "for p, l in model_losses:\n",
    "    print(f\"{p.name}: {l:.4f}\")\n",
    "\n",
    "# Average embeddings\n",
    "X_train, X_val = cebra_utils.average_cebra_embeddings_procrustes(\n",
    "    model_losses, spikes_train_arr, spikes_val_arr, loss_threshold=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Decode using averaged CEBRA embeddings.\"\"\"\n",
    "\n",
    "best = decode.decode_with_lag_sweep(\n",
    "    X_tr=X_train,\n",
    "    X_va=X_val,\n",
    "    y_tr=y_train,\n",
    "    y_va=y_val,\n",
    "    lags=range(0, 6),\n",
    "    scaler=StandardScaler(with_mean=True, with_std=True),\n",
    "    alpha=1.0,\n",
    ")\n",
    "\n",
    "print(f\"Best lag (bins): {best['lag']}\")\n",
    "print(f\"R² per dimension: {best['r2_per_dim']}\")\n",
    "print(f\"Mean R²: {best['r2_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Find features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pick whether to find features in training or validation set.\"\"\"\n",
    "\n",
    "search_train = True\n",
    "\n",
    "if search_train:\n",
    "    acts_df_split = acts_df_train\n",
    "    metadata_binned_split = metadata_binned[train_mask].copy()\n",
    "    spikes_df_split = spikes_df[train_mask].copy()\n",
    "else:\n",
    "    acts_df_split = acts_df_val\n",
    "    metadata_binned_split = metadata_binned[val_mask].copy()\n",
    "    spikes_df_split = spikes_df[val_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically map latents to metadata\n",
    "\n",
    "Now we see if latents represent properties of known continuous and discrete behavioral and environmental variables in the MC_Maze task.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "Latents are mapped to real-world variables through the calculation of a selectivity score. For a latent $l$ and condition $c$ (variable/value combination e.g., velocity is between 0 and 1, or maze condition = 3, etc.):\n",
    "\n",
    "$$\n",
    "\\text{activation\\_frac\\_during} =\n",
    "\\frac{\\#\\{\\text{activations of } l \\text{ in examples with } c\\}}\n",
    "     {\\#\\{\\text{examples with } c\\}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{activation\\_frac\\_outside} =\n",
    "\\frac{\\#\\{\\text{activations of } l \\text{ in examples without } c\\}}\n",
    "     {\\#\\{\\text{examples without } c\\}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{selectivity\\_score} =\n",
    "\\frac{\\text{activation\\_frac\\_during}}\n",
    "     {\\text{activation\\_frac\\_during} + \\text{activation\\_frac\\_outside}}\n",
    "$$\n",
    "\n",
    "- $\\approx 1$: latent mainly active *during* the condition (highly selective)  \n",
    "- $\\approx 0.5$: latent active equally in/out (not selective)  \n",
    "- $\\approx 0$: latent mostly active *outside* the condition\n",
    "\n",
    "The `map_latents_to_metadata` function:\n",
    "1. For discrete variables: computes activation fractions + selectivity score per condition value.  \n",
    "2. For continuous variables: bins, then reuses discrete analysis.  \n",
    "3. Results are ranked by selectivity score and the `top_n_mappings` are returned.\n",
    "\n",
    "**Key arguments to play with:**\n",
    "- `discrete_vars` and `continuous_vars`: as default only one of each was included because the function takes time to run, but you may be interested in exploring different or additional variables\n",
    "- `n_bins_continuous`: number of bins for continuous variables. This will affect whether you find more general features (small number of bins so you have less granularity e.g., you can only distinguish between fast vs slow hand velocity) or specific features (larger number of bins for more granularity e.g., you can now distinguish between very fast vs fast vs intermediate vs slow vs very slow hand velocity)\n",
    "- `min_activation_frac`: minimum fraction of condition examples a latent must activate in\n",
    "- `top_n_mappings`: number of highest-scoring mappings kept per variable/value/instance combination (default `3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Map latents to metadata variables.\"\"\"\n",
    "\n",
    "# discrete_vars = ['event', 'maze_condition', 'barriers', 'targets', 'hit_position_x', 'hit_position_y', 'hit_position_angle']\n",
    "# continuous_vars = ['vel_magnitude', 'accel_magnitude', 'movement_angle']\n",
    "discrete_vars = ['event']\n",
    "continuous_vars = ['vel_magnitude']\n",
    "latent_metadata_mapping = pipeline.map_latents_to_metadata(  # this can take a minute\n",
    "    acts_df_split,\n",
    "    metadata_binned_split,\n",
    "    discrete_vars=discrete_vars,\n",
    "    continuous_vars=continuous_vars,\n",
    "    min_activation_frac=0.5,\n",
    "    n_bins_continuous=[[0, 1, 2, 4, 6, 10, 15, 40, 100, 250, 1300]], # Or [12] for 12 equal-width bins\n",
    "    top_n_mappings=5\n",
    ")\n",
    "latent_metadata_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find meaningful features and their contributing units\n",
    "\n",
    "The code below generates a dashboard like this one, providing an interactive way to explore a model's latents in search for meaningful features and their contributing units:\n",
    "\n",
    "![](./figures/feature_finding_dashboard.png)\n",
    "\n",
    "*(If the image above does not render, see ./figures/feature_finding_dashboard.png)*\n",
    "\n",
    "The `latent_metadata_mapping` dataframe generated is used as a starting point to identify promising features (\"preset\" option on the dashboard). If you want more preset options to explore, feel free to return to the \"Automatically map latents to metadata\" section and play with the arguments that control the mapping process.\n",
    "\n",
    "You may also choose to look through latents manually, though this tends to take more time (\"manual selection\" option on the dashboard).\n",
    "\n",
    "Remember:\n",
    "- Matryoshka SAEs split latents across multiple levels, with higher levels capturing broad patterns and lower levels capturing more specific ones. Check the `dsae` settings you used to train your SAEs to see what latents were allocated to each level.\n",
    "- The neural recordings come from two brain regions (PMd and M1). Use the `units_df` dataframe generated below to look up the mapping of unit IDs to brain regions - this can help you see whether particular features are driven more strongly by units in one region or the other.\n",
    "- By default, this tutorial set `search_train = True` at the beginning of this section, so the feature search runs on the training set. You can switch to the validation set instead, which lets you check that the SAEs generalise to unseen data (and to unseen sessions if you chose to split by session in section \"1. Load and prepare data\").\n",
    "\n",
    "*Note*: Here we present a prepared dashboard for viewing the activity of latents in the context of the MC_Maze task. In general when working with new datasets, we recommend spending some time designing similar simple, bespoke dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Print unit to brain region mapping for reference.\"\"\"\n",
    "\n",
    "unit_stats_ids = [u.decode() for u in session.units.id]\n",
    "units_df = pd.DataFrame({\n",
    "    \"unit_id\": [int(u.split(\"elec\")[1]) for u in unit_stats_ids],\n",
    "    \"region\": [\"PMd\" if \"group_1\" in u else \"M1\" for u in unit_stats_ids]\n",
    "})\n",
    "\n",
    "units_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature finding dashboard.\"\"\"\n",
    "\n",
    "save_path = Path(r\"../saved_plot_data_paper\")\n",
    "plot_data_save_path = save_path / data_identifier / \"plot_data_all_data\"\n",
    "\n",
    "pipeline.build_feature_finding_dashboard(\n",
    "    latent_metadata_mapping=latent_metadata_mapping,\n",
    "    acts_df=acts_df_split,\n",
    "    spikes_df=spikes_df_split,\n",
    "    metadata_binned=metadata_binned_split,\n",
    "    save_path=plot_data_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Make paper plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_save_path_nitschke_all_data = save_path / f\"nitschke_20090812_20090819_20090910\" / \"plot_data_all_data\"\n",
    "plot_data_save_path_nitschke_sessions_1_2 = save_path / f\"nitschke_20090812_20090819_20090910\" / \"plot_data_sessions_1_2\"\n",
    "plot_data_save_path_jenkins_all_data = save_path / f\"jenkins_20090912_20090916_20090918_20090923\" / \"plot_data_all_data\"\n",
    "\n",
    "# Extreme velocity features\n",
    "fig = pipeline.plot_selectivity_score_from_saved(\n",
    "    plot_data_save_path_nitschke_all_data / \"plotdata_inst0_latent24_vel_magnitude_binned.csv\",\n",
    "    plot_data_save_path_nitschke_sessions_1_2 / \"plotdata_inst1_latent98_vel_magnitude_binned.csv\",\n",
    "    plot_data_save_path_jenkins_all_data / \"plotdata_inst1_latent78_vel_magnitude_binned.csv\",\n",
    "    labels=[\"N\", \"N (unseen session)\", \"J\"],\n",
    "    y_max=0.8,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# High velocity features\n",
    "fig = pipeline.plot_selectivity_score_from_saved(\n",
    "    plot_data_save_path_nitschke_all_data / \"plotdata_inst0_latent2_vel_magnitude_binned.csv\",\n",
    "    plot_data_save_path_nitschke_sessions_1_2 / \"plotdata_inst1_latent62_vel_magnitude_binned.csv\",\n",
    "    plot_data_save_path_jenkins_all_data / \"plotdata_inst1_latent137_vel_magnitude_binned.csv\",\n",
    "    labels=[\"N\", \"N (unseen session)\", \"J\"],\n",
    "    y_max=0.8,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Low velocity features\n",
    "fig = pipeline.plot_selectivity_score_from_saved(\n",
    "    plot_data_save_path_nitschke_all_data / \"plotdata_inst0_latent127_vel_magnitude_binned.csv\",\n",
    "    plot_data_save_path_nitschke_sessions_1_2 / \"plotdata_inst1_latent505_vel_magnitude_binned.csv\",\n",
    "    plot_data_save_path_jenkins_all_data / \"plotdata_inst1_latent43_vel_magnitude_binned.csv\",\n",
    "    labels=[\"N\", \"N (unseen session)\", \"J\"],\n",
    "    y_max=0.8,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Intermediate velocity features\n",
    "fig = pipeline.plot_selectivity_score_from_saved(\n",
    "    plot_data_save_path_nitschke_all_data / \"plotdata_inst0_latent208_vel_magnitude_binned.csv\",\n",
    "    plot_data_save_path_nitschke_sessions_1_2 / \"plotdata_inst1_latent191_vel_magnitude_binned.csv\",\n",
    "    labels=[\"N\", \"N (unseen session)\"],\n",
    "    y_max=0.8,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Hit position on right feature\n",
    "fig = pipeline.plot_selectivity_score_from_saved(\n",
    "    plot_data_save_path_nitschke_all_data / \"plotdata_inst0_latent169_hit_position_x.csv\",\n",
    "    labels=[\"N\"],\n",
    "    y_max=0.6,\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
