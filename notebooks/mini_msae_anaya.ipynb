{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSAE training and feature analysis\n",
    "\n",
    "Main goal: look at different levels of features in a 3-level MSAE\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Train MSAE on spikes dataset\n",
    "\n",
    "2. Create (topk) sae feature df\n",
    "\n",
    "3. Create stim (meta)data df\n",
    "\n",
    "4. Interp / autointerp\n",
    "    \n",
    "    a. Create infra for finding stim at times of particular feature(s)\n",
    "    \n",
    "    b. Create infra for finding topk SAE features that fire at times of particular stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set notebook settings.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %flow mode reactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import packages.\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "# IPython and Jupyter-related imports\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Third-party libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import temporaldata as td\n",
    "import torch as t\n",
    "from einops import (\n",
    "    asnumpy,\n",
    "    einsum,\n",
    "    pack,\n",
    "    parse_shape,\n",
    "    rearrange,\n",
    "    reduce,\n",
    "    repeat,\n",
    "    unpack,\n",
    ")\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from jaxtyping import Float, Int\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from rich import print as rprint\n",
    "from scipy import stats\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, r2_score\n",
    "from temporaldata import Data\n",
    "from torch import Tensor, bfloat16, nn\n",
    "from torch.nn import functional as F\n",
    "from torcheval.metrics.functional import r2_score as tm_r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Local project modules\n",
    "from mini import train as mt\n",
    "from mini.util import vec_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max rows and cols for df display\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "pd.set_option(\"display.max_columns\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_session_data(session):\n",
    "    \"\"\"Clean session data by filtering trials and spikes based on quality criteria.\"\"\"\n",
    "    \n",
    "    # Mark Churchland gave me a matlab script which I have converted to python\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    # I think is_valid is already defined by brainsets as (session.trials.discard_trial == 0) & (session.trials.task_success == 1) so it's a bit redundant but including it all just to be sure\n",
    "    # In theory I should also filter based on whether the maze was possible or not (a field called \"unhittable\") but I cannot find this in the data, perhaps this has already been done in this release of the data\n",
    "    good_trials = (session.trials.trial_type > 0) & (session.trials.is_valid == 1) & (session.trials.discard_trial == 0) & (session.trials.novel_maze == 0) & (session.trials.trial_version < 3) \n",
    "    session.trials = session.trials.select_by_mask(good_trials)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out extraneous trials, went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    success = (session.trials.task_success == 1)\n",
    "    session.trials = session.trials.select_by_mask(success)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out unsuccessful trials, went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    post_move = 0.8 # to be kept, there must be at least this many ms after the movement onset\n",
    "    long_enough = (session.trials.end - session.trials.move_begins_time >= post_move) # should essentially always be true for successes\n",
    "    session.trials = session.trials.select_by_mask(long_enough)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out trials that were too short, went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    consistent = (session.trials.correct_reach == 1)\n",
    "    session.trials = session.trials.select_by_mask(consistent)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out trials with inconsistent reaches (not similar enough to the \\\"prototypical\\\" trial), went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    primary_conditions = np.unique(session.trials.maze_condition)\n",
    "    num_conditions = len(primary_conditions)\n",
    "    print(\"Number of primary conditions:\", num_conditions)\n",
    "    # Check to make sure they are monotonic, starting from 1 and counting up\n",
    "    if min(primary_conditions) != 1 or len(np.unique(np.diff(primary_conditions))) != 1:\n",
    "        raise ValueError(\"Primary conditions are not monotonic or do not start from 1\")\n",
    "\n",
    "    # In theory I should filter units based on a ranking from 1-4 but I cannot find the ranking in the data, perhaps this has already been done in this release of the data\n",
    "\n",
    "    # Only keep spikes that are within the cleaned trials\n",
    "    session.spikes = session.spikes.select_by_interval(session.trials)\n",
    "    session.hand = session.hand.select_by_interval(session.trials)\n",
    "    session.eye = session.eye.select_by_interval(session.trials)\n",
    "\n",
    "    # Convert session recording date to timestamp\n",
    "    session.session.recording_date = datetime.strptime(session.session.recording_date, '%Y-%m-%d %H:%M:%S')\n",
    "    session.session.recording_date = session.session.recording_date.timestamp()\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_maze_conditions(session):\n",
    "    \"\"\"\n",
    "    Analyze what each maze_condition corresponds to in terms of \n",
    "    maze parameters (barriers, targets, hit position).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique maze conditions\n",
    "    unique_conditions = np.unique(session.trials.maze_condition)\n",
    "    \n",
    "    # Create a summary for each condition\n",
    "    condition_summary = []\n",
    "    \n",
    "    for condition in unique_conditions:\n",
    "        # Get trials for this condition\n",
    "        condition_mask = session.trials.maze_condition == condition\n",
    "        \n",
    "        # Get the unique values for this condition\n",
    "        barriers = np.unique(session.trials.maze_num_barriers[condition_mask])\n",
    "        targets = np.unique(session.trials.maze_num_targets[condition_mask])\n",
    "        hit_position = np.unique(session.trials.hit_target_position[condition_mask], axis=0)\n",
    "        if len(hit_position) > 1:\n",
    "            raise ValueError(f\"Condition {condition} has multiple hit positions: {hit_position}\")\n",
    "        else:\n",
    "            hit_position = hit_position[0]\n",
    "        \n",
    "        # Count trials for this condition\n",
    "        num_trials = np.sum(condition_mask)\n",
    "        \n",
    "        # Store for summary table\n",
    "        condition_summary.append({\n",
    "            'Maze Condition': condition,\n",
    "            'Trials': num_trials,\n",
    "            'Barriers': barriers,\n",
    "            'Targets': targets,\n",
    "            'Hit Position': hit_position, \n",
    "            'Hit Position Angles': str(np.degrees(np.arctan2(hit_position[1], hit_position[0])))\n",
    "        })\n",
    "    summary_df = pd.DataFrame(condition_summary)\n",
    "    \n",
    "    # Convert hit positions to tuples temporarily for proper duplicate detection\n",
    "    summary_df_temp = summary_df.copy()\n",
    "    summary_df_temp['Hit Position Tuple'] = summary_df_temp['Hit Position'].apply(tuple)\n",
    "    plot_df = summary_df_temp.drop_duplicates(subset=['Hit Position Tuple'], keep='first')\n",
    "    plot_df = plot_df.drop('Hit Position Tuple', axis=1)  # Remove the temporary column\n",
    "    \n",
    "    # Create a proper DataFrame for plotting\n",
    "    plot_data = pd.DataFrame({\n",
    "        'Hit Position X': plot_df['Hit Position'].apply(lambda x: x[0]),\n",
    "        'Hit Position Y': plot_df['Hit Position'].apply(lambda x: x[1]),\n",
    "        'Maze Condition': plot_df['Maze Condition'].astype(str)\n",
    "    })\n",
    "    \n",
    "    # Generate unique colors for each maze condition\n",
    "    import plotly.colors as pc\n",
    "    n_conditions = len(plot_data['Maze Condition'].unique())\n",
    "    colors = pc.sample_colorscale('viridis', [i/(max(n_conditions-1, 1)) for i in range(n_conditions)])\n",
    "    \n",
    "    # Plot hit position by maze condition\n",
    "    fig = px.scatter(\n",
    "        plot_data,\n",
    "        x='Hit Position X',\n",
    "        y='Hit Position Y',\n",
    "        color='Maze Condition',\n",
    "        labels={'Hit Position X': 'Hit Position X', 'Hit Position Y': 'Hit Position Y', 'color': 'Maze Condition'},\n",
    "        title='Hit Position by Maze Condition',\n",
    "        color_discrete_sequence=colors,\n",
    "        hover_data=['Maze Condition']\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis=dict(scaleanchor=\"y\", scaleratio=1, range=[-150, 150]),\n",
    "        yaxis=dict(constrain=\"domain\", range=[-100, 100]),\n",
    "        width=600,\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your data directory\n",
    "data_path = r\"C:\\Users\\pouge\\Documents\\mini_data\\brainsets\\processed\\churchland_shenoy_neural_2012\"\n",
    "# data_path = \"/ceph/aeon/aeon/SANe/brainsets_data/processed/churchland_shenoy_neural_2012\"\n",
    "data_path = Path(data_path)\n",
    "\n",
    "# List all h5 files in the directory\n",
    "h5_files = [f for f in os.listdir(data_path) if f.endswith('.h5')]\n",
    "print(f\"Available h5 files: {h5_files}\")\n",
    "\n",
    "# User parameters\n",
    "subject_name = \"nitschke\"  # Change to \"nitschke\" or \"jenkins\"\n",
    "num_files_to_load = 3     # Change to desired number of files, max 6 (only 3 work) for nitschke, 4 for jenkins\n",
    "\n",
    "# Filter files by subject\n",
    "subject_files = [f for f in h5_files if subject_name.lower() in f.lower()]\n",
    "print(f\"\\nFiles for subject {subject_name}: {subject_files}\")\n",
    "\n",
    "if len(subject_files) == 0:\n",
    "    print(f\"No files found for subject {subject_name}\")\n",
    "elif len(subject_files) < num_files_to_load:\n",
    "    print(f\"Only {len(subject_files)} files available for {subject_name}, loading all of them\")\n",
    "    num_files_to_load = len(subject_files)\n",
    "\n",
    "# Load and clean the specified number of files\n",
    "sessions = []\n",
    "for i in range(min(num_files_to_load, len(subject_files))):\n",
    "    file_path = os.path.join(data_path, subject_files[i])\n",
    "    print(f\"\\nLoading file {i+1}/{num_files_to_load}: {subject_files[i]}\")\n",
    "    \n",
    "    # Read neural data from HDF5\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        session = Data.from_hdf5(f)\n",
    "\n",
    "        session.spikes.materialize()\n",
    "        session.trials.materialize()\n",
    "        session.hand.materialize()\n",
    "        session.eye.materialize()\n",
    "        session.session.materialize()\n",
    "\n",
    "        print(\"Session ID: \", session.session.id)\n",
    "        print(\"Session subject id: \", session.subject.id)\n",
    "        print(\"Session subject sex: \", session.subject.sex)\n",
    "        print(\"Session subject species: \", session.subject.species)\n",
    "        print(\"Session recording date: \", session.session.recording_date)\n",
    "        print(\"Original number of trials:\", len(session.trials.start))\n",
    "        \n",
    "        # Clean the session data\n",
    "        try:\n",
    "            session = clean_session_data(session)\n",
    "            print(\"Final number of trials after cleaning:\", len(session.trials.start))\n",
    "\n",
    "            print(\"Summary of primary conditions:\")\n",
    "            primary_conditions_summary = analyze_maze_conditions(session)\n",
    "            display(primary_conditions_summary)\n",
    "            \n",
    "            sessions.append(session)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {session.session.id}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nSuccessfully loaded and cleaned {len(sessions)} sessions for subject {subject_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_maze_conditions_consistency(sessions):\n",
    "    \"\"\"\n",
    "    Fix maze condition numbering to be consistent across all sessions.\n",
    "    \n",
    "    Args:\n",
    "        sessions: List of session objects\n",
    "    \n",
    "    Returns:\n",
    "        List of cleaned sessions with consistent maze condition numbering\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_maze_signature(session, condition):\n",
    "        \"\"\"Get maze parameters for a specific condition to create a signature\"\"\"\n",
    "        condition_mask = session.trials.maze_condition == condition\n",
    "        \n",
    "        # Get unique values for this condition\n",
    "        barriers = tuple(np.unique(session.trials.maze_num_barriers[condition_mask]))\n",
    "        targets = tuple(np.unique(session.trials.maze_num_targets[condition_mask]))\n",
    "        hit_position = np.unique(session.trials.hit_target_position[condition_mask], axis=0)\n",
    "        if len(barriers) > 1 or len(targets) > 1 or len(hit_position) > 1:\n",
    "            raise ValueError(f\"Condition {condition} has the following >1 unique values for one of the following: \"\n",
    "                             f\"barriers={barriers}, targets={targets}, hit_position={hit_position}. \"\n",
    "                             \"This should not be possible.\")\n",
    "        else:\n",
    "            hit_position = tuple(tuple(hit_position[0]))\n",
    "        \n",
    "        return (barriers, targets, hit_position)\n",
    "    \n",
    "    def get_group_signature(session, group_conditions):\n",
    "        \"\"\"Get combined signature for a group of 3 conditions\"\"\"\n",
    "        group_sigs = []\n",
    "        for condition in sorted(group_conditions):\n",
    "            sig = get_maze_signature(session, condition)\n",
    "            group_sigs.append(sig)\n",
    "        return tuple(group_sigs)\n",
    "    \n",
    "    # Remove sessions that don't have multiples of 3 maze conditions\n",
    "    valid_sessions = []\n",
    "    for i, session in enumerate(sessions):\n",
    "        unique_conditions = np.unique(session.trials.maze_condition)\n",
    "        num_conditions = len(unique_conditions)\n",
    "        \n",
    "        if num_conditions % 3 != 0:\n",
    "            print(f\"WARNING: Removing session {session.session.id} - has {num_conditions} maze conditions (not multiple of 3)\")\n",
    "            continue\n",
    "        \n",
    "        valid_sessions.append(session)\n",
    "    \n",
    "    if len(valid_sessions) == 0:\n",
    "        raise ValueError(\"No valid sessions remaining after filtering\")\n",
    "    \n",
    "    print(f\"Kept {len(valid_sessions)} out of {len(sessions)} sessions after filtering\")\n",
    "    \n",
    "    # Pick reference session (most unique maze conditions)\n",
    "    condition_counts = []\n",
    "    for session in valid_sessions:\n",
    "        unique_conditions = len(np.unique(session.trials.maze_condition))\n",
    "        condition_counts.append(unique_conditions)\n",
    "    \n",
    "    max_conditions = max(condition_counts)\n",
    "    ref_idx = condition_counts.index(max_conditions)\n",
    "    reference_session = valid_sessions[ref_idx]\n",
    "    \n",
    "    print(f\"Using session {reference_session.session.id} as reference (has {max_conditions} maze conditions)\")\n",
    "    \n",
    "    # Group conditions and check for duplicates in reference session\n",
    "    ref_conditions = sorted(np.unique(reference_session.trials.maze_condition))\n",
    "    ref_groups = []\n",
    "    \n",
    "    for i in range(0, len(ref_conditions), 3):\n",
    "        group = ref_conditions[i:i+3]\n",
    "        if len(group) != 3:\n",
    "            raise ValueError(f\"Reference session has incomplete group: {group}\")\n",
    "        ref_groups.append(group)\n",
    "    \n",
    "    # Create signatures for reference groups\n",
    "    ref_group_signatures = {}\n",
    "    ref_signatures_to_group = {}\n",
    "    \n",
    "    for group_idx, group in enumerate(ref_groups):\n",
    "        signature = get_group_signature(reference_session, group)\n",
    "        \n",
    "        if signature in ref_signatures_to_group:\n",
    "            existing_group = ref_signatures_to_group[signature]\n",
    "            print(f\"WARNING: Duplicate group found in reference session!\")\n",
    "            print(f\"  Group {existing_group} and Group {group} have identical maze parameters\")\n",
    "        \n",
    "        ref_group_signatures[group_idx] = signature\n",
    "        ref_signatures_to_group[signature] = group\n",
    "    \n",
    "    print(f\"Reference session has {len(ref_groups)} groups of maze conditions\")\n",
    "\n",
    "    print(\"Table of reference session maze conditions:\")\n",
    "    primary_conditions_summary = analyze_maze_conditions(reference_session)\n",
    "    display(primary_conditions_summary) \n",
    "    \n",
    "    # Process all sessions to match groups and renumber\n",
    "    processed_sessions = []\n",
    "    \n",
    "    for session in valid_sessions:\n",
    "        print(f\"Processing session {session.session.id}...\")\n",
    "        \n",
    "        # Get conditions and group them\n",
    "        conditions = sorted(np.unique(session.trials.maze_condition))\n",
    "        session_groups = []\n",
    "        \n",
    "        for i in range(0, len(conditions), 3):\n",
    "            group = conditions[i:i+3]\n",
    "            session_groups.append(group)\n",
    "        \n",
    "        # Match each group to reference\n",
    "        condition_mapping = {}  # old_condition -> new_condition\n",
    "        \n",
    "        for group in session_groups:\n",
    "            group_sig = get_group_signature(session, group)\n",
    "            \n",
    "            # Find matching reference group\n",
    "            matched_ref_group_idx = None\n",
    "            for ref_idx, ref_sig in ref_group_signatures.items():\n",
    "                if group_sig == ref_sig:\n",
    "                    matched_ref_group_idx = ref_idx\n",
    "                    break\n",
    "            \n",
    "            if matched_ref_group_idx is None:\n",
    "                print(f\"ERROR: Could not match group {group} in session {session.session.id}\")\n",
    "                print(f\"Group signature: {group_sig}\")\n",
    "                print(\"Available reference signatures:\")\n",
    "                for ref_idx, ref_sig in ref_group_signatures.items():\n",
    "                    ref_group = ref_groups[ref_idx]\n",
    "                    print(f\"  Reference group {ref_group}: {ref_sig}\")\n",
    "                raise ValueError(f\"Unmatchable maze conditions {group} in session {session.session.id}\")\n",
    "            \n",
    "            # Map old conditions to new conditions\n",
    "            new_base_condition = matched_ref_group_idx * 3 + 1  # 1, 4, 7, 10, ...\n",
    "            for i, old_condition in enumerate(sorted(group)):\n",
    "                new_condition = new_base_condition + i\n",
    "                condition_mapping[old_condition] = new_condition\n",
    "        \n",
    "        \n",
    "        # Apply the mapping to the session\n",
    "        new_maze_conditions = np.array([condition_mapping[old] for old in session.trials.maze_condition])\n",
    "        session.trials.maze_condition = new_maze_conditions\n",
    "        \n",
    "        processed_sessions.append(session)\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(processed_sessions)} sessions with consistent maze condition numbering\")\n",
    "    return processed_sessions\n",
    "\n",
    "# Add this after your existing code, after sessions are loaded and cleaned:\n",
    "print(\"Fixing maze condition consistency across sessions...\")\n",
    "\n",
    "sessions = fix_maze_conditions_consistency(sessions)\n",
    "\n",
    "# Show updated condition summaries for verification\n",
    "print(\"\\nCondition remapping summary:\")\n",
    "for i, session in enumerate(sessions):\n",
    "    unique_conditions = sorted(np.unique(session.trials.maze_condition))\n",
    "    print(f\"Session {session.session.id}: {unique_conditions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train a model on a specific session post fixing maze conditions consistency\n",
    "# print(len(sessions))\n",
    "# sessions =  [sessions[i] for i in [2]]\n",
    "# print(len(sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bin_size = 0.05\n",
    "\n",
    "# Check unit consistency across sessions\n",
    "unit_ids = np.unique(sessions[0].spikes.unit_index)\n",
    "for session in sessions:\n",
    "    unique_units = np.unique(session.spikes.unit_index)\n",
    "    if not np.array_equal(unique_units, unit_ids):\n",
    "        raise ValueError(\"Sessions do not have the same unit IDs. Cannot combine spike data.\")\n",
    "\n",
    "# Determine global bin alignment start point\n",
    "global_start = min(session.session.recording_date + session.trials.start.min() for session in sessions)\n",
    "global_start = np.floor(global_start / bin_size) * bin_size  # ensure clean bin alignment\n",
    "\n",
    "# Convert to consistent timestamps\n",
    "n_decimals = int(-np.log10(bin_size)) + 1 if bin_size < 1 else 0\n",
    "\n",
    "# Accumulator for all binned trials\n",
    "binned_dfs = []\n",
    "\n",
    "# Loop over sessions\n",
    "for session in sessions:\n",
    "    # Shift spike timestamps to absolute time\n",
    "    abs_timestamps = session.spikes.timestamps + session.session.recording_date\n",
    "    unit_ids_this_session = session.spikes.unit_index\n",
    "    df_spikes = pd.DataFrame({\n",
    "        'timestamp': abs_timestamps,\n",
    "        'unit': unit_ids_this_session\n",
    "    }).sort_values('timestamp')\n",
    "\n",
    "    # Convert trial times to absolute time\n",
    "    trial_starts = session.trials.start + session.session.recording_date\n",
    "    trial_ends = session.trials.end + session.session.recording_date\n",
    "    df_trials = pd.DataFrame({\n",
    "        'trial_start': trial_starts,\n",
    "        'trial_end': trial_ends\n",
    "    }).sort_values('trial_start')\n",
    "\n",
    "    # Assign each spike to the most recent trial_start <= timestamp\n",
    "    df_merged = pd.merge_asof(\n",
    "        df_spikes,\n",
    "        df_trials[['trial_start', 'trial_end']],\n",
    "        left_on='timestamp',\n",
    "        right_on='trial_start',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    # Drop spikes that fall outside their trial interval\n",
    "    df_merged = df_merged[df_merged['timestamp'] < df_merged['trial_end']]\n",
    "\n",
    "    # Compute bin index relative to global bin start\n",
    "    df_merged['bin'] = ((df_merged['timestamp'] - global_start) / bin_size).astype(int)\n",
    "\n",
    "    # Group by (bin, unit) and count spikes\n",
    "    df_counts = (\n",
    "        df_merged\n",
    "        .groupby(['bin', 'unit'], observed=True)\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # Pivot to wide format: units as columns\n",
    "    spk_cts = df_counts.pivot_table(\n",
    "        index='bin',\n",
    "        columns='unit',\n",
    "        values='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "\n",
    "    # Convert to consistent timestamps\n",
    "    session_timestamps = np.round(global_start + spk_cts.index * bin_size, n_decimals)\n",
    "    spk_cts.index = pd.Index(session_timestamps, name='timestamp')\n",
    "    spk_cts.columns.name = None\n",
    "\n",
    "    # Collect results\n",
    "    binned_dfs.append(spk_cts)\n",
    "\n",
    "# Concatenate all binned trials across sessions\n",
    "spk_cts_df = pd.concat(binned_dfs)\n",
    "spk_cts_df = spk_cts_df[~spk_cts_df.index.duplicated()]\n",
    "spk_cts_df.sort_index(inplace=True)\n",
    "\n",
    "# Result: each row = time bin, each col = unit, values = spike count\n",
    "display(spk_cts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean firing rate (Hz) per unit\n",
    "duration_sec = len(spk_cts_df) * bin_size\n",
    "mean_firing_rates = spk_cts_df.sum(axis=0) / duration_sec  # spikes/sec\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(mean_firing_rates, bins=30, edgecolor='black')\n",
    "plt.xlabel('Mean Firing Rate (Hz)')\n",
    "plt.ylabel('Number of Units')\n",
    "plt.title('Distribution of Mean Firing Rates')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary stats\n",
    "print(\"Mean firing rate across units: {:.2f} Hz\".format(mean_firing_rates.mean()))\n",
    "print(\"Median: {:.2f} Hz\".format(np.median(mean_firing_rates)))\n",
    "print(\"Range: {:.2f}–{:.2f} Hz\".format(mean_firing_rates.min(), mean_firing_rates.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the spike count matrix to a 1D array\n",
    "flattened_spike_counts = spk_cts_df.values.flatten()\n",
    "\n",
    "# Plot histogram of spike counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(flattened_spike_counts, bins=50, edgecolor='black')\n",
    "plt.title(\"Distribution of Spike Counts per Unit per Time Bin\")\n",
    "plt.xlabel(\"Spike Count\")\n",
    "plt.ylabel(\"Number of Unit-Bin Combinations\")\n",
    "plt.yscale(\"log\")  # Optional: log scale to better visualize skewed distributions\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check sparsity of binned spike counts.\"\"\"\n",
    "\n",
    "frac_nonzero_bins = (spk_cts_df != 0).values.sum() / spk_cts_df.size\n",
    "frac_nonzero_examples = (spk_cts_df.sum(axis=1) > 0).mean()\n",
    "print(f\"{frac_nonzero_bins=:.4f}\")\n",
    "print(f\"{frac_nonzero_examples=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load spikes and set sae config.\"\"\"\n",
    "\n",
    "# gpu for training\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device=}\")\n",
    "\n",
    "spk_cts = t.from_numpy(spk_cts_df.to_numpy()).bfloat16().to(device)\n",
    "spk_cts /= spk_cts.max()  # max normalize spike counts\n",
    "\n",
    "dsae_topk_map = {256: 8, 512: 16, 1024: 24}\n",
    "dsae_topk_map = dict(sorted(dsae_topk_map.items()))  # ensure sorted from smallest to largest\n",
    "dsae_loss_x_map = {256: 1, 512: 1.25, 1024: 1.5}\n",
    "dsae_loss_x_map = dict(sorted(dsae_loss_x_map.items()))\n",
    "# dsae_topk_map = {1024: 12, 2048: 24, 4096: 48}\n",
    "dsae = max(dsae_topk_map.keys())\n",
    "n_inst = 2\n",
    "\n",
    "display(spk_cts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MSAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sae_cfg = mt.SaeConfig(\n",
    "    n_input_ae=spk_cts.shape[1],\n",
    "    dsae_topk_map=dsae_topk_map,\n",
    "    dsae_loss_x_map=dsae_loss_x_map,\n",
    "    seq_len=1,\n",
    "    n_instances=n_inst,\n",
    ")\n",
    "sae = mt.Sae(sae_cfg).to(device)\n",
    "loss_fn = mt.msle\n",
    "tau = 1.0\n",
    "lr = 5e-3\n",
    "\n",
    "n_epochs = 20\n",
    "batch_sz = 1024\n",
    "n_steps = spk_cts.shape[0] // batch_sz * n_epochs\n",
    "log_freq = n_steps // n_epochs // 2\n",
    "dead_neuron_window = n_steps // n_epochs // 3\n",
    "\n",
    "data_log = mt.optimize(  # train model\n",
    "    spk_cts=spk_cts,\n",
    "    sae=sae,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=t.optim.Adam(sae.parameters(), lr=lr),\n",
    "    use_lr_sched=True,\n",
    "    dead_neuron_window=dead_neuron_window,\n",
    "    n_steps=n_steps,\n",
    "    log_freq=log_freq,\n",
    "    batch_sz=batch_sz,\n",
    "    log_wandb=False,\n",
    "    plot_l0=False,\n",
    "    tau=tau\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check for nans in weights.\"\"\"\n",
    "\n",
    "sae.W_dec.isnan().sum(), sae.W_enc.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize weights.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for inst in range(n_inst):\n",
    "    W_dec_flat = asnumpy(sae.W_dec[inst].float()).ravel()\n",
    "    sns.histplot(W_dec_flat, bins=1000, stat=\"probability\", alpha=0.7, label=f\"SAE {inst}\")\n",
    "    \n",
    "ax.set_title(\"SAE decoder weights\")\n",
    "ax.set_xlabel(\"Weight value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize metrics over all examples and units.\"\"\"\n",
    "\n",
    "topk_acts_4d, recon_spk_cts, r2_per_unit, _, cossim_per_unit, _ = mt.eval_model(spk_cts, sae, batch_sz=batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate variance explained of summed spike counts.\"\"\"\n",
    "\n",
    "# Var explained for summed spike counts.\n",
    "n_recon_examples = recon_spk_cts.shape[0]\n",
    "recon_summed_spk_cts = reduce(recon_spk_cts, \"example inst unit -> example inst\", \"sum\")\n",
    "actual_summed_spk_cts = reduce(spk_cts, \"example unit -> example\", \"sum\")\n",
    "actual_summed_spk_cts = actual_summed_spk_cts[0:n_recon_examples]  # trim to match\n",
    "for inst in range(n_inst):\n",
    "    r2 = r2_score(\n",
    "        asnumpy(actual_summed_spk_cts.float()), asnumpy(recon_summed_spk_cts[:, inst].float())\n",
    "    )\n",
    "    print(f\"SAE instance {inst} R² (summed spike count over all units per example) = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If cosine similarity is high but r2 is low, it suggests that the model is capturing the structure of the data but not the magnitude.\n",
    "# Calculate scale ratio of norms to check this\n",
    "\n",
    "# Expand spk_cts to shape [n_examples, 1, n_units]\n",
    "spk_cts_exp = spk_cts[:recon_spk_cts.shape[0]].unsqueeze(1)\n",
    "\n",
    "# Compute norms\n",
    "true_norms = t.norm(spk_cts_exp, dim=-1)               # shape: [n_examples, 1]\n",
    "recon_norms = t.norm(recon_spk_cts, dim=-1)            # shape: [n_examples, n_instances]\n",
    "\n",
    "# Compute scale ratio per example and instance\n",
    "scale = true_norms / recon_norms                       # shape: [n_examples, n_instances]\n",
    "\n",
    "print(scale.mean(dim=0))  # if it’s consistently >1 or <1, your model is biased in magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_cts_trimmed = spk_cts[:recon_spk_cts.shape[0]]\n",
    "bias = (recon_spk_cts - spk_cts_trimmed.unsqueeze(1)).mean(dim=0)\n",
    "print(bias.mean(dim=0))  # mean bias per unit, averaged across examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_var = spk_cts_trimmed.var(dim=0).mean()\n",
    "pred_var = recon_spk_cts.var(dim=0).mean()\n",
    "print(f\"True variance: {true_var.item():.4f}, Pred variance: {pred_var.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bad units and retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set threshold for removing units\n",
    "r2_thresh = 0.1\n",
    "inst = 0\n",
    "r2_per_unit = r2_per_unit[:, inst]\n",
    "keep_mask = r2_per_unit > r2_thresh\n",
    "print(f\"frac units above {r2_thresh=}: {keep_mask.sum() / keep_mask.shape[0]:.2f}\")\n",
    "print(f\"Number to keep: {keep_mask.sum()} / {keep_mask.shape[0]}\")\n",
    "\n",
    "# Remove units and retrain\n",
    "spk_cts = spk_cts[:, keep_mask]\n",
    "\n",
    "sae_cfg = mt.SaeConfig(\n",
    "    n_input_ae=spk_cts.shape[1],\n",
    "    dsae_topk_map=dsae_topk_map,\n",
    "    dsae_loss_x_map=dsae_loss_x_map,\n",
    "    seq_len=1,\n",
    "    n_instances=n_inst,\n",
    ")\n",
    "sae = mt.Sae(sae_cfg).to(device)\n",
    "loss_fn = mt.msle\n",
    "tau = 1.0\n",
    "lr = 5e-3\n",
    "\n",
    "n_epochs = 20\n",
    "batch_sz = 1024\n",
    "n_steps = spk_cts.shape[0] // batch_sz * n_epochs\n",
    "log_freq = n_steps // n_epochs // 2\n",
    "dead_neuron_window = n_steps // n_epochs // 3\n",
    "\n",
    "data_log = mt.optimize(  # train model\n",
    "    spk_cts=spk_cts,\n",
    "    sae=sae,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=t.optim.Adam(sae.parameters(), lr=lr),\n",
    "    use_lr_sched=True,\n",
    "    dead_neuron_window=dead_neuron_window,\n",
    "    n_steps=n_steps,\n",
    "    log_freq=log_freq,\n",
    "    batch_sz=batch_sz,\n",
    "    log_wandb=False,\n",
    "    plot_l0=False,\n",
    "    tau=tau\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Re-Visualize metrics over all examples and units.\"\"\"\n",
    "\n",
    "topk_acts_4d, recon_spk_cts, r2_per_unit, _, cossim_per_unit, _ = mt.eval_model(spk_cts, sae, batch_sz=batch_sz)\n",
    "\n",
    "n_recon_examples = recon_spk_cts.shape[0]\n",
    "recon_summed_spk_cts = reduce(recon_spk_cts, \"example inst unit -> example inst\", \"sum\")\n",
    "actual_summed_spk_cts = reduce(spk_cts, \"example unit -> example\", \"sum\")\n",
    "actual_summed_spk_cts = actual_summed_spk_cts[0:n_recon_examples]  # trim to match\n",
    "for inst in range(n_inst):\n",
    "    r2 = r2_score(\n",
    "        asnumpy(actual_summed_spk_cts.float()), asnumpy(recon_summed_spk_cts[:, inst].float())\n",
    "    )\n",
    "    print(f\"SAE instance {inst} R² (summed spike count over all units per example) = {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get environment / behavior (meta)data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data from all sessions into arrays\n",
    "hand_data = defaultdict(list)\n",
    "eye_data = defaultdict(list)\n",
    "trial_data = defaultdict(list)\n",
    "\n",
    "for i, session in enumerate(sessions):\n",
    "    recording_date = session.session.recording_date\n",
    "    \n",
    "    # Hand data - extract timestamps and 2D arrays\n",
    "    timestamps = session.hand.timestamps + recording_date\n",
    "    acc = session.hand.acc_2d\n",
    "    pos = session.hand.pos_2d\n",
    "    vel = session.hand.vel_2d\n",
    "    \n",
    "    hand_data['timestamp'].append(timestamps)\n",
    "    hand_data['acc_x'].append(acc[:, 0])\n",
    "    hand_data['acc_y'].append(acc[:, 1])\n",
    "    hand_data['pos_x'].append(pos[:, 0])\n",
    "    hand_data['pos_y'].append(pos[:, 1])\n",
    "    hand_data['vel_x'].append(vel[:, 0])\n",
    "    hand_data['vel_y'].append(vel[:, 1])\n",
    "    hand_data['session'].append(np.full(len(timestamps), i))\n",
    "    \n",
    "    # Eye data - extract timestamps and position arrays\n",
    "    eye_timestamps = session.eye.timestamps + recording_date\n",
    "    eye_pos = session.eye.pos\n",
    "    \n",
    "    eye_data['timestamp'].append(eye_timestamps)\n",
    "    eye_data['pos_x'].append(eye_pos[:, 0])\n",
    "    eye_data['pos_y'].append(eye_pos[:, 1])\n",
    "    \n",
    "    # Trial data - add recording_date to all time columns\n",
    "    trial_data['start'].append(session.trials.start + recording_date)\n",
    "    trial_data['end'].append(session.trials.end + recording_date)\n",
    "    trial_data['target_on_time'].append(session.trials.target_on_time + recording_date)\n",
    "    trial_data['go_cue_time'].append(session.trials.go_cue_time + recording_date)\n",
    "    trial_data['move_begins_time'].append(session.trials.move_begins_time + recording_date)\n",
    "    trial_data['move_ends_time'].append(session.trials.move_ends_time + recording_date)\n",
    "    trial_data['maze_condition'].append(session.trials.maze_condition)\n",
    "    trial_data['barriers'].append(session.trials.maze_num_barriers)\n",
    "    trial_data['targets'].append(session.trials.maze_num_targets)\n",
    "    trial_data['hit_position_x'].append([pos[0] for pos in session.trials.hit_target_position])\n",
    "    trial_data['hit_position_y'].append([pos[1] for pos in session.trials.hit_target_position])\n",
    "    trial_data['hit_position_angle'].append([np.degrees(np.arctan2(pos[1], pos[0])) for pos in session.trials.hit_target_position])\n",
    "\n",
    "# Concatenate all arrays into final datasets\n",
    "combined_hand_data = {}\n",
    "for key, arrays in hand_data.items():\n",
    "    combined_hand_data[key] = np.concatenate(arrays)\n",
    "\n",
    "combined_eye_data = {}\n",
    "for key, arrays in eye_data.items():\n",
    "    combined_eye_data[key] = np.concatenate(arrays)\n",
    "\n",
    "combined_trial_data = {}\n",
    "for key, arrays in trial_data.items():\n",
    "    combined_trial_data[key] = np.concatenate(arrays)\n",
    "\n",
    "# Create final DataFrames\n",
    "combined_hand_df = pd.DataFrame(combined_hand_data).set_index('timestamp')\n",
    "combined_eye_df = pd.DataFrame(combined_eye_data).set_index('timestamp')\n",
    "combined_trials_df = pd.DataFrame(combined_trial_data)\n",
    "\n",
    "# Create unified timestamp index from all data sources\n",
    "all_event_ts = np.concatenate([\n",
    "    combined_trials_df['target_on_time'].values,\n",
    "    combined_trials_df['go_cue_time'].values,\n",
    "    combined_trials_df['move_begins_time'].values,\n",
    "    combined_trials_df['move_ends_time'].values,\n",
    "])\n",
    "\n",
    "# Get unique timestamps across all data\n",
    "all_ts = np.unique(np.concatenate([\n",
    "    combined_hand_df.index.values,\n",
    "    combined_eye_df.index.values,\n",
    "    all_event_ts\n",
    "]))\n",
    "\n",
    "# Create master dataframe with unified timestamp index\n",
    "metadata = pd.DataFrame(index=all_ts)\n",
    "metadata.index.name = 'timestamp'\n",
    "\n",
    "# Merge hand and eye data\n",
    "metadata = metadata.join(combined_hand_df, how='left')\n",
    "metadata = metadata.join(combined_eye_df, how='left', rsuffix='_eye')\n",
    "\n",
    "# Add event column - mark timestamps that correspond to trial events\n",
    "event_map = {\n",
    "    'target_on_time': 'target_on',\n",
    "    'go_cue_time': 'go_cue',\n",
    "    'move_begins_time': 'move_begins',\n",
    "    'move_ends_time': 'move_ends',\n",
    "}\n",
    "event_col = pd.Series(index=metadata.index, dtype=\"object\")\n",
    "for col, label in event_map.items():\n",
    "    event_times = combined_trials_df[col].values\n",
    "    mask = np.isin(metadata.index.values, event_times)\n",
    "    event_col.iloc[mask] = label\n",
    "metadata['event'] = event_col\n",
    "\n",
    "# Add trial_idx column - assign each timestamp to its trial\n",
    "# Use binary search to efficiently find which trial each timestamp belongs to\n",
    "trial_idx_series = pd.Series(index=metadata.index, dtype='float64')\n",
    "\n",
    "# Sort trials by start time for binary search\n",
    "trial_sort_idx = np.argsort(combined_trials_df['start'].values)\n",
    "starts = combined_trials_df['start'].values[trial_sort_idx]\n",
    "ends = combined_trials_df['end'].values[trial_sort_idx]\n",
    "\n",
    "# Find potential trial for each timestamp\n",
    "timestamps = metadata.index.values\n",
    "start_positions = np.searchsorted(starts, timestamps, side='right') - 1\n",
    "\n",
    "# Check which timestamps are within valid trial intervals\n",
    "valid_mask = (start_positions >= 0) & (start_positions < len(starts))\n",
    "valid_positions = start_positions[valid_mask]\n",
    "valid_timestamps = timestamps[valid_mask]\n",
    "\n",
    "# Verify timestamps are before trial end times\n",
    "end_mask = valid_timestamps <= ends[valid_positions]\n",
    "final_valid_mask = np.zeros(len(timestamps), dtype=bool)\n",
    "final_valid_mask[valid_mask] = end_mask\n",
    "\n",
    "# Assign trial indices to timestamps\n",
    "trial_indices = np.full(len(timestamps), np.nan)\n",
    "trial_indices[final_valid_mask] = trial_sort_idx[valid_positions[end_mask]]\n",
    "trial_idx_series.iloc[:] = trial_indices\n",
    "\n",
    "metadata['trial_idx'] = trial_idx_series\n",
    "\n",
    "# Map trial properties using the trial indices\n",
    "metadata['maze_condition'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['maze_condition']\n",
    ")\n",
    "metadata['barriers'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['barriers']\n",
    ")\n",
    "metadata['targets'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['targets']\n",
    ")\n",
    "metadata['hit_position_x'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['hit_position_x']\n",
    ")\n",
    "metadata['hit_position_y'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['hit_position_y']\n",
    ")\n",
    "metadata['hit_position_angle'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['hit_position_angle']\n",
    ")\n",
    "\n",
    "# Add movement_angle column based on position difference\n",
    "pos_delta_x = metadata['pos_x'].diff()\n",
    "pos_delta_y = metadata['pos_y'].diff()\n",
    "metadata['movement_angle'] = np.degrees(np.arctan2(pos_delta_y, pos_delta_x))\n",
    "\n",
    "# Calculate speed and acceleration magnitudes from vector components\n",
    "metadata['vel_magnitude'] = np.sqrt(\n",
    "    metadata['vel_x'].values**2 + metadata['vel_y'].values**2\n",
    ")\n",
    "metadata['accel_magnitude'] = np.sqrt(\n",
    "    metadata['acc_x'].values**2 + metadata['acc_y'].values**2\n",
    ")\n",
    "\n",
    "# Show result\n",
    "print(\"Metadata:\")\n",
    "display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata_binned with consistent timestamps\n",
    "ts = spk_cts_df.index.values\n",
    "metadata_binned = pd.DataFrame(index=pd.Index(ts, name='timestamp'))\n",
    "\n",
    "# Assign each metadata row to a bin index\n",
    "bin_ids = np.digitize(metadata.index.values, ts) - 1\n",
    "bin_ids = np.clip(bin_ids, 0, len(ts) - 1)\n",
    "\n",
    "# Handle event aggregation \n",
    "event_values = metadata['event'].values\n",
    "event_mask = pd.notna(event_values)\n",
    "\n",
    "if event_mask.any():\n",
    "    event_bin_ids = bin_ids[event_mask]\n",
    "    valid_events = event_values[event_mask].astype(str)\n",
    "    \n",
    "    # Create event assignments using vectorized operations\n",
    "    event_agg = np.full(len(ts), None, dtype=object)\n",
    "    \n",
    "    # Calculate target bins for all events at once\n",
    "    event_indices = np.arange(len(valid_events))\n",
    "    target_bins = event_bin_ids + event_indices\n",
    "    \n",
    "    # Only assign events that fall within valid bin range\n",
    "    valid_targets = target_bins < len(ts)\n",
    "    event_agg[target_bins[valid_targets]] = valid_events[valid_targets]\n",
    "else:\n",
    "    event_agg = np.full(len(ts), None, dtype=object)\n",
    "\n",
    "# Efficient nearest neighbor reindexing using searchsorted\n",
    "metadata_timestamps = metadata.index.values\n",
    "ts_positions = np.searchsorted(metadata_timestamps, ts, side='left')\n",
    "\n",
    "# Handle edge cases and find true nearest neighbors\n",
    "ts_positions = np.clip(ts_positions, 0, len(metadata_timestamps) - 1)\n",
    "\n",
    "# For positions not at the start, check if the previous position is closer\n",
    "mask = ts_positions > 0\n",
    "left_positions = ts_positions.copy()\n",
    "left_positions[mask] = ts_positions[mask] - 1\n",
    "\n",
    "# Calculate distances to determine nearest\n",
    "left_distances = np.abs(ts - metadata_timestamps[left_positions])\n",
    "right_distances = np.abs(ts - metadata_timestamps[ts_positions])\n",
    "\n",
    "# Choose the nearest position\n",
    "final_positions = np.where(left_distances < right_distances, left_positions, ts_positions)\n",
    "\n",
    "# Copy all columns from nearest metadata\n",
    "for col in metadata.columns:\n",
    "    if col != 'event':\n",
    "        metadata_binned[col] = metadata[col].iloc[final_positions].values\n",
    "\n",
    "# Insert distributed events\n",
    "metadata_binned['event'] = event_agg\n",
    "\n",
    "# Result: metadata for all trial bins, one row per bin\n",
    "print(\"Metadata binned:\")\n",
    "display(metadata_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hand_trajectories_by_maze_condition(metadata_binned, max_conditions=None, figsize_per_plot=(3, 3)):\n",
    "    \"\"\"\n",
    "    Plot hand trajectories grouped by maze condition in an optimized grid layout.\n",
    "    \n",
    "    Parameters:\n",
    "    metadata_binned (pd.DataFrame): DataFrame with hand position data and maze conditions\n",
    "    max_conditions (int): Maximum number of conditions to plot (None for all)\n",
    "    figsize_per_plot (tuple): Size of each individual subplot\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Filter out rows without valid position data or maze condition\n",
    "    valid_data = metadata_binned.dropna(subset=['pos_x', 'pos_y', 'maze_condition', 'trial_idx'])\n",
    "    \n",
    "    if len(valid_data) == 0:\n",
    "        print(\"No valid data found for plotting trajectories\")\n",
    "        return\n",
    "    \n",
    "    # Get unique maze conditions\n",
    "    maze_conditions = sorted(valid_data['maze_condition'].unique())\n",
    "    \n",
    "    # Limit conditions if specified\n",
    "    if max_conditions is not None:\n",
    "        maze_conditions = maze_conditions[:max_conditions]\n",
    "        print(f\"Showing first {len(maze_conditions)} of {len(valid_data['maze_condition'].unique())} conditions\")\n",
    "    \n",
    "    n_conditions = len(maze_conditions)\n",
    "    \n",
    "    # Calculate grid dimensions with max 6 columns\n",
    "    cols = min(6, int(np.ceil(np.sqrt(n_conditions))))\n",
    "    rows = int(np.ceil(n_conditions / cols))\n",
    "    \n",
    "    # Create figure with appropriate size\n",
    "    fig_width = cols * figsize_per_plot[0]\n",
    "    fig_height = rows * figsize_per_plot[1]\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(fig_width, fig_height))\n",
    "    \n",
    "    # Flatten axes array for easy indexing\n",
    "    if n_conditions == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "    \n",
    "    # Pre-filter data by conditions for efficiency\n",
    "    condition_data = {condition: valid_data[valid_data['maze_condition'] == condition] \n",
    "                     for condition in maze_conditions}\n",
    "    \n",
    "    # Color palette\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, 12))  # Use Set3 for better distinction\n",
    "    \n",
    "    # Process each condition\n",
    "    for i, condition in enumerate(maze_conditions):\n",
    "        ax = axes[i]\n",
    "        data = condition_data[condition]\n",
    "        \n",
    "        # Group by trial for efficient processing\n",
    "        trial_groups = data.groupby('trial_idx')\n",
    "        trial_count = 0\n",
    "        \n",
    "        for trial_idx, trial_data in trial_groups:\n",
    "            if len(trial_data) > 1:  # Only plot if we have multiple points\n",
    "                # Sort by timestamp for proper trajectory\n",
    "                trial_data = trial_data.sort_index()\n",
    "                \n",
    "                color = colors[trial_count % len(colors)]\n",
    "                \n",
    "                # Plot trajectory as single line (much faster than individual segments)\n",
    "                ax.plot(trial_data['pos_x'].values, trial_data['pos_y'].values, \n",
    "                       alpha=0.6, linewidth=1, color=color)\n",
    "                \n",
    "                # Mark start and end points with clear distinction\n",
    "                ax.scatter(trial_data['pos_x'].iloc[0], trial_data['pos_y'].iloc[0], \n",
    "                          color='green', marker='o', s=40, alpha=0.9, \n",
    "                          edgecolor='darkgreen', linewidth=1.5, zorder=5)\n",
    "                ax.scatter(trial_data['pos_x'].iloc[-1], trial_data['pos_y'].iloc[-1], \n",
    "                          color='red', marker='X', s=50, alpha=0.9, \n",
    "                          edgecolor='darkred', linewidth=1.5, zorder=5)\n",
    "                \n",
    "                trial_count += 1\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_title(f'Condition {condition}\\n({trial_count} trials)', fontsize=10, pad=10)\n",
    "        ax.set_xlabel('X Position', fontsize=8)\n",
    "        ax.set_ylabel('Y Position', fontsize=8)\n",
    "        ax.set_xlim(-150, 150)\n",
    "        ax.set_ylim(-150, 150)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=7)\n",
    "        ax.grid(True, alpha=0.3, linewidth=0.5)\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_conditions, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Add overall title and legend\n",
    "    fig.suptitle('Hand Trajectories by Maze Condition', fontsize=14, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Add a single legend to the figure\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='green', linestyle='None',\n",
    "                  markersize=8, label='Start', markerfacecolor='green', \n",
    "                  markeredgecolor='darkgreen', markeredgewidth=1.5),\n",
    "        plt.Line2D([0], [0], marker='X', color='red', linestyle='None',\n",
    "                  markersize=10, label='End', markerfacecolor='red',\n",
    "                  markeredgecolor='darkred', markeredgewidth=1.5)\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.99, 0.93), fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)  # Make room for suptitle\n",
    "    plt.show()\n",
    "\n",
    "# Function call - show first 20 conditions for manageable viewing\n",
    "plot_hand_trajectories_by_maze_condition(metadata_binned, max_conditions=108)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set vars for saving / loading feature act data!\"\"\"\n",
    "\n",
    "load_acts = True  # if True, will load saved feature activations\n",
    "save_acts = False\n",
    "sae_datafile = \"sae_0.csv\"  # set name of data file to load (or save to)\n",
    "\n",
    "session_dates = []\n",
    "for session in sessions:\n",
    "    session_date = datetime.fromtimestamp(session.session.recording_date)\n",
    "    session_date = session_date.strftime(\"%Y%m%d\")\n",
    "    session_dates.append(session_date)\n",
    "session_dates_str = \"_\".join(session_dates)\n",
    "session_dates_str\n",
    "\n",
    "acts_df_save_path = data_path / f\"{subject_name}_{session_dates_str}\" / \"sae_features\" / sae_datafile\n",
    "acts_df_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "keep_mask_save_path = data_path / f\"{subject_name}_{session_dates_str}\" / \"sae_features\" / \"keep_mask.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load feature activations data.\"\"\"\n",
    "\n",
    "if load_acts:\n",
    "    acts_df = pd.read_csv(acts_df_save_path)\n",
    "    n_recon_examples = int(acts_df.iloc[-1][\"example_idx\"]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create dfs of feature activations.\"\"\"\n",
    "\n",
    "if not load_acts:\n",
    "\n",
    "    # Convert tensor to numpy array for easier handling\n",
    "    acts_array = asnumpy(topk_acts_4d)\n",
    "\n",
    "    # Create DataFrame with the data\n",
    "    acts_df = pd.DataFrame({\n",
    "        \"example_idx\": acts_array[:, 0],\n",
    "        \"instance_idx\": acts_array[:, 1],\n",
    "        \"feature_idx\": acts_array[:, 2],\n",
    "        \"activation_value\": acts_array[:, 3]\n",
    "    })\n",
    "\n",
    "    # Convert appropriate cols to ints\n",
    "    acts_df[\"example_idx\"] = acts_df[\"example_idx\"].astype(int)\n",
    "    acts_df[\"feature_idx\"] = acts_df[\"feature_idx\"].astype(int)\n",
    "    acts_df[\"instance_idx\"] = acts_df[\"instance_idx\"].astype(int)\n",
    "\n",
    "    n_recon_examples = int(acts_df.iloc[-1][\"example_idx\"]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create df with info per feature.\"\"\"\n",
    "\n",
    "# Get a features df from acts df\n",
    "features_df = acts_df.groupby([\"instance_idx\", \"feature_idx\"]).agg(\n",
    "    activation_mean=(\"activation_value\", \"mean\"),\n",
    "    activation_std=(\"activation_value\", \"std\"),\n",
    "    activation_count=(\"activation_value\", \"count\")\n",
    ").reset_index()\n",
    "features_df[\"act_mean_over_std\"] = features_df[\"activation_mean\"] / features_df[\"activation_std\"]\n",
    "features_df[\"activation_frac\"] = features_df[\"activation_count\"] / n_recon_examples\n",
    "features_df = features_df.drop(columns=[\"activation_count\"])\n",
    "features_df = features_df.dropna().reset_index(drop=True)\n",
    "\n",
    "if not load_acts:\n",
    "    # Keep only features from features_df in acts_df\n",
    "    acts_df = acts_df[acts_df[\"feature_idx\"].isin(features_df[\"feature_idx\"])].reset_index(drop=True)\n",
    "\n",
    "if save_acts:\n",
    "    acts_df.to_csv(acts_df_save_path, index=False)\n",
    "    np.save(keep_mask_save_path, keep_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(acts_df)\n",
    "display(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compare features from (\"broad\" or \"general\") and (\"specific\" or \"nested\") groups\"\"\"\n",
    "\n",
    "last_feat_idx_general = list(dsae_topk_map.keys())[0]\n",
    "first_feat_idx_specific = list(dsae_topk_map.keys())[1]\n",
    "\n",
    "# general\n",
    "print(features_df[features_df[\"feature_idx\"] < last_feat_idx_general][\"activation_frac\"].describe())\n",
    "print()\n",
    "print(features_df[np.logical_and(\n",
    "    features_df[\"feature_idx\"] > last_feat_idx_general,\n",
    "    features_df[\"feature_idx\"] < first_feat_idx_specific\n",
    ")][\"activation_frac\"].describe())\n",
    "print()\n",
    "# specific\n",
    "print(features_df[features_df[\"feature_idx\"] > first_feat_idx_specific][\"activation_frac\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunt for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Automatically map features to metadata\"\"\"\n",
    "\n",
    "def analyze_discrete_variable(\n",
    "    acts_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    variable: str,\n",
    "    min_activation_frac: float\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyzes a discrete variable, calculating activation metrics for features meeting a minimum activation fraction.\n",
    "    This version is corrected to handle features that are 100% selective for an event.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    unique_values = metadata_binned[variable].dropna().unique()\n",
    "\n",
    "    for value in unique_values:\n",
    "        try:\n",
    "            event_idxs = np.where(metadata_binned[variable] == value)[0]\n",
    "            if len(event_idxs) == 0:\n",
    "                continue\n",
    "\n",
    "            event_acts_df = acts_df[acts_df[\"example_idx\"].isin(event_idxs)]\n",
    "            if len(event_acts_df) == 0:\n",
    "                continue\n",
    "\n",
    "            event_features_df = event_acts_df.groupby([\"instance_idx\", \"feature_idx\"]).agg(\n",
    "                activation_count=(\"activation_value\", \"count\")\n",
    "            ).reset_index()\n",
    "            n_event_examples = len(event_idxs)\n",
    "            event_features_df[\"activation_frac_event\"] = event_features_df[\"activation_count\"] / n_event_examples\n",
    "\n",
    "            promising_features = event_features_df[event_features_df[\"activation_frac_event\"] >= min_activation_frac]\n",
    "            if promising_features.empty:\n",
    "                continue\n",
    "\n",
    "            non_event_mask = ~acts_df[\"example_idx\"].isin(event_idxs)\n",
    "            non_event_acts_df = acts_df[non_event_mask].merge(\n",
    "                promising_features[[\"instance_idx\", \"feature_idx\"]],\n",
    "                on=[\"instance_idx\", \"feature_idx\"], how=\"inner\"\n",
    "            )\n",
    "\n",
    "            if not non_event_acts_df.empty:\n",
    "                non_event_features_df = non_event_acts_df.groupby([\"instance_idx\", \"feature_idx\"]).agg(\n",
    "                    activation_count=(\"activation_value\", \"count\")\n",
    "                ).reset_index()\n",
    "                n_non_event_examples = len(metadata_binned) - n_event_examples\n",
    "                non_event_features_df[\"activation_frac_non_event\"] = non_event_features_df[\"activation_count\"] / n_non_event_examples\n",
    "                ratio_df = promising_features.merge(\n",
    "                    non_event_features_df, on=[\"instance_idx\", \"feature_idx\"], how=\"left\"\n",
    "                )\n",
    "                ratio_df[\"activation_frac_non_event\"] = ratio_df[\"activation_frac_non_event\"].fillna(0.0)\n",
    "            else:\n",
    "                ratio_df = promising_features.copy()\n",
    "                ratio_df[\"activation_frac_non_event\"] = 0.0\n",
    "            \n",
    "            ratio_df[\"activation_ratio\"] = ratio_df[\"activation_frac_event\"] / (ratio_df[\"activation_frac_non_event\"] + 1e-9)\n",
    "            ratio_df[\"rate_proportion\"] = ratio_df[\"activation_frac_event\"] / (ratio_df[\"activation_frac_event\"] + ratio_df[\"activation_frac_non_event\"])\n",
    "\n",
    "            for _, row in ratio_df.iterrows():\n",
    "                results.append({\n",
    "                    'variable': variable, 'variable_type': 'discrete', 'value': value,\n",
    "                    'instance_idx': row['instance_idx'], 'feature_idx': row['feature_idx'],\n",
    "                    'activation_ratio': row['activation_ratio'],\n",
    "                    'activation_frac_during': row['activation_frac_event'],\n",
    "                    'activation_frac_outside': row['activation_frac_non_event'],\n",
    "                    'rate_proportion': row['rate_proportion']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Could not analyze {variable}={value}: {e}\")\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def analyze_continuous_variable(\n",
    "    acts_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    variable: str,\n",
    "    n_bins: int,\n",
    "    min_activation_frac: float\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyzes a continuous variable by binning it and then using the discrete analysis method.\n",
    "    \"\"\"\n",
    "    print(f\"  Binning '{variable}' into {n_bins} bins...\")\n",
    "    binned_col_name = f\"{variable}_binned\"\n",
    "\n",
    "    data_to_bin = metadata_binned[variable].dropna()\n",
    "    if data_to_bin.empty:\n",
    "        return []\n",
    "\n",
    "    if variable == 'movement_angle':\n",
    "        bins = np.linspace(-180, 180, n_bins + 1)\n",
    "        labels = [f\"({bins[i]:.0f}, {bins[i+1]:.0f}]\" for i in range(n_bins)]\n",
    "        metadata_binned[binned_col_name] = pd.cut(data_to_bin, bins=bins, labels=labels, include_lowest=True)\n",
    "    else:\n",
    "        metadata_binned[binned_col_name] = pd.qcut(data_to_bin, q=n_bins, labels=None, duplicates='drop')\n",
    "\n",
    "    results = analyze_discrete_variable(acts_df, metadata_binned, binned_col_name, min_activation_frac)\n",
    "\n",
    "    for res in results:\n",
    "        res['variable'] = variable\n",
    "        res['variable_type'] = 'continuous'\n",
    "\n",
    "    return results\n",
    "\n",
    "def map_features_to_metadata(\n",
    "    acts_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    discrete_vars: List[str] = None,\n",
    "    continuous_vars: List[str] = None,\n",
    "    exclude_columns: List[str] = None,\n",
    "    min_activation_frac: float = 0.1,\n",
    "    n_bins_continuous: int = 10,\n",
    "    top_n_features: int = 3\n",
    ") -> pd.DataFrame:  \n",
    "    \"\"\"\n",
    "    Automatically maps SAE features to metadata by finding top N features for each condition.\n",
    "    Returns a single DataFrame with both discrete and continuous results.\n",
    "    \"\"\"\n",
    "    if discrete_vars is None: discrete_vars = []\n",
    "    if continuous_vars is None: continuous_vars = []\n",
    "    if exclude_columns is None: exclude_columns = ['trial_idx', 'session']\n",
    "    \n",
    "    all_results = []\n",
    "    print(\"🚀 Starting automated feature-to-metadata mapping...\")\n",
    "    \n",
    "    for variable in metadata_binned.columns:\n",
    "        if variable in exclude_columns:\n",
    "            continue\n",
    "        print(f\"\\nAnalyzing variable: {variable}\")\n",
    "        \n",
    "        if variable in discrete_vars:\n",
    "            print(f\"  Treating as: discrete\")\n",
    "            results = analyze_discrete_variable(acts_df, metadata_binned, variable, min_activation_frac)\n",
    "            all_results.extend(results)\n",
    "        elif variable in continuous_vars:\n",
    "            print(f\"  Treating as: continuous\")\n",
    "            results = analyze_continuous_variable(acts_df, metadata_binned, variable, n_bins=n_bins_continuous, min_activation_frac=min_activation_frac)\n",
    "            all_results.extend(results)\n",
    "        else:\n",
    "            print(f\"  Skipping (not in discrete_vars or continuous_vars list)\")\n",
    "            continue\n",
    "        print(f\"  Found {len(results)} potential associations.\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"\\nNo associations found meeting the minimum activation fraction!\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame instead of tuple\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df['value'] = results_df['value'].astype(str)\n",
    "    \n",
    "    print(f\"\\nRanking features and selecting top {top_n_features} for each condition...\")\n",
    "    ranked_df = (\n",
    "        results_df.sort_values('activation_ratio', ascending=False)\n",
    "        .groupby(['variable', 'value', 'instance_idx'])\n",
    "        .head(top_n_features)\n",
    "    )\n",
    "    \n",
    "    # Sort the combined results\n",
    "    sort_order = ['variable_type', 'variable', 'value', 'instance_idx', 'activation_ratio']\n",
    "    ascending_order = [True, True, True, True, False]\n",
    "    \n",
    "    final_df = ranked_df.sort_values(by=sort_order, ascending=ascending_order).reset_index(drop=True)\n",
    "    \n",
    "    discrete_count = len(final_df[final_df['variable_type'] == 'discrete'])\n",
    "    continuous_count = len(final_df[final_df['variable_type'] == 'continuous'])\n",
    "    \n",
    "    print(f\"\\n✅ Found {discrete_count} top discrete associations.\")\n",
    "    print(f\"✅ Found {continuous_count} top continuous associations.\")\n",
    "    print(f\"✅ Total: {len(final_df)} associations returned in single DataFrame.\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "discrete_vars = ['event', 'maze_condition', 'barriers', 'targets', 'hit_position_x', 'hit_position_y', 'hit_position_angle']\n",
    "continuous_vars = ['vel_magnitude', 'accel_magnitude', 'movement_angle']\n",
    "\n",
    "results = map_features_to_metadata(\n",
    "    acts_df, metadata_binned,\n",
    "    discrete_vars=discrete_vars,\n",
    "    continuous_vars=continuous_vars,\n",
    "    min_activation_frac=0.5,\n",
    "    n_bins_continuous=12,\n",
    "    top_n_features=3\n",
    ")\n",
    "display(results)\n",
    "\n",
    "# # Optional filtering (ratio > 2.0, proportion > 0.5)\n",
    "# results = results[\n",
    "#     (results['activation_ratio'] > 2.0) & \n",
    "#     (results['rate_proportion'] > 0.5)\n",
    "# ].reset_index(drop=True)\n",
    "# display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate z-scores for spike counts across neurons.\"\"\"\n",
    "# Calculate mean and standard deviation for each neuron (column)\n",
    "neuron_means = spk_cts_df.mean(axis=0)\n",
    "neuron_stds = spk_cts_df.std(axis=0)\n",
    "\n",
    "# Calculate z-scores\n",
    "# Handle cases where standard deviation is zero to avoid division by zero\n",
    "spk_z_scores_df = spk_cts_df.sub(neuron_means, axis=1).div(neuron_stds, axis=1)\n",
    "spk_z_scores_df = spk_z_scores_df.replace([np.inf, -np.inf], np.nan) # Replace inf with NaN for clarity\n",
    "\n",
    "# Set z-score to 0 where standard deviation was 0 (and thus z-score would be NaN)\n",
    "spk_z_scores_df = spk_z_scores_df.fillna(0.0)\n",
    "\n",
    "display(spk_z_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualisation functions for event-feature associations\"\"\"\n",
    "def create_canonical_timeline(combined_trials_df, maze_conditions=None, hit_target_positions=None):\n",
    "    \"\"\"\n",
    "    Create a canonical timeline based on average event durations from the data\n",
    "    \n",
    "    Parameters:\n",
    "    - combined_trials_df: trial metadata with absolute timestamps\n",
    "    - maze_conditions: list of maze conditions to include (None for all)\n",
    "    - hit_target_positions: list of target positions to include (None for all)\n",
    "    \n",
    "    Returns:\n",
    "    - canonical_events: dict with canonical event times\n",
    "    - filtered_trials: dataframe with filtered trials\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter trials based on conditions\n",
    "    filtered_trials = combined_trials_df.copy()\n",
    "    \n",
    "    if maze_conditions is not None:\n",
    "        filtered_trials = filtered_trials[filtered_trials['maze_condition'].isin(maze_conditions)]\n",
    "        print(f\"Filtered to maze conditions: {maze_conditions}\")\n",
    "    \n",
    "    if hit_target_positions is not None:\n",
    "        # Handle the tuple format of hit_target_position\n",
    "        if len(hit_target_positions) > 0 and not isinstance(hit_target_positions[0], tuple):\n",
    "            # Convert to tuples if needed\n",
    "            hit_target_positions = [tuple(pos) if isinstance(pos, (list, np.ndarray)) else pos \n",
    "                                  for pos in hit_target_positions]\n",
    "        filtered_trials = filtered_trials[filtered_trials['hit_target_position'].isin(hit_target_positions)]\n",
    "        print(f\"Filtered to target positions: {hit_target_positions}\")\n",
    "    \n",
    "    print(f\"Using {len(filtered_trials)} trials (from {len(combined_trials_df)} total) to create canonical timeline\")\n",
    "    \n",
    "    if len(filtered_trials) == 0:\n",
    "        print(\"No trials match the filtering criteria!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Calculate average durations between consecutive events\n",
    "    events_sequence = ['start', 'target_on_time', 'go_cue_time', 'move_begins_time', 'move_ends_time', 'end']\n",
    "    \n",
    "    # Remove trials with missing events\n",
    "    for event in events_sequence:\n",
    "        filtered_trials = filtered_trials[filtered_trials[event].notna()]\n",
    "    \n",
    "    if len(filtered_trials) == 0:\n",
    "        print(\"No trials have all required events!\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Computing durations from {len(filtered_trials)} complete trials\")\n",
    "    \n",
    "    # Calculate durations between consecutive events\n",
    "    durations = {}\n",
    "    for i in range(len(events_sequence) - 1):\n",
    "        event1, event2 = events_sequence[i], events_sequence[i + 1]\n",
    "        trial_durations = filtered_trials[event2] - filtered_trials[event1]\n",
    "        avg_duration = trial_durations.mean()\n",
    "        durations[f\"{event1}_to_{event2}\"] = avg_duration\n",
    "        print(f\"  {event1} to {event2}: {avg_duration:.3f}s (±{trial_durations.std():.3f})\")\n",
    "    \n",
    "    # Build canonical timeline starting from 0\n",
    "    canonical_events = {'start': 0.0}\n",
    "    current_time = 0.0\n",
    "    \n",
    "    for i in range(len(events_sequence) - 1):\n",
    "        event1, event2 = events_sequence[i], events_sequence[i + 1]\n",
    "        duration_key = f\"{event1}_to_{event2}\"\n",
    "        current_time += durations[duration_key]\n",
    "        canonical_events[event2] = current_time\n",
    "    \n",
    "    print(f\"\\nCanonical timeline: {canonical_events}\")\n",
    "    print(f\"Total canonical duration: {current_time:.3f}s\")\n",
    "    \n",
    "    return canonical_events, filtered_trials\n",
    "\n",
    "def warp_trials_to_canonical_timeline(combined_trials_df, acts_df, spk_z_scores_df, metadata_binned,\n",
    "                                    instance_idx=0, feature_idx=None, \n",
    "                                    maze_conditions=None, hit_target_positions=None):\n",
    "    \"\"\"\n",
    "    Warp filtered trials to a data-driven canonical timeline\n",
    "    \n",
    "    Parameters:\n",
    "    - combined_trials_df: trial metadata with absolute timestamps\n",
    "    - acts_df: feature activation dataframe  \n",
    "    - spk_z_scores_df: z-scored spike count dataframe\n",
    "    - metadata_binned: binned metadata\n",
    "    - instance_idx: SAE instance to analyze\n",
    "    - feature_idx: feature to analyze (if None, will use most active feature)\n",
    "    - maze_conditions: list of maze conditions to include (None for all)\n",
    "    - hit_target_positions: list of target positions to include (None for all)\n",
    "    \n",
    "    Returns:\n",
    "    - warped_data: dict with warped activations and canonical timeline\n",
    "    - feature_idx: feature index analyzed\n",
    "    - top_unit: top unit index\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create canonical timeline from filtered data\n",
    "    canonical_events, filtered_trials = create_canonical_timeline(\n",
    "        combined_trials_df, maze_conditions, hit_target_positions\n",
    "    )\n",
    "    \n",
    "    if canonical_events is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    # If no feature specified, find most active feature for this instance\n",
    "    if feature_idx is None:\n",
    "        instance_acts = acts_df[acts_df['instance_idx'] == instance_idx]\n",
    "        if len(instance_acts) == 0:\n",
    "            print(f\"No activations found for instance {instance_idx}\")\n",
    "            return None, None, None\n",
    "        feature_counts = instance_acts['feature_idx'].value_counts()\n",
    "        feature_idx = feature_counts.index[0]\n",
    "        print(f\"Using most active feature: {feature_idx} ({feature_counts.iloc[0]} activations)\")\n",
    "    \n",
    "    # Get feature activations for this instance/feature\n",
    "    feature_acts = acts_df[\n",
    "        (acts_df['instance_idx'] == instance_idx) & \n",
    "        (acts_df['feature_idx'] == feature_idx)\n",
    "    ].copy()\n",
    "    \n",
    "    # Find top unit for this feature based on z-scores when feature is active\n",
    "    if len(feature_acts) > 0:\n",
    "        feature_active_indices = feature_acts['example_idx'].values\n",
    "        # Get z-scores when feature is active\n",
    "        active_zscores = spk_z_scores_df.iloc[feature_active_indices]\n",
    "        # Calculate mean z-score for each unit when feature is active\n",
    "        unit_mean_zscores = active_zscores.mean(axis=0)\n",
    "        # Find unit with highest mean z-score\n",
    "        top_unit = unit_mean_zscores.idxmax()\n",
    "        top_zscore = unit_mean_zscores[top_unit]\n",
    "        print(f\"Top co-active unit: {top_unit} (mean z-score when feature active: {top_zscore:.3f})\")\n",
    "    else:\n",
    "        top_unit = spk_z_scores_df.mean().idxmax()\n",
    "        print(f\"No feature activations found, using unit with highest mean z-score: {top_unit}\")\n",
    "    \n",
    "    print(f\"Analyzing Feature {feature_idx}, Top Unit: {top_unit}\")\n",
    "    \n",
    "    # Create canonical time axis ending exactly at trial end (no buffer)\n",
    "    canonical_duration = max(canonical_events.values())\n",
    "    canonical_time_axis = np.linspace(0, canonical_duration, int(canonical_duration / 0.05))\n",
    "    \n",
    "    print(f\"Canonical duration: {canonical_duration:.1f}s, {len(canonical_time_axis)} bins\")\n",
    "    \n",
    "    # Storage for warped data\n",
    "    warped_feature_acts = []\n",
    "    warped_unit_acts = []\n",
    "    trial_info_list = []\n",
    "    \n",
    "    print(\"Warping trials...\")\n",
    "    valid_trials = 0\n",
    "    \n",
    "    for trial_idx, trial in filtered_trials.iterrows():\n",
    "        # Get trial event times\n",
    "        required_events = ['start', 'target_on_time', 'go_cue_time', 'move_begins_time', 'move_ends_time', 'end']\n",
    "        trial_event_times = {}\n",
    "        \n",
    "        skip_trial = False\n",
    "        for event in required_events:\n",
    "            if pd.isna(trial[event]):\n",
    "                skip_trial = True\n",
    "                break\n",
    "            trial_event_times[event] = trial[event]\n",
    "        \n",
    "        if skip_trial:\n",
    "            continue\n",
    "        \n",
    "        # Get trial data from binned metadata (ending exactly at trial end)\n",
    "        trial_start = trial['start']\n",
    "        trial_end = trial['end']\n",
    "        \n",
    "        # Find bins for this trial\n",
    "        trial_mask = (metadata_binned.index >= trial_start) & (metadata_binned.index <= trial_end)\n",
    "        \n",
    "        if not trial_mask.any():\n",
    "            continue\n",
    "        \n",
    "        trial_bin_indices = np.where(trial_mask)[0]\n",
    "        trial_timestamps = metadata_binned.index[trial_bin_indices].values\n",
    "        \n",
    "        # Create warping function from original time to canonical time\n",
    "        original_event_times = np.array([trial_event_times[event] for event in required_events])\n",
    "        canonical_event_times = np.array([canonical_events[event] for event in required_events])\n",
    "        \n",
    "        # Warp timestamps using piecewise linear interpolation\n",
    "        warped_timestamps = np.interp(trial_timestamps, original_event_times, canonical_event_times)\n",
    "        \n",
    "        # Extract feature activations for this trial\n",
    "        trial_feature_acts = np.zeros(len(canonical_time_axis))\n",
    "        trial_feature_data = feature_acts[feature_acts['example_idx'].isin(trial_bin_indices)]\n",
    "        \n",
    "        for _, act in trial_feature_data.iterrows():\n",
    "            bin_idx = int(act['example_idx'])\n",
    "            bin_time = metadata_binned.index[bin_idx]\n",
    "            warped_time = np.interp(bin_time, original_event_times, canonical_event_times)\n",
    "            \n",
    "            # Find closest canonical time point\n",
    "            time_idx = np.argmin(np.abs(canonical_time_axis - warped_time))\n",
    "            if 0 <= time_idx < len(trial_feature_acts):\n",
    "                trial_feature_acts[time_idx] = act['activation_value']\n",
    "        \n",
    "        # Extract unit z-scores for this trial (using z-scored data)\n",
    "        trial_unit_acts = np.zeros(len(canonical_time_axis))\n",
    "        \n",
    "        for bin_idx in trial_bin_indices:\n",
    "            bin_idx = int(bin_idx)\n",
    "            if bin_idx < len(spk_z_scores_df):\n",
    "                bin_time = metadata_binned.index[bin_idx]\n",
    "                warped_time = np.interp(bin_time, original_event_times, canonical_event_times)\n",
    "                \n",
    "                time_idx = np.argmin(np.abs(canonical_time_axis - warped_time))\n",
    "                if 0 <= time_idx < len(trial_unit_acts):\n",
    "                    trial_unit_acts[time_idx] = spk_z_scores_df.iloc[bin_idx][top_unit]\n",
    "        \n",
    "        # Store warped data\n",
    "        warped_feature_acts.append(trial_feature_acts)\n",
    "        warped_unit_acts.append(trial_unit_acts)\n",
    "        \n",
    "        # Store trial info\n",
    "        trial_info = trial.copy()\n",
    "        trial_info['trial_idx'] = trial_idx\n",
    "        trial_info_list.append(trial_info)\n",
    "        \n",
    "        valid_trials += 1\n",
    "    \n",
    "    print(f\"Successfully warped {valid_trials} trials\")\n",
    "    \n",
    "    if valid_trials == 0:\n",
    "        return None, feature_idx, top_unit\n",
    "    \n",
    "    # Convert to arrays\n",
    "    warped_feature_acts = np.array(warped_feature_acts)\n",
    "    warped_unit_acts = np.array(warped_unit_acts)\n",
    "    trial_info_df = pd.DataFrame(trial_info_list).reset_index(drop=True)\n",
    "    \n",
    "    warped_data = {\n",
    "        'feature_activations': warped_feature_acts,\n",
    "        'unit_activations': warped_unit_acts,\n",
    "        'trial_info': trial_info_df,\n",
    "        'canonical_time_axis': canonical_time_axis,\n",
    "        'canonical_events': canonical_events\n",
    "    }\n",
    "    \n",
    "    return warped_data, feature_idx, top_unit\n",
    "\n",
    "def plot_warped_trials(warped_data, instance_idx, feature_idx, top_unit,\n",
    "                      highlight_trials=None, max_individual_trials=10,\n",
    "                      smooth_window=None, show_event_regions=True):\n",
    "    \"\"\"\n",
    "    Plot trial-warped feature and unit activations on canonical timeline\n",
    "    \n",
    "    Parameters:\n",
    "    - warped_data: output from warp_trials_to_canonical_timeline\n",
    "    - instance_idx: SAE instance index being analyzed\n",
    "    - feature_idx: feature index being plotted\n",
    "    - top_unit: top unit index  \n",
    "    - highlight_trials: list of trial indices to highlight, or None for random selection\n",
    "    - max_individual_trials: maximum number of individual trials to show\n",
    "    - smooth_window: optional smoothing window size in bins\n",
    "    - show_event_regions: whether to show colored regions for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    if warped_data is None:\n",
    "        print(\"No warped data available\")\n",
    "        return\n",
    "    \n",
    "    feature_acts = warped_data['feature_activations']\n",
    "    unit_acts = warped_data['unit_activations']\n",
    "    trial_info = warped_data['trial_info']\n",
    "    time_axis = warped_data['canonical_time_axis']\n",
    "    canonical_events = warped_data['canonical_events']\n",
    "    \n",
    "    print(f\"Plotting {len(feature_acts)} warped trials\")\n",
    "    \n",
    "    # Apply smoothing if requested\n",
    "    if smooth_window is not None and smooth_window > 1:\n",
    "        feature_acts = uniform_filter1d(feature_acts, size=smooth_window, axis=1)\n",
    "        unit_acts = uniform_filter1d(unit_acts, size=smooth_window, axis=1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    feature_mean = np.mean(feature_acts, axis=0)\n",
    "    feature_sem = np.std(feature_acts, axis=0) / np.sqrt(len(feature_acts))\n",
    "    \n",
    "    unit_mean = np.mean(unit_acts, axis=0)\n",
    "    unit_sem = np.std(unit_acts, axis=0) / np.sqrt(len(unit_acts))\n",
    "    \n",
    "    # Create single plot with two y-axes\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add background regions for different epochs if requested\n",
    "    if show_event_regions:\n",
    "        event_times = list(canonical_events.values())\n",
    "        event_names = list(canonical_events.keys())\n",
    "        colors = ['rgba(255,200,200,0.2)', 'rgba(200,255,200,0.2)', 'rgba(200,200,255,0.2)', \n",
    "                 'rgba(255,255,200,0.2)', 'rgba(255,200,255,0.2)']\n",
    "        \n",
    "        for i in range(len(event_times)-1):\n",
    "            fig.add_vrect(x0=event_times[i], x1=event_times[i+1], fillcolor=colors[i % len(colors)], layer=\"below\", line_width=0)\n",
    "    \n",
    "    # Plot feature activations (primary y-axis)\n",
    "    # SEM band\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=feature_mean + feature_sem, mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip', name='upper_bound'))\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=feature_mean - feature_sem, mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(0,100,80,0.3)', name='Feature ±SEM', showlegend=True))\n",
    "    \n",
    "    # Mean feature line\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=feature_mean, mode='lines', line=dict(color='darkgreen', width=4), name=f'Mean Feature {feature_idx}', yaxis='y'))\n",
    "    \n",
    "    # Individual feature trials\n",
    "    if highlight_trials is not None:\n",
    "        trial_indices = [idx for idx in highlight_trials if idx < len(feature_acts)][:max_individual_trials]\n",
    "        trial_label = \"Selected\"\n",
    "    else:\n",
    "        if len(feature_acts) <= max_individual_trials:\n",
    "            trial_indices = list(range(len(feature_acts)))\n",
    "        else:\n",
    "            trial_indices = np.random.choice(len(feature_acts), max_individual_trials, replace=False)\n",
    "        trial_label = \"Random\"\n",
    "    \n",
    "    for i, trial_idx in enumerate(trial_indices):\n",
    "        trial_info_str = \"\"\n",
    "        if 'maze_condition' in trial_info.columns:\n",
    "            maze_cond = trial_info.iloc[trial_idx]['maze_condition']\n",
    "            trial_info_str += f\"Maze: {maze_cond}\"\n",
    "        if 'hit_target_position' in trial_info.columns:\n",
    "            target_pos = trial_info.iloc[trial_idx]['hit_target_position']\n",
    "            trial_info_str += f\", Target: {target_pos}\"\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=time_axis, y=feature_acts[trial_idx], mode='lines', line=dict(color='rgba(0,150,100,0.5)', width=1), name=f'{trial_label} Feature Trials' if i == 0 else None, showlegend=i == 0, legendgroup='individual_feature_trials', hovertemplate=f'Trial {trial_idx}<br>{trial_info_str}<br>Time: %{{x}}<br>Feature: %{{y}}<extra></extra>', yaxis='y'))\n",
    "    \n",
    "    # Plot unit z-scores (secondary y-axis)\n",
    "    # SEM band for units\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=unit_mean + unit_sem, mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip', yaxis='y2'))\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=unit_mean - unit_sem, mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(0,80,150,0.3)', name='Unit Z-score ±SEM', showlegend=True, yaxis='y2'))\n",
    "    \n",
    "    # Mean unit line\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=unit_mean, mode='lines', line=dict(color='darkblue', width=4), name=f'Mean Unit {top_unit} Z-score', yaxis='y2'))\n",
    "    \n",
    "    # Individual unit trials\n",
    "    for i, trial_idx in enumerate(trial_indices):\n",
    "        trial_info_str = \"\"\n",
    "        if 'maze_condition' in trial_info.columns:\n",
    "            maze_cond = trial_info.iloc[trial_idx]['maze_condition']\n",
    "            trial_info_str += f\"Maze: {maze_cond}\"\n",
    "        if 'hit_target_position' in trial_info.columns:\n",
    "            target_pos = trial_info.iloc[trial_idx]['hit_target_position']\n",
    "            trial_info_str += f\", Target: {target_pos}\"\n",
    "            \n",
    "        fig.add_trace(go.Scatter(x=time_axis, y=unit_acts[trial_idx], mode='lines', line=dict(color='rgba(100,100,255,0.5)', width=1), name=f'{trial_label} Unit Trials' if i == 0 else None, showlegend=i == 0, legendgroup='individual_unit_trials', hovertemplate=f'Trial {trial_idx}<br>{trial_info_str}<br>Time: %{{x}}<br>Unit Z-score: %{{y}}<extra></extra>', yaxis='y2'))\n",
    "    \n",
    "    # Add event lines\n",
    "    for event_name, event_time in canonical_events.items():\n",
    "        fig.add_vline(x=event_time, line_dash=\"dash\", line_color=\"red\", line_width=2, annotation_text=event_name.replace('_', ' ').title(), annotation_position=\"top\")\n",
    "    \n",
    "    # Update layout with dual y-axes\n",
    "    fig.update_layout(\n",
    "        title=f\"Instance {instance_idx} Feature {feature_idx} & Top Unit {top_unit}\",\n",
    "        xaxis_title=\"Canonical Time (s)\",\n",
    "        yaxis=dict(title=\"Feature Activation\", side=\"left\", color=\"darkgreen\"),\n",
    "        yaxis2=dict(title=\"Unit Z-score\", side=\"right\", overlaying=\"y\", color=\"darkblue\"),\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        legend=dict(x=0.02, y=0.98, bgcolor=\"rgba(255,255,255,0.8)\")\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def explore_trial_conditions(combined_trials_df):\n",
    "    \"\"\"Explore available maze conditions and target positions for filtering\"\"\"\n",
    "    print(\"\\nAvailable Trial Conditions\")\n",
    "    \n",
    "    print(\"\\nMaze Conditions:\")\n",
    "    maze_counts = combined_trials_df['maze_condition'].value_counts().sort_index()\n",
    "    for condition, count in maze_counts.items():\n",
    "        print(f\"  {condition}: {count} trials\")\n",
    "    \n",
    "    print(\"\\nHit Target Positions:\")\n",
    "    target_counts = combined_trials_df['hit_target_position'].value_counts()\n",
    "    for position, count in target_counts.items():\n",
    "        print(f\"  {position}: {count} trials\")\n",
    "    \n",
    "    print(f\"\\nTotal trials: {len(combined_trials_df)}\")\n",
    "    \n",
    "    return maze_counts, target_counts\n",
    "\n",
    "# Example usage\n",
    "print(\"Trial Warping Analysis\")\n",
    "# First, explore what conditions are available\n",
    "print(\"Exploring available trial conditions...\")\n",
    "maze_counts, target_counts = explore_trial_conditions(combined_trials_df)\n",
    "\n",
    "# Parameters to customize\n",
    "instance_to_analyze = 0\n",
    "feature_to_analyze = 182\n",
    "# FILTERING OPTIONS - Set these to filter trials\n",
    "maze_conditions_to_include = None\n",
    "target_positions_to_include = None\n",
    "# OTHER PARAMETERS\n",
    "num_example_trials = 8\n",
    "smooth_data = 3\n",
    "show_epochs = True\n",
    "\n",
    "print(f\"\\nRunning analysis with filters:\")\n",
    "print(f\"  Maze conditions: {maze_conditions_to_include}\")\n",
    "print(f\"  Target positions: {target_positions_to_include}\")\n",
    "\n",
    "# Run warping analysis (now including spk_z_scores_df parameter)\n",
    "warped_data, feature_idx, top_unit = warp_trials_to_canonical_timeline(\n",
    "    combined_trials_df, acts_df, spk_z_scores_df, metadata_binned,\n",
    "    instance_idx=instance_to_analyze, \n",
    "    feature_idx=feature_to_analyze,\n",
    "    maze_conditions=maze_conditions_to_include,\n",
    "    hit_target_positions=target_positions_to_include\n",
    ")\n",
    "\n",
    "if warped_data is not None:\n",
    "    print(f\"\\nResults\")\n",
    "    print(f\"Analyzed Feature: {feature_idx}\")\n",
    "    print(f\"Top Co-active Unit: {top_unit}\")\n",
    "    \n",
    "    # Create the warped trial plot\n",
    "    plot_warped_trials(\n",
    "        warped_data, instance_to_analyze, feature_idx, top_unit,\n",
    "        highlight_trials=None,\n",
    "        max_individual_trials=num_example_trials,\n",
    "        smooth_window=smooth_data,\n",
    "        show_event_regions=show_epochs\n",
    "    )\n",
    "else:\n",
    "    print(\"Failed to warp trials - check your parameters and data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive UI\"\"\"\n",
    "\n",
    "# Helper to map a base variable and its type to the metadata_binned column name\n",
    "def _bvar_name(var_name, var_type):\n",
    "    return f\"{var_name}_binned\" if var_type == 'continuous' else var_name\n",
    "\n",
    "# Mode selector: preset vs manual\n",
    "mode_radio = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Preset (from results table)', 'preset'),\n",
    "        ('Manual selection',       'manual')\n",
    "    ],\n",
    "    value='preset',\n",
    "    description=''\n",
    ")\n",
    "\n",
    "# Build the preset dropdown with full metrics\n",
    "preset_entries = []\n",
    "for _, r in results.iterrows():\n",
    "    bvar = _bvar_name(r.variable, r.variable_type)\n",
    "    if bvar not in metadata_binned.columns:\n",
    "        continue\n",
    "    label = (\n",
    "        f\"Inst:{int(r.instance_idx)} | \"\n",
    "        f\"Feat:{int(r.feature_idx)} | \"\n",
    "        f\"Var:{bvar} | \"\n",
    "        f\"Val:{r['value']} | \"\n",
    "        f\"FracDuring:{r.activation_frac_during:.3f} | \"\n",
    "        f\"FracOutside:{r.activation_frac_outside:.3f} | \"\n",
    "        f\"ActRatio:{r.activation_ratio:.3f} | \"\n",
    "        f\"RateProp:{r.rate_proportion:.3f}\"\n",
    "    )\n",
    "    preset_entries.append((label, (int(r.instance_idx), int(r.feature_idx), bvar)))\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=preset_entries,\n",
    "    description='Select Result:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "preset_box = widgets.VBox([preset_dropdown])\n",
    "\n",
    "# Manual instance & feature selection\n",
    "instance_dropdown = widgets.Dropdown(\n",
    "    options=sorted(acts_df['instance_idx'].unique()),\n",
    "    description='Instance:'\n",
    ")\n",
    "\n",
    "feature_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Feature:'\n",
    ")\n",
    "\n",
    "def _on_instance_change(change):\n",
    "    inst = change['new']\n",
    "    feats = sorted(\n",
    "        acts_df.loc[acts_df['instance_idx'] == inst, 'feature_idx'].unique()\n",
    "    )\n",
    "    feature_dropdown.options = feats\n",
    "\n",
    "instance_dropdown.observe(_on_instance_change, names='value')\n",
    "_on_instance_change({'new': instance_dropdown.value})\n",
    "\n",
    "manual_box = widgets.VBox([instance_dropdown, feature_dropdown])\n",
    "manual_box.layout.display = 'none'\n",
    "\n",
    "# Maze condition and target position filters\n",
    "maze_options = sorted(combined_trials_df['maze_condition'].dropna().unique())\n",
    "maze_dropdown = widgets.Dropdown(\n",
    "    options=[None] + maze_options,\n",
    "    description='Maze cond:'\n",
    ")\n",
    "\n",
    "target_positions = combined_trials_df['hit_target_position'].dropna().unique()\n",
    "target_strs = [str(pos) for pos in target_positions]\n",
    "target_dropdown = widgets.Dropdown(\n",
    "    options=[None] + target_strs,\n",
    "    description='Target pos:'\n",
    ")\n",
    "\n",
    "# Toggle preset vs manual\n",
    "def _on_mode_change(change):\n",
    "    if change['new'] == 'preset':\n",
    "        preset_box.layout.display = ''\n",
    "        manual_box.layout.display = 'none'\n",
    "    else:\n",
    "        preset_box.layout.display = 'none'\n",
    "        manual_box.layout.display = ''\n",
    "\n",
    "mode_radio.observe(_on_mode_change, names='value')\n",
    "_on_mode_change({'new': mode_radio.value})\n",
    "\n",
    "# Generate button and output area\n",
    "generate_btn = widgets.Button(description='Generate Plot', button_style='info')\n",
    "out = widgets.Output()\n",
    "\n",
    "def _on_generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        if mode_radio.value == 'preset':\n",
    "            inst, feat, _ = preset_dropdown.value\n",
    "        else:\n",
    "            inst = instance_dropdown.value\n",
    "            feat = feature_dropdown.value\n",
    "\n",
    "        maze = [maze_dropdown.value] if maze_dropdown.value is not None else None\n",
    "        tgt = target_dropdown.value\n",
    "        hit_positions = [eval(tgt)] if tgt is not None else None\n",
    "\n",
    "        warped_data, used_feat, top_unit = warp_trials_to_canonical_timeline(\n",
    "            combined_trials_df,\n",
    "            acts_df,\n",
    "            spk_z_scores_df,\n",
    "            metadata_binned,\n",
    "            instance_idx=inst,\n",
    "            feature_idx=feat,\n",
    "            maze_conditions=maze,\n",
    "            hit_target_positions=hit_positions\n",
    "        )\n",
    "\n",
    "        if warped_data is not None:\n",
    "            plot_warped_trials(warped_data, inst, used_feat, top_unit)\n",
    "        else:\n",
    "            print(\"No data to display. Check your selections.\")\n",
    "\n",
    "generate_btn.on_click(_on_generate)\n",
    "\n",
    "# Assemble and display the UI\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Warp Trials Visualization</h2>\"),\n",
    "    mode_radio,\n",
    "    preset_box,\n",
    "    manual_box,\n",
    "    maze_dropdown,\n",
    "    target_dropdown,\n",
    "    generate_btn,\n",
    "    out\n",
    "])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualisation functions for all feature associations\"\"\"\n",
    "\n",
    "def plot_feature_tuning(\n",
    "    acts_df: pd.DataFrame,\n",
    "    spk_z_scores_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    variable: str,\n",
    "    instance_idx: int,\n",
    "    feature_idx: int\n",
    "):\n",
    "    \"\"\"Visualizes SAE feature tuning to metadata variables.\"\"\"\n",
    "    # Get feature activations\n",
    "    feature_acts = acts_df[(acts_df['instance_idx'] == instance_idx) & (acts_df['feature_idx'] == feature_idx)]\n",
    "    \n",
    "    # Find top/bottom co-active units\n",
    "    if len(feature_acts) > 0:\n",
    "        feature_active_indices = feature_acts['example_idx'].values\n",
    "        neuron_mean_zscores = spk_z_scores_df.iloc[feature_active_indices].mean(axis=0)\n",
    "        top_unit = neuron_mean_zscores.idxmax()\n",
    "        bottom_unit = neuron_mean_zscores.idxmin()\n",
    "        top_zscore = neuron_mean_zscores[top_unit]\n",
    "        bottom_zscore = neuron_mean_zscores[bottom_unit]\n",
    "        print(f\"Top co-active unit: {top_unit} (z-score: {top_zscore:.3f})\")\n",
    "        print(f\"Bottom co-active unit: {bottom_unit} (z-score: {bottom_zscore:.3f})\")\n",
    "    else:\n",
    "        print(\"No feature activations found for this instance/feature.\")\n",
    "        return\n",
    "    \n",
    "    # Create complete dataset with zeros for inactive features\n",
    "    all_examples = pd.DataFrame({'example_idx': range(len(metadata_binned))})\n",
    "    all_examples = all_examples.merge(feature_acts[['example_idx', 'activation_value']], on='example_idx', how='left').fillna(0)\n",
    "    \n",
    "    if all_examples.empty:\n",
    "        print(f\"⚠️ No examples found for Instance {instance_idx}, Feature {feature_idx}. Cannot generate plot.\")\n",
    "        return\n",
    "    \n",
    "    # Get metadata and z-scores for all examples\n",
    "    metadata_slice = metadata_binned[[variable]].iloc[all_examples['example_idx']]\n",
    "    top_unit_slice = spk_z_scores_df[[top_unit]].iloc[all_examples['example_idx']]\n",
    "    bottom_unit_slice = spk_z_scores_df[[bottom_unit]].iloc[all_examples['example_idx']]\n",
    "    \n",
    "    # Create plotting dataframe\n",
    "    data_df = metadata_slice.reset_index(drop=True)\n",
    "    data_df['activation_value'] = all_examples['activation_value'].reset_index(drop=True)\n",
    "    data_df['top_unit_zscore'] = top_unit_slice[top_unit].reset_index(drop=True)\n",
    "    data_df['bottom_unit_zscore'] = bottom_unit_slice[bottom_unit].reset_index(drop=True)\n",
    "    data_df = data_df.dropna(subset=[variable])\n",
    "    \n",
    "    if data_df.empty:\n",
    "        print(f\"⚠️ No matching metadata found for feature bins. Cannot generate plot.\")\n",
    "        return\n",
    "    \n",
    "    # Check if data is interval type\n",
    "    try:\n",
    "        is_interval_data = pd.api.types.is_interval_dtype(data_df[variable].cat.categories)\n",
    "    except AttributeError:\n",
    "        is_interval_data = False\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    stats_df = data_df.groupby(variable).agg({\n",
    "        'activation_value': ['mean', 'sem'],\n",
    "        'top_unit_zscore': ['mean', 'sem'],\n",
    "        'bottom_unit_zscore': ['mean', 'sem']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    stats_df.columns = [variable, 'feature_mean', 'feature_sem', 'top_unit_mean', 'top_unit_sem', 'bottom_unit_mean', 'bottom_unit_sem']\n",
    "    \n",
    "    # Calculate rate proportions\n",
    "    if not feature_acts.empty:\n",
    "        condition_masks = {condition: metadata_binned[variable] == condition for condition in stats_df[variable]}\n",
    "        active_example_set = set(feature_acts['example_idx'])\n",
    "        \n",
    "        rate_props = []\n",
    "        for _, row in stats_df.iterrows():\n",
    "            condition = row[variable]\n",
    "            condition_mask = condition_masks[condition]\n",
    "            \n",
    "            condition_example_idxs = np.where(condition_mask)[0]\n",
    "            condition_activations = len(active_example_set.intersection(condition_example_idxs))\n",
    "            activation_frac_during = condition_activations / len(condition_example_idxs) if len(condition_example_idxs) > 0 else 0\n",
    "            \n",
    "            non_condition_example_idxs = np.where(~condition_mask)[0]\n",
    "            non_condition_activations = len(active_example_set.intersection(non_condition_example_idxs))\n",
    "            activation_frac_outside = non_condition_activations / len(non_condition_example_idxs) if len(non_condition_example_idxs) > 0 else 0\n",
    "            \n",
    "            rate_proportion = activation_frac_during / (activation_frac_during + activation_frac_outside) if (activation_frac_during + activation_frac_outside) > 0 else 0\n",
    "            rate_props.append(rate_proportion)\n",
    "        \n",
    "        stats_df['rate_proportion'] = rate_props\n",
    "    else:\n",
    "        stats_df['rate_proportion'] = 0\n",
    "    \n",
    "    # Calculate z-score stats for bar plot\n",
    "    if len(feature_acts) > 0:\n",
    "        zscore_stats = spk_z_scores_df.iloc[feature_active_indices].agg(['mean', 'sem']).T\n",
    "        zscore_stats.columns = ['mean_zscore', 'sem_zscore']\n",
    "        zscore_stats = zscore_stats.reset_index()\n",
    "        zscore_stats.columns = ['neuron', 'mean_zscore', 'sem_zscore']\n",
    "    else:\n",
    "        zscore_stats = pd.DataFrame({'neuron': spk_z_scores_df.columns, 'mean_zscore': 0, 'sem_zscore': 0})\n",
    "    \n",
    "    # Create plots based on variable type\n",
    "    if 'angle' in variable:  # Polar plot\n",
    "        stats_df['theta'] = stats_df[variable].apply(lambda x: x.mid if isinstance(x, pd.Interval) else x)\n",
    "        stats_df = stats_df.sort_values('theta')\n",
    "        plot_df = pd.concat([stats_df, stats_df.head(1)], ignore_index=True)\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=4,\n",
    "            specs=[[{\"type\": \"polar\"}, {\"type\": \"polar\"}, {\"type\": \"polar\"}, {\"type\": \"polar\"}],\n",
    "                   [{\"type\": \"xy\", \"colspan\": 4}, None, None, None]],\n",
    "            horizontal_spacing=0.1,\n",
    "            vertical_spacing=0.15,\n",
    "            subplot_titles=[\"Feature Activation\", \"Top Unit Z-score\", \"Bottom Unit Z-score\", \"Rate Proportion\", \"Mean Z-scores when Feature Active\"]\n",
    "        )\n",
    "        \n",
    "        # Feature activation polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['feature_mean'] + plot_df['feature_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['feature_mean'] - plot_df['feature_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), fill='tonext', fillcolor='rgba(220,20,60,0.2)', name='Feature ±SEM'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['feature_mean'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='crimson', width=3), name='Feature Activation'), row=1, col=1)\n",
    "        \n",
    "        # Top unit polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['top_unit_mean'] + plot_df['top_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), showlegend=False), row=1, col=2)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['top_unit_mean'] - plot_df['top_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), fill='tonext', fillcolor='rgba(0,0,139,0.2)', name='Top Unit ±SEM'), row=1, col=2)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['top_unit_mean'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='darkblue', width=3), name='Top Unit Z-score'), row=1, col=2)\n",
    "        \n",
    "        # Bottom unit polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['bottom_unit_mean'] + plot_df['bottom_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), showlegend=False), row=1, col=3)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['bottom_unit_mean'] - plot_df['bottom_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), fill='tonext', fillcolor='rgba(255,165,0,0.2)', name='Bottom Unit ±SEM'), row=1, col=3)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['bottom_unit_mean'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='orange', width=3), name='Bottom Unit Z-score'), row=1, col=3)\n",
    "        \n",
    "        # Rate proportion polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['rate_proportion'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='green', width=3), name='Rate Proportion'), row=1, col=4)\n",
    "        \n",
    "        # Z-score bar plot\n",
    "        fig.add_trace(go.Bar(x=zscore_stats['neuron'], y=zscore_stats['mean_zscore'], error_y=dict(type='data', array=zscore_stats['sem_zscore']), marker_color='purple', marker_line_width=0, opacity=0.7, name='Mean Z-score'), row=2, col=1)\n",
    "        \n",
    "        # Create tick labels\n",
    "        if 'movement_angle' not in variable:\n",
    "            tick_labels = [f\"{int(round(theta))}°\" for theta in stats_df['theta']]\n",
    "        else:\n",
    "            tick_labels = [f\"{int(interval.mid)}°\" if hasattr(interval, 'mid') else str(interval) for interval in stats_df[variable]]    \n",
    "        \n",
    "        fig.update_layout(title=f\"Instance {instance_idx} Feature {feature_idx} & Top Unit {top_unit} & Bottom Unit {bottom_unit}\", showlegend=True, height=800, width=1600, margin=dict(t=80, b=60, l=50, r=50))\n",
    "        \n",
    "        # Update polar plots\n",
    "        rotation = 195 if 'movement_angle' in variable else 0\n",
    "        tickvals = stats_df['theta'].tolist() if 'movement_angle' in variable else [(angle % 360) for angle in stats_df['theta'].tolist()]\n",
    "        for i in range(1, 5):\n",
    "            polar_key = f'polar{i if i > 1 else \"\"}'\n",
    "            fig.update_layout(**{polar_key: dict(angularaxis=dict(direction=\"counterclockwise\", rotation=rotation, tickvals=tickvals, ticktext=tick_labels, tickfont=dict(size=10)), radialaxis=dict(range=[0, None]))})\n",
    "\n",
    "        fig.update_xaxes(title_text=\"Neuron\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Mean Z-score\", row=2, col=1)\n",
    "            \n",
    "    else:  # Linear plot\n",
    "        fig = make_subplots(rows=2, cols=4, specs=[[{}, {}, {}, {}], [{\"colspan\": 4}, None, None, None]], subplot_titles=[\"Feature Activation\", \"Top Unit Z-score\", \"Bottom Unit Z-score\", \"Rate Proportion\", \"Mean Z-scores when Feature Active\"], horizontal_spacing=0.1, vertical_spacing=0.3)\n",
    "        \n",
    "        # Setup x-axis labels\n",
    "        if is_interval_data:\n",
    "            x_axis_labels = stats_df[variable].apply(lambda x: str(x))\n",
    "            stats_df = stats_df.sort_values(by=variable)\n",
    "        else:\n",
    "            x_axis_labels = stats_df[variable].astype(str)\n",
    "        \n",
    "        # Plot based on data type\n",
    "        if is_interval_data:\n",
    "            # Line plots for continuous data\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['feature_mean'] + stats_df['feature_sem'], mode='lines', line_color='rgba(0,0,0,0)', showlegend=False), row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['feature_mean'] - stats_df['feature_sem'], mode='lines', line_color='rgba(0,0,0,0)', fill='tonexty', fillcolor='rgba(220,20,60,0.2)', name='Feature ±SEM'), row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['feature_mean'], mode='lines+markers', line_color='crimson', name='Feature Activation'), row=1, col=1)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['top_unit_mean'] + stats_df['top_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', showlegend=False), row=1, col=2)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['top_unit_mean'] - stats_df['top_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', fill='tonexty', fillcolor='rgba(0,0,139,0.2)', name='Top Unit ±SEM'), row=1, col=2)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['top_unit_mean'], mode='lines+markers', line_color='darkblue', name='Top Unit Z-score'), row=1, col=2)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['bottom_unit_mean'] + stats_df['bottom_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', showlegend=False), row=1, col=3)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['bottom_unit_mean'] - stats_df['bottom_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', fill='tonexty', fillcolor='rgba(255,165,0,0.2)', name='Bottom Unit ±SEM'), row=1, col=3)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['bottom_unit_mean'], mode='lines+markers', line_color='orange', name='Bottom Unit Z-score'), row=1, col=3)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['rate_proportion'], mode='lines+markers', line=dict(color='green', width=3), name='Rate Proportion'), row=1, col=4)\n",
    "        else:\n",
    "            # Bar plots for categorical data\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['feature_mean'], error_y=dict(type='data', array=stats_df['feature_sem']), marker_color='crimson', marker_line_width=0, opacity=0.7, name='Feature Activation'), row=1, col=1)\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['top_unit_mean'], error_y=dict(type='data', array=stats_df['top_unit_sem']), marker_color='darkblue', marker_line_width=0, opacity=0.7, name='Top Unit Z-score'), row=1, col=2)\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['bottom_unit_mean'], error_y=dict(type='data', array=stats_df['bottom_unit_sem']), marker_color='orange', marker_line_width=0, opacity=0.7, name='Bottom Unit Z-score'), row=1, col=3)\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['rate_proportion'], marker_color='green', marker_line_width=0, opacity=0.7, name='Rate Proportion'), row=1, col=4)\n",
    "        \n",
    "        # Z-score bar plot\n",
    "        fig.add_trace(go.Bar(x=zscore_stats['neuron'], y=zscore_stats['mean_zscore'], error_y=dict(type='data', array=zscore_stats['sem_zscore']), marker_color='purple', marker_line_width=0, opacity=0.7, name='Mean Z-score'), row=2, col=1)\n",
    "        \n",
    "        fig.update_layout(title=f\"Instance {instance_idx} Feature {feature_idx} & Top Unit {top_unit} & Bottom Unit {bottom_unit}\", height=800, width=1600, showlegend=True, margin=dict(t=80, b=60, l=50, r=50))\n",
    "        \n",
    "        # Update axes\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=1)\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=2)\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=3)\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=4)\n",
    "        fig.update_xaxes(title_text=\"Neuron\", row=2, col=1)\n",
    "        \n",
    "        fig.update_yaxes(title_text=\"Feature Activation\", color=\"crimson\", range=[0, None], row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Top Unit Z-score\", color=\"darkblue\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Bottom Unit Z-score\", color=\"orange\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"Rate Proportion\", range=[0, None], color=\"green\", row=1, col=4)\n",
    "        fig.update_yaxes(title_text=\"Mean Z-score\", row=2, col=1)\n",
    "        \n",
    "        fig.update_yaxes(rangemode='tozero', row=1, col=1)\n",
    "        fig.update_yaxes(rangemode='tozero', row=1, col=4)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Find go_cue feature and plot\n",
    "event_feature = results[results['value'] == 'go_cue'].sort_values('activation_ratio', ascending=False).iloc[0]\n",
    "display(event_feature)\n",
    "plot_feature_tuning(acts_df=acts_df, spk_z_scores_df=spk_z_scores_df, metadata_binned=metadata_binned, variable='event', instance_idx=int(event_feature['instance_idx']), feature_idx=int(event_feature['feature_idx']))\n",
    "\n",
    "# Find movement_angle feature and plot  \n",
    "move_angle_feature = results[results['variable'] == 'movement_angle'].sort_values('activation_ratio', ascending=False).iloc[0]\n",
    "display(move_angle_feature)\n",
    "plot_feature_tuning(acts_df=acts_df, spk_z_scores_df=spk_z_scores_df, metadata_binned=metadata_binned, variable='movement_angle_binned', instance_idx=int(move_angle_feature['instance_idx']), feature_idx=int(move_angle_feature['feature_idx']))\n",
    "\n",
    "# Find vel_magnitude feature and plot\n",
    "velocity_feature = results[results['variable'] == 'vel_magnitude'].sort_values('activation_ratio', ascending=False).iloc[0]\n",
    "display(velocity_feature)\n",
    "plot_feature_tuning(acts_df=acts_df, spk_z_scores_df=spk_z_scores_df, metadata_binned=metadata_binned, variable='vel_magnitude_binned', instance_idx=int(velocity_feature['instance_idx']), feature_idx=int(velocity_feature['feature_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive UI\"\"\"\n",
    "\n",
    "# Helper to map a base variable and its type to the metadata_binned column name\n",
    "def _bvar_name(var_name, var_type):\n",
    "    return f\"{var_name}_binned\" if var_type == 'continuous' else var_name\n",
    "\n",
    "# Selector for preset vs manual mode\n",
    "mode_radio = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Preset (from results table)', 'preset'),\n",
    "        ('Manual selection', 'manual')\n",
    "    ],\n",
    "    value='preset',\n",
    "    description=''\n",
    ")\n",
    "\n",
    "# Build the preset dropdown and store (instance, feature, variable) as the value\n",
    "preset_entries = []\n",
    "for _, r in results.iterrows():\n",
    "    bvar = _bvar_name(r.variable, r.variable_type)\n",
    "    if bvar not in metadata_binned.columns:\n",
    "        continue  # Skip variables you haven’t binned\n",
    "    label = (\n",
    "        f\"Inst:{int(r.instance_idx)} | \"\n",
    "        f\"Feat:{int(r.feature_idx)} | \"\n",
    "        f\"Var:{bvar} | \"\n",
    "        f\"Val:{r['value']} | \"\n",
    "        f\"FracDuring:{r.activation_frac_during:.3f} | \"\n",
    "        f\"FracOutside:{r.activation_frac_outside:.3f} | \"\n",
    "        f\"ActRatio:{r.activation_ratio:.3f} | \"\n",
    "        f\"RateProp:{r.rate_proportion:.3f}\"\n",
    "    )\n",
    "    preset_entries.append((label, (int(r.instance_idx), int(r.feature_idx), bvar)))\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=preset_entries,\n",
    "    description='Select Result:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "preset_box = widgets.VBox([preset_dropdown])\n",
    "\n",
    "# Build the manual selection box\n",
    "instance_dropdown = widgets.Dropdown(\n",
    "    options=sorted(acts_df['instance_idx'].unique()),\n",
    "    description='Instance:'\n",
    ")\n",
    "\n",
    "# Only include properly binned variables\n",
    "unique_vars = results[['variable','variable_type']].drop_duplicates()\n",
    "manual_var_options = [\n",
    "    (_bvar_name(v, t), _bvar_name(v, t))\n",
    "    for v, t in unique_vars.values\n",
    "    if _bvar_name(v, t) in metadata_binned.columns\n",
    "]\n",
    "variable_dropdown = widgets.Dropdown(\n",
    "    options=manual_var_options,\n",
    "    description='Variable:'\n",
    ")\n",
    "\n",
    "# Precompute bvar column on results for filtering feature indices\n",
    "results['bvar'] = results.apply(\n",
    "    lambda r: _bvar_name(r.variable, r.variable_type), axis=1\n",
    ")\n",
    "\n",
    "# Replace fixed feature input with a dropdown that updates based on variable\n",
    "feature_dropdown = widgets.Dropdown(\n",
    "    description='Feature:',\n",
    "    options=[]\n",
    ")\n",
    "\n",
    "# Callback to repopulate feature options when variable changes\n",
    "def _on_var_change(change):\n",
    "    sel_var = change['new']\n",
    "    feats = sorted(\n",
    "        results.loc[results['bvar'] == sel_var, 'feature_idx'].unique()\n",
    "    )\n",
    "    feature_dropdown.options = feats\n",
    "\n",
    "variable_dropdown.observe(_on_var_change, names='value')\n",
    "_on_var_change({'new': variable_dropdown.value})\n",
    "\n",
    "manual_box = widgets.VBox([\n",
    "    instance_dropdown,\n",
    "    variable_dropdown,\n",
    "    feature_dropdown\n",
    "])\n",
    "manual_box.layout.display = 'none'  # Start hidden\n",
    "\n",
    "# Buttons for generating or clearing the plot, and an output area\n",
    "generate_btn = widgets.Button(description='Generate Plot', button_style='info')\n",
    "clear_btn    = widgets.Button(description='Clear',         button_style='warning')\n",
    "button_box   = widgets.HBox([generate_btn, clear_btn])\n",
    "out = widgets.Output()\n",
    "\n",
    "# Toggle between preset and manual views\n",
    "def _on_mode_change(change):\n",
    "    if change['new'] == 'preset':\n",
    "        preset_box.layout.display = ''\n",
    "        manual_box.layout.display = 'none'\n",
    "    else:\n",
    "        preset_box.layout.display = 'none'\n",
    "        manual_box.layout.display = ''\n",
    "\n",
    "mode_radio.observe(_on_mode_change, names='value')\n",
    "\n",
    "# Callback to generate the tuning plot\n",
    "def _on_generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        if mode_radio.value == 'preset':\n",
    "            inst, feat, var = preset_dropdown.value\n",
    "        else:\n",
    "            inst = instance_dropdown.value\n",
    "            feat = feature_dropdown.value\n",
    "            var  = variable_dropdown.value\n",
    "\n",
    "        plot_feature_tuning(\n",
    "            acts_df=acts_df,\n",
    "            spk_z_scores_df=spk_z_scores_df,\n",
    "            metadata_binned=metadata_binned,\n",
    "            variable=var,\n",
    "            instance_idx=inst,\n",
    "            feature_idx=feat\n",
    "        )\n",
    "\n",
    "# Callback to clear the output\n",
    "def _on_clear(_):\n",
    "    out.clear_output()\n",
    "\n",
    "generate_btn.on_click(_on_generate)\n",
    "clear_btn.on_click(_on_clear)\n",
    "\n",
    "# Assemble and display the UI\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>SAE Feature Visualization</h2>\"),\n",
    "    mode_radio,\n",
    "    preset_box,\n",
    "    manual_box,\n",
    "    button_box,\n",
    "    out\n",
    "])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 0\n",
    "feature = 474\n",
    "feature_activity = acts_df[\n",
    "    (acts_df[\"instance_idx\"] == instance) &\n",
    "    (acts_df[\"feature_idx\"] == feature)\n",
    "]\n",
    "display(feature_activity.sort_values(\"example_idx\"))\n",
    "feat_act_ts = metadata.iloc[feature_activity[\"example_idx\"]].index\n",
    "print(feat_act_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col_name = spk_cts_df.iloc[feature_activity[\"example_idx\"]].mean().idxmax()\n",
    "unit_activity = spk_cts_df[max_col_name].values.cumsum()\n",
    "unit_activity = unit_activity / unit_activity[-1]\n",
    "print(unit_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_binned.dtypes, metadata_binned.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize feature activity vs. movement blocks from metadata.\"\"\"\n",
    "\n",
    "# Parameters\n",
    "instance = 0\n",
    "feature = 474\n",
    "\n",
    "# Get feature activity\n",
    "acts = acts_df[(acts_df[\"instance_idx\"] == instance) & (acts_df[\"feature_idx\"] == feature)]\n",
    "feat_act_ts = metadata_binned.iloc[acts[\"example_idx\"]].index\n",
    "\n",
    "# Build normalized cumulative feature activity\n",
    "feat_timeline = np.zeros(len(metadata_binned))\n",
    "feat_timeline[acts[\"example_idx\"].values] = 1\n",
    "feat_cumsum = np.cumsum(feat_timeline)\n",
    "feat_norm = feat_cumsum / feat_cumsum[-1] if feat_cumsum[-1] > 0 else feat_cumsum\n",
    "\n",
    "# Get top unit activity\n",
    "top_unit = spk_cts_df.iloc[acts[\"example_idx\"]].mean().idxmax()\n",
    "unit_cumsum = spk_cts_df[top_unit].values.cumsum()\n",
    "unit_norm = unit_cumsum / unit_cumsum[-1]\n",
    "\n",
    "# Calculate speed and normalize\n",
    "speed = np.sqrt(metadata_binned[\"vel_x\"]**2 + metadata_binned[\"vel_y\"]**2)\n",
    "speed_norm = speed / speed.max()\n",
    "\n",
    "# Create subplots: feature/unit/speed on top, movement block on bottom\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    row_heights=[0.8, 0.2]\n",
    ")\n",
    "\n",
    "# Top traces: Feature activity, unit activity, speed\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=metadata_binned.index,\n",
    "        y=feat_norm,\n",
    "        mode=\"lines\",\n",
    "        name=\"Feature Activity\",\n",
    "        line=dict(color=\"black\")\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=metadata_binned.index[: len(unit_norm)],\n",
    "        y=unit_norm,\n",
    "        mode=\"lines\",\n",
    "        name=\"Top Unit Activity\",\n",
    "        line=dict(color=\"blue\"),\n",
    "        opacity=0.5\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "subsample = max(1, len(metadata_binned) // 5000)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=metadata_binned.index[::subsample],\n",
    "        y=speed_norm[::subsample],\n",
    "        mode=\"lines\",\n",
    "        name=\"Running Speed\",\n",
    "        line=dict(color=\"red\"),\n",
    "        opacity=0.3\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Bottom trace: blocks where is_moving == True\n",
    "# Assumes metadata_binned has an 'is_moving' boolean column\n",
    "moving_int = metadata_binned[\"is_moving\"].astype(int)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=metadata_binned.index,\n",
    "        y=moving_int,\n",
    "        mode=\"none\",\n",
    "        fill='tozeroy',\n",
    "        name='Is Moving',\n",
    "        fillcolor='rgba(0,200,0,0.3)'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Normalized Cumulative Count\",\n",
    "    tickfont=dict(size=17),\n",
    "    titlefont=dict(size=18),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Moving\",\n",
    "    showticklabels=False,\n",
    "    range=[0,1],\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Time (timestamp)\",\n",
    "    tickfont=dict(size=17),\n",
    "    titlefont=dict(size=19),\n",
    "    tickangle=-45,\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Layout tweaks\n",
    "total_act = feat_cumsum[-1] if len(feat_cumsum) > 0 else 0\n",
    "fig.update_layout(\n",
    "    title_text=f\"Instance {instance}, Feature {feature} - Total Activations: {total_act}\",\n",
    "    margin=dict(l=40, r=20, t=40, b=20),\n",
    "    legend=dict(\n",
    "        y=0.99,\n",
    "        x=0.01,\n",
    "        font=dict(size=13),\n",
    "        itemsizing='constant',\n",
    "        bgcolor=\"rgba(255,255,255,0.3)\"\n",
    "    ),\n",
    "    title=dict(font=dict(size=23)),\n",
    "    height=600,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Visualize feature activity vs. events from metadata.\"\"\"\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.express as px\n",
    "\n",
    "# # Parameters\n",
    "# instance = 0\n",
    "# feature = 151\n",
    "\n",
    "# # Get feature activity\n",
    "# feature_activity = acts_df[\n",
    "#     (acts_df[\"instance_idx\"] == instance) &\n",
    "#     (acts_df[\"feature_idx\"] == feature)\n",
    "# ]\n",
    "# feat_act_ts = metadata_binned.iloc[feature_activity[\"example_idx\"]].index\n",
    "\n",
    "# # Create feature activity timeline (cumulative normalized count)\n",
    "# feat_activity_timeline = np.zeros(len(metadata_binned))\n",
    "# feat_indices = feature_activity[\"example_idx\"].values\n",
    "# feat_activity_timeline[feat_indices] = 1\n",
    "# feat_activity_cumsum = np.cumsum(feat_activity_timeline)\n",
    "# feat_activity_norm = feat_activity_cumsum / feat_activity_cumsum[-1] if feat_activity_cumsum[-1] > 0 else feat_activity_cumsum\n",
    "\n",
    "# # Get top unit activity\n",
    "# max_col_name = spk_cts_df.iloc[feature_activity[\"example_idx\"]].mean().idxmax()\n",
    "# unit_activity = spk_cts_df[max_col_name].values.cumsum()\n",
    "# unit_activity = unit_activity / unit_activity[-1]\n",
    "\n",
    "# # Calculate speed from vel_x, vel_y\n",
    "# speed = np.sqrt(metadata_binned[\"vel_x\"]**2 + metadata_binned[\"vel_y\"]**2)\n",
    "# speed_normalized = speed / speed.max()\n",
    "\n",
    "# # Get events data - filter out empty string events\n",
    "# events_df = metadata_binned[\n",
    "#     (metadata_binned[\"event\"].notna()) & \n",
    "#     (metadata_binned[\"event\"] != '') & \n",
    "#     (metadata_binned[\"event\"].str.strip() != '')\n",
    "# ].copy()\n",
    "# event_types = events_df[\"event\"].unique()\n",
    "\n",
    "# # Create subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=2, \n",
    "#     cols=1,\n",
    "#     shared_xaxes=True,\n",
    "#     vertical_spacing=0.05,\n",
    "#     row_heights=[0.8, 0.2]\n",
    "# )\n",
    "\n",
    "# # Add feature activity (top plot)\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=metadata_binned.index,\n",
    "#         y=feat_activity_norm,\n",
    "#         mode=\"lines\",\n",
    "#         name=\"Feature Activity\",\n",
    "#         line=dict(color=\"black\")\n",
    "#     ),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # Add top unit activity (top plot)\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=metadata_binned.index[:len(unit_activity)],\n",
    "#         y=unit_activity,\n",
    "#         mode=\"lines\",\n",
    "#         name=\"Top Unit Activity\",\n",
    "#         line=dict(color=\"blue\"),\n",
    "#         opacity=0.5,\n",
    "#     ),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # Add speed (top plot) - subsample for performance\n",
    "# subsample_rate = max(1, len(metadata_binned) // 5000)  # Adjust for performance\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=metadata_binned.index[::subsample_rate],\n",
    "#         y=speed_normalized.fillna(0)[::subsample_rate],\n",
    "#         mode=\"lines\",\n",
    "#         name=\"Running Speed\",\n",
    "#         line=dict(color=\"red\"),\n",
    "#         opacity=0.3\n",
    "#     ),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # Add events (bottom plot) - each event type in its own row\n",
    "# # First, get all individual event types (including from combined events)\n",
    "# all_individual_events = set()\n",
    "# for event in event_types:\n",
    "#     if ',' in event:\n",
    "#         # Split combined events\n",
    "#         individual_events = [e.strip() for e in event.split(',')]\n",
    "#         all_individual_events.update(individual_events)\n",
    "#     else:\n",
    "#         all_individual_events.add(event)\n",
    "\n",
    "# all_individual_events = sorted(list(all_individual_events))\n",
    "\n",
    "# # Create colors for individual events\n",
    "# colors = px.colors.qualitative.Plotly\n",
    "# event_colors = {event: colors[i % len(colors)] for i, event in enumerate(all_individual_events)}\n",
    "\n",
    "# # Calculate row positions for each event type\n",
    "# num_event_types = len(all_individual_events)\n",
    "# row_height = 0.8 / num_event_types  # Use 80% of vertical space, leaving margins\n",
    "# row_spacing = 0.1 / (num_event_types + 1)  # Spacing between rows\n",
    "\n",
    "# event_row_positions = {}\n",
    "# for i, event in enumerate(all_individual_events):\n",
    "#     # Calculate bottom and top of each row\n",
    "#     y_bottom = row_spacing + i * (row_height + row_spacing)\n",
    "#     y_top = y_bottom + row_height\n",
    "#     event_row_positions[event] = (y_bottom, y_top)\n",
    "\n",
    "# print(f\"Individual event types: {all_individual_events}\")\n",
    "# print(f\"Row positions: {event_row_positions}\")\n",
    "\n",
    "# # Keep track of which events have been added to legend\n",
    "# events_in_legend = set()\n",
    "\n",
    "# # Sort event types to control drawing order\n",
    "# event_types_sorted = sorted(event_types, key=lambda x: (x == 'move_ends', x))\n",
    "\n",
    "# # Process each original event type\n",
    "# for event_type in event_types_sorted:\n",
    "#     event_times = events_df[events_df[\"event\"] == event_type].index\n",
    "    \n",
    "#     if ',' in event_type:\n",
    "#         # Handle combined events - plot each component in its own row\n",
    "#         individual_events = [e.strip() for e in event_type.split(',')]\n",
    "        \n",
    "#         for individual_event in individual_events:\n",
    "#             y_bottom, y_top = event_row_positions[individual_event]\n",
    "            \n",
    "#             # Create vertical lines in this event's dedicated row\n",
    "#             x_coords = []\n",
    "#             y_coords = []\n",
    "            \n",
    "#             for event_time in event_times:\n",
    "#                 x_coords.extend([event_time, event_time, None])\n",
    "#                 y_coords.extend([y_bottom, y_top, None])\n",
    "            \n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=x_coords,\n",
    "#                     y=y_coords,\n",
    "#                     mode=\"lines\",\n",
    "#                     name=individual_event,\n",
    "#                     line=dict(color=event_colors[individual_event], width=1),\n",
    "#                     opacity=0.9,\n",
    "#                     hovertemplate=f\"<b>Event:</b> {individual_event}<br><b>Time:</b> %{{x}}<br><b>Combined from:</b> {event_type}<extra></extra>\",\n",
    "#                     showlegend=individual_event not in events_in_legend,\n",
    "#                     legendgroup=individual_event\n",
    "#                 ),\n",
    "#                 row=2, col=1\n",
    "#             )\n",
    "#             events_in_legend.add(individual_event)\n",
    "#     else:\n",
    "#         # Handle single events in their dedicated row\n",
    "#         y_bottom, y_top = event_row_positions[event_type]\n",
    "        \n",
    "#         x_coords = []\n",
    "#         y_coords = []\n",
    "        \n",
    "#         for event_time in event_times:\n",
    "#             x_coords.extend([event_time, event_time, None])\n",
    "#             y_coords.extend([y_bottom, y_top, None])\n",
    "        \n",
    "#         fig.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=x_coords,\n",
    "#                 y=y_coords,\n",
    "#                 mode=\"lines\",\n",
    "#                 name=event_type,\n",
    "#                 line=dict(color=event_colors[event_type], width=1),\n",
    "#                 opacity=0.9,\n",
    "#                 hovertemplate=f\"<b>Event:</b> {event_type}<br><b>Time:</b> %{{x}}<extra></extra>\",\n",
    "#                 showlegend=event_type not in events_in_legend\n",
    "#             ),\n",
    "#             row=2, col=1\n",
    "#         )\n",
    "#         events_in_legend.add(event_type)\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Normalized Cumulative Count\",\n",
    "#     tickfont=dict(size=17),\n",
    "#     titlefont=dict(size=18),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "# fig.update_yaxes(\n",
    "#     title_text=\"Events\",\n",
    "#     showticklabels=False,\n",
    "#     range=[0, 1],  # Fixed range for event display\n",
    "#     row=2, col=1\n",
    "# )\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Time (timestamp)\",\n",
    "#     tickfont=dict(size=17),\n",
    "#     titlefont=dict(size=19),\n",
    "#     tickangle=-45,\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "# # Set title\n",
    "# total_activations = feat_activity_cumsum[-1] if len(feat_activity_cumsum) > 0 else 0\n",
    "# fig.update_layout(\n",
    "#     title_text=f\"Instance {instance}, Feature {feature} - Feature Activity (Total: {total_activations} activations)\"\n",
    "# )\n",
    "\n",
    "# fig.update_layout(\n",
    "#     # Reduce margin space around the entire figure\n",
    "#     margin=dict(l=40, r=20, t=40, b=20),\n",
    "    \n",
    "#     # Make the legend more compact\n",
    "#     legend=dict(\n",
    "#         y=0.999,\n",
    "#         x=0.001,\n",
    "#         font=dict(size=13),\n",
    "#         itemsizing='constant',\n",
    "#         bgcolor=\"rgba(255, 255, 255, 0.3)\"\n",
    "#     ),\n",
    "    \n",
    "#     # Title styling\n",
    "#     title=dict(\n",
    "#         font=dict(size=23),\n",
    "#     ),\n",
    "    \n",
    "#     height=600,\n",
    "#     width=1000\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jai's code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize unit spiking variability over SAE feature activity.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"variable\", \n",
    "    y=\"value\", \n",
    "    data=pd.melt(spk_cts_df.iloc[f_ex_idxs]), \n",
    "    showfliers=False,\n",
    "    width=1,\n",
    "    whis=0.75, \n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel(\"units\", fontsize=26)\n",
    "ax.set_ylabel(\"normalized spike counts\", fontsize=26)# ax.set_yticks([])\n",
    "ax.set_xticklabels([])\n",
    "# Set yticklabel font size\n",
    "ax.tick_params(axis=\"y\", labelsize=24)\n",
    "ax.set_title(f\"feature ({inst_i} : {feat_i}) normalized unit spike counts when active\", fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of data that is flashes\n",
    "len(metadata[metadata[\"stimulus_name\"] == \"flashes\"]) / len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11000 / 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize SAE-natural feature confusion matrix.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.bar(\n",
    "    x=[\n",
    "        \"gratings\", \n",
    "        \"gratings\", \n",
    "        \"gratings\\n(spatial frequency = 0.04)\", \n",
    "        \"gratings\\n(spatial frequency = 0.04)\"\n",
    "    ],\n",
    "    height=[0.56, 0.37, 0.6, 0.17],\n",
    "    color=[\"blue\", \"black\", \"blue\", \"black\"],\n",
    "    alpha=0.7,\n",
    "    width=0.5\n",
    ")\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# ax.bar(\n",
    "#     x=[\n",
    "#         \"flashes\", \n",
    "#         \"flashes\", \n",
    "#     ],\n",
    "#     height=[0.89, 0.03],\n",
    "#     color=[\"blue\", \"black\"],\n",
    "#     alpha=0.7,\n",
    "#     width=0.3\n",
    "# )\n",
    "\n",
    "ax.set_title(f\"({inst_i} : {feat_i}) feature active on\", fontsize=26)\n",
    "\n",
    "# update fontsizes of ticks labels and titles\n",
    "ax.tick_params(axis=\"x\", labelsize=22)\n",
    "ax.tick_params(axis=\"y\", labelsize=19)\n",
    "ax.set_ylabel(\"fraction of presentations\", fontsize=22)\n",
    "# ax.set_xticklabels([\"drifting_gratings\", \"\", \"drifting_gratings\\n(temporal frequency < 4 Hz)\", \"\"])\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "# fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize boxplots of feature activations across levels as a proxy for represented hierarchy.\"\"\"\n",
    "\n",
    "l0_act_frac = features_df[features_df[\"feature_idx\"] < last_feat_idx_general][\"activation_frac\"] + 0.01\n",
    "\n",
    "l1_act_frac = features_df[np.logical_and(\n",
    "    features_df[\"feature_idx\"] > last_feat_idx_general,\n",
    "    features_df[\"feature_idx\"] < first_feat_idx_specific\n",
    ")][\"activation_frac\"] + 0.01\n",
    "\n",
    "l2_act_frac = features_df[features_df[\"feature_idx\"] > first_feat_idx_specific][\"activation_frac\"] - 0.01\n",
    "l2_act_frac = l2_act_frac.clip(lower=0)\n",
    "\n",
    "# First, create a dataframe to hold all three sets of data with appropriate labels\n",
    "boxplot_data = pd.DataFrame({\n",
    "    \"Layer\": [\"0 (General)\"] * len(l0_act_frac) + \n",
    "             [\"1 (Intermediate)\"] * len(l1_act_frac) + \n",
    "             [\"2 (Specific)\"] * len(l2_act_frac),\n",
    "    \"Activation Fraction\": pd.concat([l0_act_frac, l1_act_frac, l2_act_frac])\n",
    "})\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(\n",
    "    x=\"Layer\", \n",
    "    y=\"Activation Fraction\", \n",
    "    data=boxplot_data,\n",
    "    showfliers=False,  # This hides the outliers\n",
    "    width=0.6,         # Controls the width of the boxes\n",
    "    palette=\"viridis\",  # Optional: choose a color palette\n",
    "    # show mean\n",
    "    showmeans=True,\n",
    "    meanprops={\"marker\": \"v\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"black\", \"markersize\": 10},\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Feature activation fraction by matryoshka level\", fontsize=14)\n",
    "plt.xlabel(\"Level\", fontsize=12)\n",
    "plt.ylabel(\"Activation Fraction\", fontsize=12)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize boxplots of feature activations across levels as a proxy for represented hierarchy.\"\"\"\n",
    "\n",
    "acts_df_feat_on_idx = np.logical_and(\n",
    "    acts_df[\"feature_idx\"].isin(features_df[\"feature_idx\"].unique()), \n",
    "    acts_df[\"instance_idx\"] == 0\n",
    ")\n",
    "\n",
    "acts_df_comp = acts_df[acts_df_feat_on_idx].copy().drop(\n",
    "    columns=[\"activation_value\", \"instance_idx\"], axis=1\n",
    ")\n",
    "\n",
    "feature_idxs = np.sort(acts_df_comp[\"feature_idx\"].unique())\n",
    "corr_mat_idxs = np.arange(0, len(feature_idxs))\n",
    "corr_mat_feat_idx_map = {feature_idxs[i]: corr_mat_idxs[i] for i in range(len(feature_idxs))}\n",
    "acts_df_comp[\"corr_mat_idx\"] = acts_df_comp[\"feature_idx\"].map(corr_mat_feat_idx_map)\n",
    "\n",
    "feat_on_mask = np.zeros((n_recon_examples, len(feature_idxs)))\n",
    "feat_on_mask[acts_df_comp[\"example_idx\"].values, acts_df_comp[\"corr_mat_idx\"].values] = True\n",
    "\n",
    "tot_feat_act = feat_on_mask.sum(axis=0)\n",
    "\n",
    "feat_corr = t.tensor(feat_on_mask.T, device=device) @ t.tensor(feat_on_mask, device=device)\n",
    "feat_corr_norm = (feat_corr / t.tensor(tot_feat_act, device=device).unsqueeze(0))\n",
    "\n",
    "l0_co_idx = np.where(feature_idxs > 256)[0][0]\n",
    "l1_co_idx = np.where(feature_idxs > 512)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    asnumpy(feat_corr_norm), \n",
    "    cmap=\"viridis\", \n",
    "    vmin=0, \n",
    "    vmax=1, \n",
    "    square=True,\n",
    "    xticklabels=range(feat_on_mask.shape[1]),\n",
    "    yticklabels=range(feat_on_mask.shape[1]),\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(\"SAE feature conditional correlation matrix\", fontsize=20)\n",
    "ax.set_xlabel(\"SAE feature i\", fontsize=16)\n",
    "ax.set_ylabel(\"SAE feature j ( P(j | i) )\", fontsize=16)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Draw red horizontal and vertical lines at l0_co_idx and l1_co_idx\n",
    "for idx in [l0_co_idx, l1_co_idx]:\n",
    "    ax.axhline(y=idx, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "    ax.axvline(x=idx, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize MSAE latents as bases of neural manifolds.\"\"\"\n",
    "\n",
    "# For SAE features: look at manifolds, feature activity over time (over stimuli xaxis bar), \n",
    "# extra: feature correlations ?\n",
    "\n",
    "features_df.sort_values(\"activation_std\", ascending=True)\n",
    "acts_df_man = acts_df[acts_df[\"feature_idx\"].isin([246, 41, 22])]\n",
    "acts_df_man = acts_df_man[acts_df_man[\"instance_idx\"] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts_df_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where instance_idx == 1\n",
    "\n",
    "man_vals = np.zeros((n_recon_examples, 3))\n",
    "\n",
    "for i, feat_i in enumerate([246, 41, 22]):\n",
    "    idxs = acts_df_man[acts_df_man[\"feature_idx\"] == feat_i][\"example_idx\"].values\n",
    "    man_vals[idxs, i] = acts_df_man[acts_df_man[\"feature_idx\"] == feat_i][\"activation_value\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the numpy array\n",
    "man_df = pd.DataFrame(man_vals)\n",
    "man_df.index = man_df.index * bin_s\n",
    "man_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced version with better visuals and controls\n",
    "def create_enhanced_3d_animation(df, save_path=None, fps=30, dpi=100):\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extract data\n",
    "    x_data = df.iloc[:, 0].values\n",
    "    y_data = df.iloc[:, 1].values\n",
    "    z_data = df.iloc[:, 2].values\n",
    "    \n",
    "    # Calculate ranges for better visualization\n",
    "    x_range = max(x_data) - min(x_data)\n",
    "    y_range = max(y_data) - min(y_data)\n",
    "    z_range = max(z_data) - min(z_data)\n",
    "    \n",
    "    # Set equal aspect ratio\n",
    "    max_range = max(x_range, y_range, z_range) / 2\n",
    "    mid_x = (max(x_data) + min(x_data)) / 2\n",
    "    mid_y = (max(y_data) + min(y_data)) / 2\n",
    "    mid_z = (max(z_data) + min(z_data)) / 2\n",
    "    \n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(df.columns[0], fontsize=12)\n",
    "    ax.set_ylabel(df.columns[1], fontsize=12)\n",
    "    ax.set_zlabel(df.columns[2], fontsize=12)\n",
    "    ax.set_title('Top 3 SAE latents manifold over time', fontsize=14)\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Initialize line and point objects\n",
    "    line, = ax.plot([], [], [], lw=2, color='blue')\n",
    "    point, = ax.plot([], [], [], 'ro', markersize=10)\n",
    "    \n",
    "    # Add a shadow on the floor\n",
    "    ax.plot(x_data, y_data, min(z_data), 'k--', alpha=0.2)\n",
    "    \n",
    "    # Add vector from origin to current point\n",
    "    vector, = ax.plot([0, 0], [0, 0], [0, 0], 'g-', lw=1.5, alpha=0.7)\n",
    "    \n",
    "    # Time display\n",
    "    time_template = 'Time = %.3f s'\n",
    "    time_text = ax.text2D(0.05, 0.95, '', transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    # Progress bar\n",
    "    progress_bar_ax = fig.add_axes([0.2, 0.05, 0.6, 0.03])\n",
    "    progress_bar = plt.Rectangle((0, 0), 0, 1, fc='blue', alpha=0.5)\n",
    "    progress_bar_ax.add_patch(progress_bar)\n",
    "    progress_bar_ax.set_xlim(0, 1)\n",
    "    progress_bar_ax.set_ylim(0, 1)\n",
    "    progress_bar_ax.axis('off')\n",
    "    \n",
    "    # Get time values\n",
    "    if isinstance(df.index[0], (int, float)):\n",
    "        times = df.index.values\n",
    "    else:\n",
    "        times = np.arange(len(df))\n",
    "    \n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        line.set_3d_properties([])\n",
    "        point.set_data([], [])\n",
    "        point.set_3d_properties([])\n",
    "        vector.set_data([], [])\n",
    "        vector.set_3d_properties([])\n",
    "        time_text.set_text('')\n",
    "        progress_bar.set_width(0)\n",
    "        return line, point, vector, time_text, progress_bar\n",
    "    \n",
    "    def update(frame):\n",
    "        i = min(frame, len(df) - 1)\n",
    "        progress = i / (len(df) - 1)\n",
    "        \n",
    "        # Update line data\n",
    "        line.set_data(x_data[:i+1], y_data[:i+1])\n",
    "        line.set_3d_properties(z_data[:i+1])\n",
    "        \n",
    "        # Update current point\n",
    "        point.set_data([x_data[i]], [y_data[i]])\n",
    "        point.set_3d_properties([z_data[i]])\n",
    "        \n",
    "        # Update vector from origin\n",
    "        vector.set_data([0, x_data[i]], [0, y_data[i]])\n",
    "        vector.set_3d_properties([0, z_data[i]])\n",
    "        \n",
    "        # Update time text\n",
    "        current_time = times[i] if i < len(times) else times[-1]\n",
    "        time_text.set_text(time_template % current_time)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_width(progress)\n",
    "        \n",
    "        # Rotate view slightly for 3D effect\n",
    "        ax.view_init(elev=30, azim=i / 5 % 360)\n",
    "        \n",
    "        return line, point, vector, time_text, progress_bar\n",
    "    \n",
    "    # Sample a reasonable number of frames\n",
    "    num_frames = min(len(df), 300)\n",
    "    frame_indices = np.linspace(0, len(df)-1, num_frames, dtype=int)\n",
    "    \n",
    "    ani = FuncAnimation(fig, update, frames=frame_indices,\n",
    "                        init_func=init, blit=False, interval=1000/fps)\n",
    "    \n",
    "    if save_path:\n",
    "        ani.save(save_path, writer='pillow', fps=fps, dpi=dpi)\n",
    "        print(f\"Enhanced animation saved to {save_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "    return ani\n",
    "\n",
    "# Use this enhanced version\n",
    "save_path = out_dir / f\"{session_id}\" / \"sae_features\" / \"sae_0\" / \"enhanced_3d_animation.gif\"\n",
    "enhanced_ani = create_enhanced_3d_animation(man_df.iloc[::500], save_path=save_path, fps=15, dpi=200)\n",
    "# HTML(enhanced_ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(cache.get_natural_scene_template(0), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[metadata[\"stimulus_name\"] == \"natural_scenes\"][\"stimulus_condition_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_data.presentationwise_spike_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_data.conditionwise_spike_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
