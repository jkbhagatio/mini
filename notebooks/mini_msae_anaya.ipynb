{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set notebook settings.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %flow mode reactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import packages.\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "# IPython and Jupyter-related imports\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Third-party libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import temporaldata as td\n",
    "import torch as t\n",
    "from einops import (\n",
    "    asnumpy,\n",
    "    einsum,\n",
    "    pack,\n",
    "    parse_shape,\n",
    "    rearrange,\n",
    "    reduce,\n",
    "    repeat,\n",
    "    unpack,\n",
    ")\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from jaxtyping import Float, Int\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from rich import print as rprint\n",
    "from scipy import stats\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import classification_report, confusion_matrix, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from temporaldata import Data\n",
    "from torch import Tensor, bfloat16, nn\n",
    "from torch.nn import functional as F\n",
    "from torcheval.metrics.functional import r2_score as tm_r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Local project modules\n",
    "from mini import train as mt\n",
    "from mini.util import vec_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max rows and cols for df display\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "pd.set_option(\"display.max_columns\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_session_data(session):\n",
    "    \"\"\"Clean session data by filtering trials and spikes based on quality criteria.\"\"\"\n",
    "    \n",
    "    # Mark Churchland gave me a matlab script which I have converted to python\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    # I think is_valid is already defined by brainsets as (session.trials.discard_trial == 0) & (session.trials.task_success == 1) so it's a bit redundant but including it all just to be sure\n",
    "    # In theory I should also filter based on whether the maze was possible or not (a field called \"unhittable\") but I cannot find this in the data, perhaps this has already been done in this release of the data\n",
    "    good_trials = (session.trials.trial_type > 0) & (session.trials.is_valid == 1) & (session.trials.discard_trial == 0) & (session.trials.novel_maze == 0) & (session.trials.trial_version < 3) \n",
    "    session.trials = session.trials.select_by_mask(good_trials)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out extraneous trials, went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    success = (session.trials.task_success == 1)\n",
    "    session.trials = session.trials.select_by_mask(success)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out unsuccessful trials, went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    post_move = 0.8 # to be kept, there must be at least this many ms after the movement onset\n",
    "    long_enough = (session.trials.end - session.trials.move_begins_time >= post_move) # should essentially always be true for successes\n",
    "    session.trials = session.trials.select_by_mask(long_enough)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out trials that were too short, went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    num_trials = len(session.trials.start)\n",
    "    consistent = (session.trials.correct_reach == 1)\n",
    "    session.trials = session.trials.select_by_mask(consistent)\n",
    "    new_num_trials = len(session.trials.start)\n",
    "    if num_trials - new_num_trials > 0:\n",
    "        print(\"Filtered out trials with inconsistent reaches (not similar enough to the \\\"prototypical\\\" trial), went from\", num_trials, \"trials to\", new_num_trials)\n",
    "\n",
    "    primary_conditions = np.unique(session.trials.maze_condition)\n",
    "    num_conditions = len(primary_conditions)\n",
    "    print(\"Number of primary conditions:\", num_conditions)\n",
    "    # Check to make sure they are monotonic, starting from 1 and counting up\n",
    "    if min(primary_conditions) != 1 or len(np.unique(np.diff(primary_conditions))) != 1:\n",
    "        raise ValueError(\"Primary conditions are not monotonic or do not start from 1\")\n",
    "\n",
    "    # In theory I should filter units based on a ranking from 1-4 but I cannot find the ranking in the data, perhaps this has already been done in this release of the data\n",
    "\n",
    "    # Only keep spikes that are within the cleaned trials\n",
    "    session.spikes = session.spikes.select_by_interval(session.trials)\n",
    "    session.hand = session.hand.select_by_interval(session.trials)\n",
    "    session.eye = session.eye.select_by_interval(session.trials)\n",
    "\n",
    "    # Convert session recording date to timestamp\n",
    "    session.session.recording_date = datetime.strptime(session.session.recording_date, '%Y-%m-%d %H:%M:%S')\n",
    "    session.session.recording_date = session.session.recording_date.timestamp()\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_maze_conditions(session):\n",
    "    \"\"\"\n",
    "    Analyze what each maze_condition corresponds to in terms of \n",
    "    maze parameters (barriers, targets, hit position).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique maze conditions\n",
    "    unique_conditions = np.unique(session.trials.maze_condition)\n",
    "    \n",
    "    # Create a summary for each condition\n",
    "    condition_summary = []\n",
    "    \n",
    "    for condition in unique_conditions:\n",
    "        # Get trials for this condition\n",
    "        condition_mask = session.trials.maze_condition == condition\n",
    "        \n",
    "        # Get the unique values for this condition\n",
    "        barriers = np.unique(session.trials.maze_num_barriers[condition_mask])\n",
    "        targets = np.unique(session.trials.maze_num_targets[condition_mask])\n",
    "        hit_position = np.unique(session.trials.hit_target_position[condition_mask], axis=0)\n",
    "        if len(hit_position) > 1:\n",
    "            raise ValueError(f\"Condition {condition} has multiple hit positions: {hit_position}\")\n",
    "        else:\n",
    "            hit_position = hit_position[0]\n",
    "        \n",
    "        # Count trials for this condition\n",
    "        num_trials = np.sum(condition_mask)\n",
    "        \n",
    "        # Store for summary table\n",
    "        condition_summary.append({\n",
    "            'Maze Condition': condition,\n",
    "            'Trials': num_trials,\n",
    "            'Barriers': barriers,\n",
    "            'Targets': targets,\n",
    "            'Hit Position': hit_position, \n",
    "            'Hit Position Angles': str(np.degrees(np.arctan2(hit_position[1], hit_position[0])))\n",
    "        })\n",
    "    summary_df = pd.DataFrame(condition_summary)\n",
    "    \n",
    "    # Convert hit positions to tuples temporarily for proper duplicate detection\n",
    "    summary_df_temp = summary_df.copy()\n",
    "    summary_df_temp['Hit Position Tuple'] = summary_df_temp['Hit Position'].apply(tuple)\n",
    "    plot_df = summary_df_temp.drop_duplicates(subset=['Hit Position Tuple'], keep='first')\n",
    "    plot_df = plot_df.drop('Hit Position Tuple', axis=1)  # Remove the temporary column\n",
    "    \n",
    "    # Create a proper DataFrame for plotting\n",
    "    plot_data = pd.DataFrame({\n",
    "        'Hit Position X': plot_df['Hit Position'].apply(lambda x: x[0]),\n",
    "        'Hit Position Y': plot_df['Hit Position'].apply(lambda x: x[1]),\n",
    "        'Maze Condition': plot_df['Maze Condition'].astype(str)\n",
    "    })\n",
    "    \n",
    "    # Generate unique colors for each maze condition\n",
    "    import plotly.colors as pc\n",
    "    n_conditions = len(plot_data['Maze Condition'].unique())\n",
    "    colors = pc.sample_colorscale('viridis', [i/(max(n_conditions-1, 1)) for i in range(n_conditions)])\n",
    "    \n",
    "    # Plot hit position by maze condition\n",
    "    fig = px.scatter(\n",
    "        plot_data,\n",
    "        x='Hit Position X',\n",
    "        y='Hit Position Y',\n",
    "        color='Maze Condition',\n",
    "        labels={'Hit Position X': 'Hit Position X', 'Hit Position Y': 'Hit Position Y', 'color': 'Maze Condition'},\n",
    "        title='Hit Position by Maze Condition',\n",
    "        color_discrete_sequence=colors,\n",
    "        hover_data=['Maze Condition']\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis=dict(scaleanchor=\"y\", scaleratio=1, range=[-150, 150]),\n",
    "        yaxis=dict(constrain=\"domain\", range=[-150, 150]),\n",
    "        width=600,\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your data directory\n",
    "data_path = r\"C:\\Users\\pouge\\Documents\\mini_data\\brainsets\\processed\\churchland_shenoy_neural_2012\"\n",
    "# data_path = \"/ceph/aeon/aeon/SANe/brainsets_data/processed/churchland_shenoy_neural_2012\"\n",
    "data_path = Path(data_path)\n",
    "\n",
    "# List all h5 files in the directory\n",
    "h5_files = [f for f in os.listdir(data_path) if f.endswith('.h5')]\n",
    "print(f\"Available h5 files: {h5_files}\")\n",
    "\n",
    "# User parameters\n",
    "subject_name = \"jenkins\"  # Change to \"nitschke\" or \"jenkins\"\n",
    "num_files_to_load = 4     # Change to desired number of files, max 6 (only 3 work) for nitschke, 4 for jenkins\n",
    "\n",
    "# Filter files by subject\n",
    "subject_files = [f for f in h5_files if subject_name.lower() in f.lower()]\n",
    "print(f\"\\nFiles for subject {subject_name}: {subject_files}\")\n",
    "\n",
    "if len(subject_files) == 0:\n",
    "    print(f\"No files found for subject {subject_name}\")\n",
    "elif len(subject_files) < num_files_to_load:\n",
    "    print(f\"Only {len(subject_files)} files available for {subject_name}, loading all of them\")\n",
    "    num_files_to_load = len(subject_files)\n",
    "\n",
    "# Load and clean the specified number of files\n",
    "sessions = []\n",
    "for i in range(min(num_files_to_load, len(subject_files))):\n",
    "    file_path = os.path.join(data_path, subject_files[i])\n",
    "    print(f\"\\nLoading file {i+1}/{num_files_to_load}: {subject_files[i]}\")\n",
    "    \n",
    "    # Read neural data from HDF5\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        session = Data.from_hdf5(f)\n",
    "\n",
    "        session.spikes.materialize()\n",
    "        session.trials.materialize()\n",
    "        session.hand.materialize()\n",
    "        session.eye.materialize()\n",
    "        session.session.materialize()\n",
    "        session.units.materialize()\n",
    "\n",
    "        print(\"Session ID: \", session.session.id)\n",
    "        print(\"Session subject id: \", session.subject.id)\n",
    "        print(\"Session subject sex: \", session.subject.sex)\n",
    "        print(\"Session subject species: \", session.subject.species)\n",
    "        print(\"Session recording date: \", session.session.recording_date)\n",
    "        print(\"Original number of trials:\", len(session.trials.start))\n",
    "        \n",
    "        # Clean the session data\n",
    "        try:\n",
    "            session = clean_session_data(session)\n",
    "            print(\"Final number of trials after cleaning:\", len(session.trials.start))\n",
    "\n",
    "            print(\"Summary of primary conditions:\")\n",
    "            primary_conditions_summary = analyze_maze_conditions(session)\n",
    "            display(primary_conditions_summary)\n",
    "            \n",
    "            sessions.append(session)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {session.session.id}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nSuccessfully loaded and cleaned {len(sessions)} sessions for subject {subject_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_maze_conditions_consistency(sessions):\n",
    "    \"\"\"\n",
    "    Fix maze condition numbering to be consistent across all sessions.\n",
    "    \n",
    "    Args:\n",
    "        sessions: List of session objects\n",
    "    \n",
    "    Returns:\n",
    "        List of cleaned sessions with consistent maze condition numbering\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_maze_signature(session, condition):\n",
    "        \"\"\"Get maze parameters for a specific condition to create a signature\"\"\"\n",
    "        condition_mask = session.trials.maze_condition == condition\n",
    "        \n",
    "        # Get unique values for this condition\n",
    "        barriers = tuple(np.unique(session.trials.maze_num_barriers[condition_mask]))\n",
    "        targets = tuple(np.unique(session.trials.maze_num_targets[condition_mask]))\n",
    "        hit_position = np.unique(session.trials.hit_target_position[condition_mask], axis=0)\n",
    "        if len(barriers) > 1 or len(targets) > 1 or len(hit_position) > 1:\n",
    "            raise ValueError(f\"Condition {condition} has the following >1 unique values for one of the following: \"\n",
    "                             f\"barriers={barriers}, targets={targets}, hit_position={hit_position}. \"\n",
    "                             \"This should not be possible.\")\n",
    "        else:\n",
    "            hit_position = tuple(tuple(hit_position[0]))\n",
    "        \n",
    "        return (barriers, targets, hit_position)\n",
    "    \n",
    "    def get_group_signature(session, group_conditions):\n",
    "        \"\"\"Get combined signature for a group of 3 conditions\"\"\"\n",
    "        group_sigs = []\n",
    "        for condition in sorted(group_conditions):\n",
    "            sig = get_maze_signature(session, condition)\n",
    "            group_sigs.append(sig)\n",
    "        return tuple(group_sigs)\n",
    "    \n",
    "    # Remove sessions that don't have multiples of 3 maze conditions\n",
    "    valid_sessions = []\n",
    "    for i, session in enumerate(sessions):\n",
    "        unique_conditions = np.unique(session.trials.maze_condition)\n",
    "        num_conditions = len(unique_conditions)\n",
    "        \n",
    "        if num_conditions % 3 != 0:\n",
    "            print(f\"WARNING: Removing session {session.session.id} - has {num_conditions} maze conditions (not multiple of 3)\")\n",
    "            continue\n",
    "        \n",
    "        valid_sessions.append(session)\n",
    "    \n",
    "    if len(valid_sessions) == 0:\n",
    "        raise ValueError(\"No valid sessions remaining after filtering\")\n",
    "    \n",
    "    print(f\"Kept {len(valid_sessions)} out of {len(sessions)} sessions after filtering\")\n",
    "    \n",
    "    # Pick reference session (most unique maze conditions)\n",
    "    condition_counts = []\n",
    "    for session in valid_sessions:\n",
    "        unique_conditions = len(np.unique(session.trials.maze_condition))\n",
    "        condition_counts.append(unique_conditions)\n",
    "    \n",
    "    max_conditions = max(condition_counts)\n",
    "    ref_idx = condition_counts.index(max_conditions)\n",
    "    reference_session = valid_sessions[ref_idx]\n",
    "    \n",
    "    print(f\"Using session {reference_session.session.id} as reference (has {max_conditions} maze conditions)\")\n",
    "    \n",
    "    # Group conditions and check for duplicates in reference session\n",
    "    ref_conditions = sorted(np.unique(reference_session.trials.maze_condition))\n",
    "    ref_groups = []\n",
    "    \n",
    "    for i in range(0, len(ref_conditions), 3):\n",
    "        group = ref_conditions[i:i+3]\n",
    "        if len(group) != 3:\n",
    "            raise ValueError(f\"Reference session has incomplete group: {group}\")\n",
    "        ref_groups.append(group)\n",
    "    \n",
    "    # Create signatures for reference groups\n",
    "    ref_group_signatures = {}\n",
    "    ref_signatures_to_group = {}\n",
    "    \n",
    "    for group_idx, group in enumerate(ref_groups):\n",
    "        signature = get_group_signature(reference_session, group)\n",
    "        \n",
    "        if signature in ref_signatures_to_group:\n",
    "            existing_group = ref_signatures_to_group[signature]\n",
    "            print(f\"WARNING: Duplicate group found in reference session!\")\n",
    "            print(f\"  Group {existing_group} and Group {group} have identical maze parameters\")\n",
    "        \n",
    "        ref_group_signatures[group_idx] = signature\n",
    "        ref_signatures_to_group[signature] = group\n",
    "    \n",
    "    print(f\"Reference session has {len(ref_groups)} groups of maze conditions\")\n",
    "\n",
    "    print(\"Table of reference session maze conditions:\")\n",
    "    primary_conditions_summary = analyze_maze_conditions(reference_session)\n",
    "    display(primary_conditions_summary) \n",
    "    \n",
    "    # Process all sessions to match groups and renumber\n",
    "    processed_sessions = []\n",
    "    \n",
    "    for session in valid_sessions:\n",
    "        print(f\"Processing session {session.session.id}...\")\n",
    "        \n",
    "        # Get conditions and group them\n",
    "        conditions = sorted(np.unique(session.trials.maze_condition))\n",
    "        session_groups = []\n",
    "        \n",
    "        for i in range(0, len(conditions), 3):\n",
    "            group = conditions[i:i+3]\n",
    "            session_groups.append(group)\n",
    "        \n",
    "        # Match each group to reference\n",
    "        condition_mapping = {}  # old_condition -> new_condition\n",
    "        \n",
    "        for group in session_groups:\n",
    "            group_sig = get_group_signature(session, group)\n",
    "            \n",
    "            # Find matching reference group\n",
    "            matched_ref_group_idx = None\n",
    "            for ref_idx, ref_sig in ref_group_signatures.items():\n",
    "                if group_sig == ref_sig:\n",
    "                    matched_ref_group_idx = ref_idx\n",
    "                    break\n",
    "            \n",
    "            if matched_ref_group_idx is None:\n",
    "                print(f\"ERROR: Could not match group {group} in session {session.session.id}\")\n",
    "                print(f\"Group signature: {group_sig}\")\n",
    "                print(\"Available reference signatures:\")\n",
    "                for ref_idx, ref_sig in ref_group_signatures.items():\n",
    "                    ref_group = ref_groups[ref_idx]\n",
    "                    print(f\"  Reference group {ref_group}: {ref_sig}\")\n",
    "                raise ValueError(f\"Unmatchable maze conditions {group} in session {session.session.id}\")\n",
    "            \n",
    "            # Map old conditions to new conditions\n",
    "            new_base_condition = matched_ref_group_idx * 3 + 1  # 1, 4, 7, 10, ...\n",
    "            for i, old_condition in enumerate(sorted(group)):\n",
    "                new_condition = new_base_condition + i\n",
    "                condition_mapping[old_condition] = new_condition\n",
    "        \n",
    "        \n",
    "        # Apply the mapping to the session\n",
    "        new_maze_conditions = np.array([condition_mapping[old] for old in session.trials.maze_condition])\n",
    "        session.trials.maze_condition = new_maze_conditions\n",
    "        \n",
    "        processed_sessions.append(session)\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(processed_sessions)} sessions with consistent maze condition numbering\")\n",
    "    return processed_sessions\n",
    "\n",
    "# Add this after your existing code, after sessions are loaded and cleaned:\n",
    "print(\"Fixing maze condition consistency across sessions...\")\n",
    "\n",
    "sessions = fix_maze_conditions_consistency(sessions)\n",
    "\n",
    "# Show updated condition summaries for verification\n",
    "print(\"\\nCondition remapping summary:\")\n",
    "for i, session in enumerate(sessions):\n",
    "    unique_conditions = sorted(np.unique(session.trials.maze_condition))\n",
    "    print(f\"Session {session.session.id}: {unique_conditions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train a model on a specific session post fixing maze conditions consistency\n",
    "# print(len(sessions))\n",
    "# sessions =  [sessions[i] for i in [0, 1]]\n",
    "# print(len(sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bin_size = 0.05\n",
    "\n",
    "# Check unit consistency across sessions\n",
    "unit_ids = np.unique(sessions[0].spikes.unit_index)\n",
    "for session in sessions:\n",
    "    unique_units = np.unique(session.spikes.unit_index)\n",
    "    if not np.array_equal(unique_units, unit_ids):\n",
    "        raise ValueError(\"Sessions do not have the same unit IDs. Cannot combine spike data.\")\n",
    "\n",
    "# Determine global bin alignment start point\n",
    "global_start = min(session.session.recording_date + session.trials.start.min() for session in sessions)\n",
    "global_start = np.floor(global_start / bin_size) * bin_size  # ensure clean bin alignment\n",
    "\n",
    "# Convert to consistent timestamps\n",
    "n_decimals = int(-np.log10(bin_size)) + 1 if bin_size < 1 else 0\n",
    "\n",
    "# Accumulator for all binned trials\n",
    "binned_dfs = []\n",
    "\n",
    "# Loop over sessions\n",
    "for session in sessions:\n",
    "    # Shift spike timestamps to absolute time\n",
    "    abs_timestamps = session.spikes.timestamps + session.session.recording_date\n",
    "    unit_ids_this_session = session.spikes.unit_index\n",
    "    df_spikes = pd.DataFrame({\n",
    "        'timestamp': abs_timestamps,\n",
    "        'unit': unit_ids_this_session\n",
    "    }).sort_values('timestamp')\n",
    "\n",
    "    # Convert trial times to absolute time\n",
    "    trial_starts = session.trials.start + session.session.recording_date\n",
    "    trial_ends = session.trials.end + session.session.recording_date\n",
    "    df_trials = pd.DataFrame({\n",
    "        'trial_start': trial_starts,\n",
    "        'trial_end': trial_ends\n",
    "    }).sort_values('trial_start')\n",
    "\n",
    "    # Assign each spike to the most recent trial_start <= timestamp\n",
    "    df_merged = pd.merge_asof(\n",
    "        df_spikes,\n",
    "        df_trials[['trial_start', 'trial_end']],\n",
    "        left_on='timestamp',\n",
    "        right_on='trial_start',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    # Drop spikes that fall outside their trial interval\n",
    "    df_merged = df_merged[df_merged['timestamp'] < df_merged['trial_end']]\n",
    "\n",
    "    # Compute bin index relative to global bin start\n",
    "    df_merged['bin'] = ((df_merged['timestamp'] - global_start) / bin_size).astype(int)\n",
    "\n",
    "    # Group by (bin, unit) and count spikes\n",
    "    df_counts = (\n",
    "        df_merged\n",
    "        .groupby(['bin', 'unit'], observed=True)\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # Pivot to wide format: units as columns\n",
    "    spk_cts = df_counts.pivot_table(\n",
    "        index='bin',\n",
    "        columns='unit',\n",
    "        values='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "\n",
    "    # Convert to consistent timestamps\n",
    "    session_timestamps = np.round(global_start + spk_cts.index * bin_size, n_decimals)\n",
    "    spk_cts.index = pd.Index(session_timestamps, name='timestamp')\n",
    "    spk_cts.columns.name = None\n",
    "\n",
    "    # Collect results\n",
    "    binned_dfs.append(spk_cts)\n",
    "\n",
    "# Concatenate all binned trials across sessions\n",
    "spk_cts_df = pd.concat(binned_dfs)\n",
    "spk_cts_df = spk_cts_df[~spk_cts_df.index.duplicated()]\n",
    "spk_cts_df.sort_index(inplace=True)\n",
    "\n",
    "# Result: each row = time bin, each col = unit, values = spike count\n",
    "display(spk_cts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean firing rate (Hz) per unit\n",
    "duration_sec = len(spk_cts_df) * bin_size\n",
    "mean_firing_rates = spk_cts_df.sum(axis=0) / duration_sec  # spikes/sec\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(mean_firing_rates, bins=30, edgecolor='black')\n",
    "plt.xlabel('Mean Firing Rate (Hz)')\n",
    "plt.ylabel('Number of Units')\n",
    "plt.title('Distribution of Mean Firing Rates')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary stats\n",
    "print(\"Mean firing rate across units: {:.2f} Hz\".format(mean_firing_rates.mean()))\n",
    "print(\"Median: {:.2f} Hz\".format(np.median(mean_firing_rates)))\n",
    "print(\"Range: {:.2f}–{:.2f} Hz\".format(mean_firing_rates.min(), mean_firing_rates.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the spike count matrix to a 1D array\n",
    "flattened_spike_counts = spk_cts_df.values.flatten()\n",
    "\n",
    "# Plot histogram of spike counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(flattened_spike_counts, bins=50, edgecolor='black')\n",
    "plt.title(\"Distribution of Spike Counts per Unit per Time Bin\")\n",
    "plt.xlabel(\"Spike Count\")\n",
    "plt.ylabel(\"Number of Unit-Bin Combinations\")\n",
    "plt.yscale(\"log\")  # Optional: log scale to better visualize skewed distributions\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check sparsity of binned spike counts.\"\"\"\n",
    "\n",
    "frac_nonzero_bins = (spk_cts_df != 0).values.sum() / spk_cts_df.size\n",
    "frac_nonzero_examples = (spk_cts_df.sum(axis=1) > 0).mean()\n",
    "print(f\"{frac_nonzero_bins=:.4f}\")\n",
    "print(f\"{frac_nonzero_examples=:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get environment / behavior (meta)data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data from all sessions into arrays\n",
    "hand_data = defaultdict(list)\n",
    "eye_data = defaultdict(list)\n",
    "trial_data = defaultdict(list)\n",
    "\n",
    "for i, session in enumerate(sessions):\n",
    "    recording_date = session.session.recording_date\n",
    "    \n",
    "    # Hand data - extract timestamps and 2D arrays\n",
    "    timestamps = session.hand.timestamps + recording_date\n",
    "    acc = session.hand.acc_2d\n",
    "    pos = session.hand.pos_2d\n",
    "    vel = session.hand.vel_2d\n",
    "    \n",
    "    hand_data['timestamp'].append(timestamps)\n",
    "    hand_data['acc_x'].append(acc[:, 0])\n",
    "    hand_data['acc_y'].append(acc[:, 1])\n",
    "    hand_data['pos_x'].append(pos[:, 0])\n",
    "    hand_data['pos_y'].append(pos[:, 1])\n",
    "    hand_data['vel_x'].append(vel[:, 0])\n",
    "    hand_data['vel_y'].append(vel[:, 1])\n",
    "    hand_data['session'].append(np.full(len(timestamps), i))\n",
    "    \n",
    "    # Eye data - extract timestamps and position arrays\n",
    "    eye_timestamps = session.eye.timestamps + recording_date\n",
    "    eye_pos = session.eye.pos\n",
    "    \n",
    "    eye_data['timestamp'].append(eye_timestamps)\n",
    "    eye_data['pos_x'].append(eye_pos[:, 0])\n",
    "    eye_data['pos_y'].append(eye_pos[:, 1])\n",
    "    \n",
    "    # Trial data - add recording_date to all time columns\n",
    "    trial_data['start'].append(session.trials.start + recording_date)\n",
    "    trial_data['end'].append(session.trials.end + recording_date)\n",
    "    trial_data['target_on_time'].append(session.trials.target_on_time + recording_date)\n",
    "    trial_data['go_cue_time'].append(session.trials.go_cue_time + recording_date)\n",
    "    trial_data['move_begins_time'].append(session.trials.move_begins_time + recording_date)\n",
    "    trial_data['move_ends_time'].append(session.trials.move_ends_time + recording_date)\n",
    "    trial_data['maze_condition'].append(session.trials.maze_condition)\n",
    "    trial_data['barriers'].append(session.trials.maze_num_barriers)\n",
    "    trial_data['targets'].append(session.trials.maze_num_targets)\n",
    "    trial_data['hit_position_x'].append([pos[0] for pos in session.trials.hit_target_position])\n",
    "    trial_data['hit_position_y'].append([pos[1] for pos in session.trials.hit_target_position])\n",
    "    trial_data['hit_position_angle'].append([np.degrees(np.arctan2(pos[1], pos[0])) for pos in session.trials.hit_target_position])\n",
    "\n",
    "# Concatenate all arrays into final datasets\n",
    "combined_hand_data = {}\n",
    "for key, arrays in hand_data.items():\n",
    "    combined_hand_data[key] = np.concatenate(arrays)\n",
    "\n",
    "combined_eye_data = {}\n",
    "for key, arrays in eye_data.items():\n",
    "    combined_eye_data[key] = np.concatenate(arrays)\n",
    "\n",
    "combined_trial_data = {}\n",
    "for key, arrays in trial_data.items():\n",
    "    combined_trial_data[key] = np.concatenate(arrays)\n",
    "\n",
    "# Create final DataFrames\n",
    "combined_hand_df = pd.DataFrame(combined_hand_data).set_index('timestamp')\n",
    "combined_eye_df = pd.DataFrame(combined_eye_data).set_index('timestamp')\n",
    "combined_trials_df = pd.DataFrame(combined_trial_data)\n",
    "\n",
    "# Add hit target position column\n",
    "combined_trials_df['hit_target_position'] = list(zip(\n",
    "    combined_trials_df['hit_position_x'],\n",
    "    combined_trials_df['hit_position_y']\n",
    "))\n",
    "\n",
    "# Create unified timestamp index from all data sources\n",
    "all_event_ts = np.concatenate([\n",
    "    combined_trials_df['target_on_time'].values,\n",
    "    combined_trials_df['go_cue_time'].values,\n",
    "    combined_trials_df['move_begins_time'].values,\n",
    "    combined_trials_df['move_ends_time'].values,\n",
    "])\n",
    "\n",
    "# Get unique timestamps across all data\n",
    "all_ts = np.unique(np.concatenate([\n",
    "    combined_hand_df.index.values,\n",
    "    combined_eye_df.index.values,\n",
    "    all_event_ts\n",
    "]))\n",
    "\n",
    "# Create master dataframe with unified timestamp index\n",
    "metadata = pd.DataFrame(index=all_ts)\n",
    "metadata.index.name = 'timestamp'\n",
    "\n",
    "# Merge hand and eye data\n",
    "metadata = metadata.join(combined_hand_df, how='left')\n",
    "metadata = metadata.join(combined_eye_df, how='left', rsuffix='_eye')\n",
    "\n",
    "# Add event column - mark timestamps that correspond to trial events\n",
    "event_map = {\n",
    "    'target_on_time': 'target_on',\n",
    "    'go_cue_time': 'go_cue',\n",
    "    'move_begins_time': 'move_begins',\n",
    "    'move_ends_time': 'move_ends',\n",
    "}\n",
    "event_col = pd.Series(index=metadata.index, dtype=\"object\")\n",
    "for col, label in event_map.items():\n",
    "    event_times = combined_trials_df[col].values\n",
    "    mask = np.isin(metadata.index.values, event_times)\n",
    "    event_col.iloc[mask] = label\n",
    "metadata['event'] = event_col\n",
    "\n",
    "# Add trial_idx column - assign each timestamp to its trial\n",
    "# Use binary search to efficiently find which trial each timestamp belongs to\n",
    "trial_idx_series = pd.Series(index=metadata.index, dtype='float64')\n",
    "\n",
    "# Sort trials by start time for binary search\n",
    "trial_sort_idx = np.argsort(combined_trials_df['start'].values)\n",
    "starts = combined_trials_df['start'].values[trial_sort_idx]\n",
    "ends = combined_trials_df['end'].values[trial_sort_idx]\n",
    "\n",
    "# Find potential trial for each timestamp\n",
    "timestamps = metadata.index.values\n",
    "start_positions = np.searchsorted(starts, timestamps, side='right') - 1\n",
    "\n",
    "# Check which timestamps are within valid trial intervals\n",
    "valid_mask = (start_positions >= 0) & (start_positions < len(starts))\n",
    "valid_positions = start_positions[valid_mask]\n",
    "valid_timestamps = timestamps[valid_mask]\n",
    "\n",
    "# Verify timestamps are before trial end times\n",
    "end_mask = valid_timestamps <= ends[valid_positions]\n",
    "final_valid_mask = np.zeros(len(timestamps), dtype=bool)\n",
    "final_valid_mask[valid_mask] = end_mask\n",
    "\n",
    "# Assign trial indices to timestamps\n",
    "trial_indices = np.full(len(timestamps), np.nan)\n",
    "trial_indices[final_valid_mask] = trial_sort_idx[valid_positions[end_mask]]\n",
    "trial_idx_series.iloc[:] = trial_indices\n",
    "\n",
    "metadata['trial_idx'] = trial_idx_series\n",
    "\n",
    "# Map trial properties using the trial indices\n",
    "metadata['maze_condition'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['maze_condition']\n",
    ")\n",
    "metadata['barriers'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['barriers']\n",
    ")\n",
    "metadata['targets'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['targets']\n",
    ")\n",
    "metadata['hit_position_x'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['hit_position_x']\n",
    ")\n",
    "metadata['hit_position_y'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['hit_position_y']\n",
    ")\n",
    "metadata['hit_position_angle'] = metadata['trial_idx'].astype('Int64').map(\n",
    "    combined_trials_df['hit_position_angle']\n",
    ")\n",
    "\n",
    "# Add movement_angle column based on position difference\n",
    "pos_delta_x = metadata['pos_x'].diff()\n",
    "pos_delta_y = metadata['pos_y'].diff()\n",
    "metadata['movement_angle'] = np.degrees(np.arctan2(pos_delta_y, pos_delta_x))\n",
    "\n",
    "# Calculate speed and acceleration magnitudes from vector components\n",
    "metadata['vel_magnitude'] = np.sqrt(\n",
    "    metadata['vel_x'].values**2 + metadata['vel_y'].values**2\n",
    ")\n",
    "metadata['accel_magnitude'] = np.sqrt(\n",
    "    metadata['acc_x'].values**2 + metadata['acc_y'].values**2\n",
    ")\n",
    "\n",
    "# Show result\n",
    "print(\"Metadata:\")\n",
    "display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bin the metadata to match spike counts.\"\"\"\n",
    "\n",
    "# Create metadata_binned with consistent timestamps\n",
    "ts = spk_cts_df.index.values\n",
    "metadata_binned = pd.DataFrame(index=pd.Index(ts, name='timestamp'))\n",
    "\n",
    "# Assign each metadata row to a bin index (half-open [t, t+bin))\n",
    "bin_ids = np.searchsorted(ts, metadata.index.values, side='right') - 1\n",
    "bin_ids = np.clip(bin_ids, 0, len(ts) - 1)\n",
    "\n",
    "# Handle event aggregation \n",
    "event_values = metadata['event'].values\n",
    "event_mask = pd.notna(event_values)\n",
    "\n",
    "if event_mask.any():\n",
    "    event_bin_ids = bin_ids[event_mask]\n",
    "    valid_events = event_values[event_mask].astype(str)\n",
    "    \n",
    "    # Create event assignments using vectorized operations\n",
    "    event_agg = np.full(len(ts), None, dtype=object)\n",
    "    \n",
    "    # Calculate target bins for all events at once\n",
    "    event_indices = np.arange(len(valid_events))\n",
    "    target_bins = event_bin_ids + event_indices\n",
    "    \n",
    "    # Only assign events that fall within valid bin range\n",
    "    valid_targets = target_bins < len(ts)\n",
    "    event_agg[target_bins[valid_targets]] = valid_events[valid_targets]\n",
    "else:\n",
    "    event_agg = np.full(len(ts), None, dtype=object)\n",
    "\n",
    "# Efficient nearest neighbor reindexing using searchsorted\n",
    "metadata_timestamps = metadata.index.values\n",
    "ts_positions = np.searchsorted(metadata_timestamps, ts, side='left')\n",
    "\n",
    "# Handle edge cases and find true nearest neighbors\n",
    "ts_positions = np.clip(ts_positions, 0, len(metadata_timestamps) - 1)\n",
    "\n",
    "# For positions not at the start, check if the previous position is closer\n",
    "mask = ts_positions > 0\n",
    "left_positions = ts_positions.copy()\n",
    "left_positions[mask] = ts_positions[mask] - 1\n",
    "\n",
    "# Calculate distances to determine nearest\n",
    "left_distances = np.abs(ts - metadata_timestamps[left_positions])\n",
    "right_distances = np.abs(ts - metadata_timestamps[ts_positions])\n",
    "\n",
    "# Choose the nearest position\n",
    "final_positions = np.where(left_distances < right_distances, left_positions, ts_positions)\n",
    "\n",
    "# Copy all columns from nearest metadata except event and trial_idx\n",
    "for col in metadata.columns:\n",
    "    if col not in ('event', 'trial_idx'):\n",
    "        metadata_binned[col] = metadata[col].iloc[final_positions].values\n",
    "\n",
    "# Insert distributed events\n",
    "metadata_binned['event'] = event_agg\n",
    "\n",
    "# Override trial_idx from GT intervals [start, end) to avoid NN bleed across gaps\n",
    "starts = combined_trials_df['start'].to_numpy()\n",
    "ends = combined_trials_df['end'].to_numpy()\n",
    "order = np.argsort(starts)\n",
    "s, e = starts[order], ends[order]\n",
    "\n",
    "left  = ts\n",
    "right = ts + bin_size  # each bin is [left, right)\n",
    "# Candidate trial: last trial that starts before this bin ends\n",
    "cand = np.searchsorted(s, right, side='right') - 1\n",
    "# Bin belongs if it overlaps trial interval\n",
    "valid = (cand >= 0) & (left < e[cand]) & (right > s[cand])\n",
    "\n",
    "trial_idx_binned = np.full(len(ts), np.nan)\n",
    "trial_idx_binned[valid] = order[cand[valid]]\n",
    "\n",
    "metadata_binned['trial_idx'] = pd.Series(\n",
    "    trial_idx_binned, index=metadata_binned.index, dtype='float64'\n",
    ")\n",
    "\n",
    "first = metadata_binned.groupby('trial_idx', sort=False).head(1).index\n",
    "last  = metadata_binned.groupby('trial_idx', sort=False).tail(1).index\n",
    "metadata_binned.loc[first, 'event'] = 'start'\n",
    "metadata_binned.loc[last.difference(first), 'event'] = 'end'\n",
    "\n",
    "# Fill \"A -> B\" on empty rows between events, per trial (keeps existing 'start'/'end'/'event' rows)\n",
    "metadata_binned['event'] = metadata_binned['event'].astype(object)\n",
    "prev_ev = metadata_binned.groupby('trial_idx', sort=False)['event'].ffill()\n",
    "next_ev = metadata_binned.groupby('trial_idx', sort=False)['event'].bfill()\n",
    "mask = metadata_binned['event'].isna() & prev_ev.notna() & next_ev.notna()\n",
    "metadata_binned.loc[mask, 'event'] = prev_ev[mask] + ' -> ' + next_ev[mask]\n",
    "allowed = {'start', 'start -> target_on', 'target_on', 'target_on -> go_cue', 'go_cue', 'go_cue -> move_begins',\n",
    "           'move_begins', 'move_begins -> move_ends', 'move_ends', 'move_ends -> end', 'end'}\n",
    "metadata_binned['event'] = metadata_binned['event'].where(metadata_binned['event'].isin(allowed))\n",
    "\n",
    "# Result: metadata for all trial bins, one row per bin\n",
    "print(\"Metadata binned:\")\n",
    "display(metadata_binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/val split and normalise with trial-aware shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = spk_cts_df.values.astype(np.float32)\n",
    "velocity = np.column_stack([\n",
    "    metadata_binned[\"vel_x\"].to_numpy(dtype=np.float32),\n",
    "    metadata_binned[\"vel_y\"].to_numpy(dtype=np.float32),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split trials into train/val sets (split by session)\n",
    "train_sessions = [0, 1, 2, 3]\n",
    "train_trials = metadata_binned[metadata_binned['session'].isin(train_sessions)]['trial_idx'].unique()\n",
    "if len(train_sessions) == len(sessions):\n",
    "    print(\"WARNING: All sessions are included in the training set. Validation set will be identical to training set.\")\n",
    "    val_trials = train_trials  # if all sessions are train, val is same as train\n",
    "else:\n",
    "    val_trials = metadata_binned[~metadata_binned['session'].isin(train_sessions)]['trial_idx'].unique()\n",
    "\n",
    "# # OR Split trials into train/val sets (80/20 split)\n",
    "# # Get unique trial indices and shuffle them\n",
    "# unique_trials = metadata_binned['trial_idx'].unique()\n",
    "# np.random.shuffle(unique_trials)\n",
    "# # Split \n",
    "# n_train_trials = int(len(unique_trials) * 0.8)\n",
    "# train_trials = unique_trials[:n_train_trials]\n",
    "# val_trials = unique_trials[n_train_trials:]\n",
    "\n",
    "# Create boolean masks for train/val based on trial membership\n",
    "train_mask = metadata_binned['trial_idx'].isin(train_trials)\n",
    "val_mask = metadata_binned['trial_idx'].isin(val_trials)\n",
    "\n",
    "# # Split \n",
    "# spikes_train = spikes[train_mask]\n",
    "# spikes_val = spikes[val_mask]\n",
    "# OR split and smooth spikes (avoiding cross-boundary leakage)\n",
    "sigma = 0.05 / bin_size\n",
    "spikes_train = gaussian_filter1d(spikes[train_mask], sigma=sigma, axis=0)\n",
    "spikes_val = gaussian_filter1d(spikes[val_mask], sigma=sigma, axis=0)\n",
    "\n",
    "# Normalize spikes (fit normalization on training data only)\n",
    "train_max = spikes_train.max()\n",
    "spikes_train = spikes_train / train_max\n",
    "spikes_val = spikes_val / train_max\n",
    "\n",
    "# Split velocity\n",
    "velocity_train = velocity[train_mask]\n",
    "velocity_val = velocity[val_mask]\n",
    "\n",
    "# Extract trial IDs for reference\n",
    "trial_ids_train = metadata_binned['trial_idx'][train_mask].values\n",
    "trial_ids_val = metadata_binned['trial_idx'][val_mask].values\n",
    "\n",
    "# Summary\n",
    "print(f\"Train set: {len(train_trials)} trials ({train_mask.sum()} time bins)\")\n",
    "print(f\"Val set: {len(val_trials)} trials ({val_mask.sum()} time bins)\")\n",
    "print(f\"Spike data shapes: train {spikes_train.shape}, val {spikes_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set SAE config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu for training\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device=}\")\n",
    "\n",
    "spk_train_t = t.from_numpy(spikes_train).to(device).to(dtype=t.bfloat16)\n",
    "\n",
    "dsae_topk_map = {256: 8, 512: 16, 1024: 24}\n",
    "dsae_topk_map = dict(sorted(dsae_topk_map.items()))  # ensure sorted from smallest to largest\n",
    "dsae_loss_x_map = {256: 1, 512: 1.25, 1024: 1.5}\n",
    "dsae_loss_x_map = dict(sorted(dsae_loss_x_map.items()))\n",
    "# dsae_topk_map = {1024: 12, 2048: 24, 4096: 48}\n",
    "dsae = max(dsae_topk_map.keys())\n",
    "n_inst = 2\n",
    "\n",
    "display(spk_train_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MSAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_cfg = mt.SaeConfig(\n",
    "    n_input_ae=spk_train_t.shape[1],   # input dimension = #units\n",
    "    dsae_topk_map=dsae_topk_map,\n",
    "    dsae_loss_x_map=dsae_loss_x_map,\n",
    "    seq_len=1,\n",
    "    n_instances=n_inst,\n",
    ")\n",
    "sae = mt.Sae(sae_cfg).to(device)\n",
    "loss_fn = mt.msle\n",
    "tau = 1.0\n",
    "lr = 5e-3\n",
    "\n",
    "n_epochs = 20\n",
    "batch_sz = 1024\n",
    "n_steps = (spk_train_t.shape[0] // batch_sz) * n_epochs\n",
    "log_freq = max(1, n_steps // n_epochs // 2)\n",
    "dead_neuron_window = max(1, n_steps // n_epochs // 3)\n",
    "\n",
    "data_log = mt.optimize(  # train model\n",
    "    spk_cts=spk_train_t,\n",
    "    sae=sae,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=t.optim.Adam(sae.parameters(), lr=lr),\n",
    "    use_lr_sched=True,\n",
    "    dead_neuron_window=dead_neuron_window,\n",
    "    n_steps=n_steps,\n",
    "    log_freq=log_freq,\n",
    "    batch_sz=batch_sz,\n",
    "    log_wandb=False,\n",
    "    plot_l0=False,\n",
    "    tau=tau,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check for nans in weights.\"\"\"\n",
    "\n",
    "sae.W_dec.isnan().sum(), sae.W_enc.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize weights.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for inst in range(n_inst):\n",
    "    W_dec_flat = asnumpy(sae.W_dec[inst].float()).ravel()\n",
    "    sns.histplot(W_dec_flat, bins=1000, stat=\"probability\", alpha=0.7, label=f\"SAE {inst}\")\n",
    "    \n",
    "ax.set_title(\"SAE decoder weights\")\n",
    "ax.set_xlabel(\"Weight value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize metrics over all examples and units.\"\"\"\n",
    "\n",
    "topk_acts_4d, recon_spk_cts, r2_per_unit, _, cossim_per_unit, _ = mt.eval_model(\n",
    "    spk_train_t, sae, batch_sz=batch_sz\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate variance explained of summed spike counts.\"\"\"\n",
    "\n",
    "n_recon_examples = recon_spk_cts.shape[0]\n",
    "recon_summed_spk_cts = reduce(recon_spk_cts, \"example inst unit -> example inst\", \"sum\")\n",
    "\n",
    "actual_summed_spk_cts = reduce(spk_train_t, \"example unit -> example\", \"sum\")\n",
    "actual_summed_spk_cts = actual_summed_spk_cts[:n_recon_examples]  # trim to match\n",
    "\n",
    "for inst in range(n_inst):\n",
    "    r2 = r2_score(\n",
    "        asnumpy(actual_summed_spk_cts.float()),\n",
    "        asnumpy(recon_summed_spk_cts[:, inst].float()),\n",
    "    )\n",
    "    print(f\"SAE instance {inst} R² (summed spike count over all units per example) = {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If cosine similarity is high but r2 is low, it suggests that the model is capturing the structure of the data but not the magnitude.\n",
    "# Calculate scale ratio of norms to check this\n",
    "\n",
    "# Expand to [n_examples, 1, n_units]\n",
    "spk_train_exp = spk_train_t[:n_recon_examples].unsqueeze(1)\n",
    "\n",
    "true_norms  = t.norm(spk_train_exp, dim=-1)   # [n_examples, 1]\n",
    "recon_norms = t.norm(recon_spk_cts, dim=-1)   # [n_examples, n_instances]\n",
    "scale = true_norms / recon_norms              # [n_examples, n_instances]\n",
    "\n",
    "print(scale.mean(dim=0))  # if it’s consistently >1 or <1, your model is biased in magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_train_trim = spk_train_t[:n_recon_examples]\n",
    "bias = (recon_spk_cts - spk_train_trim.unsqueeze(1)).mean(dim=0)\n",
    "print(bias.mean(dim=0))  # mean bias per unit, averaged across examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_var = spk_train_trim.var(dim=0).mean()\n",
    "pred_var = recon_spk_cts.var(dim=0).mean()\n",
    "print(f\"True variance: {true_var.item():.4f}, Pred variance: {pred_var.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bad units and retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold for removing units\n",
    "r2_thresh = 0.1\n",
    "inst = 0\n",
    "r2_inst = r2_per_unit[:, inst]              # [n_units]\n",
    "keep_mask = r2_inst > r2_thresh\n",
    "print(f\"frac units above {r2_thresh=}: {keep_mask.sum() / keep_mask.shape[0]:.2f}\")\n",
    "print(f\"Number to keep: {keep_mask.sum()} / {keep_mask.shape[0]}\")\n",
    "\n",
    "# Apply mask to train data (can be applied to val later)\n",
    "spk_train_pruned = spk_train_t[:, keep_mask]\n",
    "\n",
    "# Retrain SAE on pruned train data\n",
    "sae_cfg = mt.SaeConfig(\n",
    "    n_input_ae=spk_train_pruned.shape[1],\n",
    "    dsae_topk_map=dsae_topk_map,\n",
    "    dsae_loss_x_map=dsae_loss_x_map,\n",
    "    seq_len=1,\n",
    "    n_instances=n_inst,\n",
    ")\n",
    "sae = mt.Sae(sae_cfg).to(device)\n",
    "loss_fn = mt.msle\n",
    "tau = 1.0\n",
    "lr = 5e-3\n",
    "\n",
    "n_epochs = 20\n",
    "batch_sz = 1024\n",
    "n_steps = (spk_train_pruned.shape[0] // batch_sz) * n_epochs\n",
    "log_freq = max(1, n_steps // n_epochs // 2)\n",
    "dead_neuron_window = max(1, n_steps // n_epochs // 3)\n",
    "\n",
    "data_log = mt.optimize(\n",
    "    spk_cts=spk_train_pruned,\n",
    "    sae=sae,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=t.optim.Adam(sae.parameters(), lr=lr),\n",
    "    use_lr_sched=True,\n",
    "    dead_neuron_window=dead_neuron_window,\n",
    "    n_steps=n_steps,\n",
    "    log_freq=log_freq,\n",
    "    batch_sz=batch_sz,\n",
    "    log_wandb=False,\n",
    "    plot_l0=False,\n",
    "    tau=tau,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Re-visualize metrics over all examples and units.\"\"\"\n",
    "\n",
    "Xtr_t = spk_train_pruned\n",
    "\n",
    "topk_acts_4d_tr, recon_spk_cts_tr, r2_per_unit_tr, _, cossim_per_unit_tr, _ = mt.eval_model(\n",
    "    Xtr_t, sae, batch_sz=batch_sz\n",
    ")\n",
    "\n",
    "n_recon_examples_tr = recon_spk_cts_tr.shape[0]\n",
    "recon_summed_tr = reduce(recon_spk_cts_tr, \"example inst unit -> example inst\", \"sum\")\n",
    "\n",
    "actual_summed_tr = reduce(Xtr_t, \"example unit -> example\", \"sum\")\n",
    "actual_summed_tr = actual_summed_tr[:n_recon_examples_tr]\n",
    "\n",
    "for inst in range(n_inst):\n",
    "    r2 = r2_score(\n",
    "        asnumpy(actual_summed_tr.float()),\n",
    "        asnumpy(recon_summed_tr[:, inst].float()),\n",
    "    )\n",
    "    print(f\"[TRAIN] SAE instance {inst} R² (summed spike count per example) = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Do the same on validation data to check generalisation\"\"\"\n",
    "\n",
    "Xva_np = spikes_val[:, keep_mask]  # apply same unit mask\n",
    "Xva_t = t.from_numpy(Xva_np).to(Xtr_t.device).to(Xtr_t.dtype)\n",
    "\n",
    "topk_acts_4d_va, recon_spk_cts_va, r2_per_unit_va, _, cossim_per_unit_va, _ = mt.eval_model(\n",
    "    Xva_t, sae, batch_sz=batch_sz\n",
    ")\n",
    "\n",
    "n_recon_examples_va = recon_spk_cts_va.shape[0]\n",
    "recon_summed_va = reduce(recon_spk_cts_va, \"example inst unit -> example inst\", \"sum\")\n",
    "\n",
    "actual_summed_va = reduce(Xva_t, \"example unit -> example\", \"sum\")\n",
    "actual_summed_va = actual_summed_va[:n_recon_examples_va]\n",
    "\n",
    "for inst in range(n_inst):\n",
    "    r2 = r2_score(\n",
    "        asnumpy(actual_summed_va.float()),\n",
    "        asnumpy(recon_summed_va[:, inst].float()),\n",
    "    )\n",
    "    print(f\"[VAL] SAE instance {inst} R² (summed spike count per example) = {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"Load saved activations (Etr/Eval) if available; otherwise (only if save_activations=True) densify from top-k and save.\"\"\"\n",
    "\n",
    "load_activations = False\n",
    "save_activations = True\n",
    "dense_activations_file = \"sae_dense_activations_and_targets.npz\"\n",
    "sparse_activations_file_tr = \"sae_sparse_activations_train.csv\"\n",
    "sparse_activations_file_va = \"sae_sparse_activations_val.csv\"\n",
    "\n",
    "# Build save path (same style as before)\n",
    "session_dates = []\n",
    "for session in sessions:\n",
    "    session_date = datetime.fromtimestamp(session.session.recording_date).strftime(\"%Y%m%d\")\n",
    "    session_dates.append(session_date)\n",
    "session_dates_str = \"_\".join(session_dates)\n",
    "\n",
    "activations_save_path = data_path / f\"{subject_name}_{session_dates_str}\" / \"sae_activations_all_sessions_w_smoothing\"\n",
    "activations_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if load_activations and activations_save_path.exists():\n",
    "    # Load pre-saved activations/targets (cast to float32)\n",
    "    data = np.load(activations_save_path / dense_activations_file, allow_pickle=True)\n",
    "    Etr, Eval = data[\"Etr\"].astype(np.float32), data[\"Eval\"].astype(np.float32)\n",
    "    ytr, yva  = data[\"ytr\"].astype(np.float32), data[\"yva\"].astype(np.float32)\n",
    "\n",
    "    acts_df_tr = pd.read_csv(\n",
    "        activations_save_path / sparse_activations_file_tr,\n",
    "        dtype={\"example_idx\": int, \"instance_idx\": int, \"feature_idx\": int, \"activation_value\": np.float32}\n",
    "    )\n",
    "    acts_df_va = pd.read_csv(\n",
    "        activations_save_path / sparse_activations_file_va,\n",
    "        dtype={\"example_idx\": int, \"instance_idx\": int, \"feature_idx\": int, \"activation_value\": np.float32}\n",
    "    )\n",
    "    print(f\"Loaded activations from {activations_save_path}\")\n",
    "else:\n",
    "    # Build Etr/Eval from top-k activations (sparse → dense)\n",
    "    # Train\n",
    "    arr_tr = asnumpy(topk_acts_4d_tr)  # [example_idx, instance_idx, feature_idx, activation_value]\n",
    "    # Sparse activations (tight dtypes on indices, fp32 values)\n",
    "    acts_df_tr = pd.DataFrame({\n",
    "        \"example_idx\":      arr_tr[:, 0].astype(int),\n",
    "        \"instance_idx\":     arr_tr[:, 1].astype(int),\n",
    "        \"feature_idx\":      arr_tr[:, 2].astype(int),\n",
    "        \"activation_value\": arr_tr[:, 3].astype(np.float32),\n",
    "    })\n",
    "    # Dense activations\n",
    "    N_tr = int(arr_tr[:, 0].max()) + 1 if arr_tr.size else 0\n",
    "    Etr = np.zeros((N_tr, n_inst * dsae), dtype=np.float32)\n",
    "    cols_tr = (arr_tr[:, 1].astype(np.int64) * int(dsae)) + arr_tr[:, 2].astype(np.int64)\n",
    "    Etr[arr_tr[:, 0].astype(np.int64), cols_tr] = arr_tr[:, 3].astype(np.float32)\n",
    "\n",
    "    # Val\n",
    "    arr_va = asnumpy(topk_acts_4d_va)\n",
    "    acts_df_va = pd.DataFrame({\n",
    "        \"example_idx\":      arr_va[:, 0].astype(int),\n",
    "        \"instance_idx\":     arr_va[:, 1].astype(int),\n",
    "        \"feature_idx\":      arr_va[:, 2].astype(int),\n",
    "        \"activation_value\": arr_va[:, 3].astype(np.float32),\n",
    "    })\n",
    "    N_va = int(arr_va[:, 0].max()) + 1 if arr_va.size else 0\n",
    "    Eval = np.zeros((N_va, n_inst * dsae), dtype=np.float32)\n",
    "    cols_va = (arr_va[:, 1].astype(np.int64) * int(dsae)) + arr_va[:, 2].astype(np.int64)\n",
    "    Eval[arr_va[:, 0].astype(np.int64), cols_va] = arr_va[:, 3].astype(np.float32)\n",
    "\n",
    "    # Targets aligned to available rows (float32)\n",
    "    ytr = velocity_train[:N_tr].astype(np.float32)\n",
    "    yva = velocity_val[:N_va].astype(np.float32)\n",
    "\n",
    "    # Prune zero/near-zero-variance (already in fp32)\n",
    "    std_tr = Etr.std(axis=0, ddof=0)\n",
    "    keep = std_tr > 1e-6\n",
    "    # # Optional: also require a few nonzeros to avoid “almost dead” columns\n",
    "    # nnz_tr = (Etr != 0).sum(axis=0)\n",
    "    # keep &= nnz_tr >= 5\n",
    "\n",
    "    dropped = int((~keep).sum())\n",
    "    if dropped:\n",
    "        # Prune sparse activations\n",
    "        keep_cols = np.nonzero(keep)[0]\n",
    "        keep_inst = (keep_cols // dsae).astype(int)\n",
    "        keep_feat = (keep_cols %  dsae).astype(int)\n",
    "        _keep_pairs = pd.DataFrame({\"instance_idx\": keep_inst, \"feature_idx\": keep_feat})\n",
    "        acts_df_tr = acts_df_tr.merge(_keep_pairs, on=[\"instance_idx\", \"feature_idx\"], how=\"inner\")\n",
    "        acts_df_va = acts_df_va.merge(_keep_pairs, on=[\"instance_idx\", \"feature_idx\"], how=\"inner\")\n",
    "\n",
    "        # Prune dense activations\n",
    "        Etr = Etr[:, keep]\n",
    "        Eval = Eval[:, keep]\n",
    "        print(f\"Pruned {dropped} / {keep.size} features. New D = {Etr.shape[1]}\")\n",
    "\n",
    "    if save_activations:\n",
    "        np.savez(activations_save_path / dense_activations_file, Etr=Etr, Eval=Eval, ytr=ytr, yva=yva)\n",
    "        acts_df_tr.to_csv(activations_save_path / sparse_activations_file_tr, index=False)\n",
    "        acts_df_va.to_csv(activations_save_path / sparse_activations_file_va, index=False)\n",
    "        print(f\"Saved activations to {activations_save_path}\")\n",
    "\n",
    "print(f\"Dense activations: \\nEtr shape: {Etr.shape}, Eval shape: {Eval.shape}\")\n",
    "print(f\"Sparse activations: \\nTrain shape: {acts_df_tr.shape}, Val shape: {acts_df_va.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check feature variance, sparsity, and conditioning\"\"\"\n",
    "\n",
    "X = Etr  # or Eval\n",
    "std = X.std(axis=0, ddof=0)\n",
    "nnz = (X != 0).sum(axis=0)\n",
    "\n",
    "print(\"zero columns:\", (std == 0).sum(), \" / \", X.shape[1])\n",
    "print(\"near-constant (std<1e-6):\", (std < 1e-6).sum())\n",
    "print(\"median nnz per column:\", int(np.median(nnz)))\n",
    "\n",
    "# Rough condition number on a smaller slice (upcast to fp32 for linalg)\n",
    "r = min(5000, X.shape[0])\n",
    "svals = np.linalg.svd(X[:r], compute_uv=False)\n",
    "cond = svals.max() / max(svals.min(), 1e-12)\n",
    "print(\"approx cond:\", cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train on Etr, decode on Eval with a small lag sweep.\"\"\"\n",
    "\n",
    "def apply_lag(E_tr, E_va, y_tr, y_va, lag_bins: int):\n",
    "    if lag_bins > 0:\n",
    "        Et, yt = E_tr[:-lag_bins], y_tr[lag_bins:]\n",
    "        Ev, yv = E_va[:-lag_bins], y_va[lag_bins:]\n",
    "    elif lag_bins < 0:\n",
    "        k = -lag_bins\n",
    "        Et, yt = E_tr[k:], y_tr[:-k]\n",
    "        Ev, yv = E_va[k:], y_va[:-k]\n",
    "    else:\n",
    "        Et, yt, Ev, yv = E_tr, y_tr, E_va, y_va\n",
    "    return Et, Ev, yt, yv\n",
    "\n",
    "best = {\"lag\": 0, \"r2_mean\": -np.inf, \"r2_per_dim\": None, \"decoder\": None}\n",
    "\n",
    "for lag in range(0, 6):\n",
    "    Et, Ev, yt, yv = apply_lag(Etr, Eval, ytr, yva, lag)\n",
    "\n",
    "    # Train on train, evaluate on val\n",
    "    decoder = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        Ridge(alpha=30.0, solver=\"lsqr\"),\n",
    "    )\n",
    "    decoder.fit(Et, yt)\n",
    "    y_pred = decoder.predict(Ev)\n",
    "\n",
    "    r2_per_dim = r2_score(yv, y_pred, multioutput='raw_values')\n",
    "    r2_mean = float(np.mean(r2_per_dim))\n",
    "\n",
    "    if r2_mean > best[\"r2_mean\"]:\n",
    "        best.update(lag=lag, r2_mean=r2_mean, r2_per_dim=r2_per_dim, decoder=decoder)\n",
    "\n",
    "print(f\"Best lag (bins): {best['lag']}\")\n",
    "print(f\"R² per dimension: {best['r2_per_dim']}\")\n",
    "print(f\"Mean R²: {best['r2_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hunt for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick whether to find features in training or validation set\n",
    "search_set = \"train\" # or \"train\" or \"val\"\n",
    "\n",
    "if search_set == \"train\":\n",
    "    acts_df = acts_df_tr\n",
    "    metadata_binned_subset = metadata_binned[train_mask].copy()\n",
    "else:\n",
    "    acts_df = acts_df_va\n",
    "    metadata_binned_subset = metadata_binned[val_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Automatically map features to metadata\"\"\"\n",
    "\n",
    "def analyze_discrete_variable(\n",
    "    acts_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    variable: str,\n",
    "    min_activation_frac: float\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyzes a discrete variable, calculating activation metrics for features meeting a minimum activation fraction.\n",
    "    This version is corrected to handle features that are 100% selective for an event.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    unique_values = metadata_binned[variable].dropna().unique()\n",
    "\n",
    "    for value in unique_values:\n",
    "        try:\n",
    "            event_idxs = np.where(metadata_binned[variable] == value)[0]\n",
    "            if len(event_idxs) == 0:\n",
    "                continue\n",
    "\n",
    "            event_acts_df = acts_df[acts_df[\"example_idx\"].isin(event_idxs)]\n",
    "            if len(event_acts_df) == 0:\n",
    "                continue\n",
    "\n",
    "            event_features_df = event_acts_df.groupby([\"instance_idx\", \"feature_idx\"]).agg(\n",
    "                activation_count=(\"activation_value\", \"count\")\n",
    "            ).reset_index()\n",
    "            n_event_examples = len(event_idxs)\n",
    "            event_features_df[\"activation_frac_event\"] = event_features_df[\"activation_count\"] / n_event_examples\n",
    "\n",
    "            promising_features = event_features_df[event_features_df[\"activation_frac_event\"] >= min_activation_frac]\n",
    "            if promising_features.empty:\n",
    "                continue\n",
    "\n",
    "            non_event_mask = ~acts_df[\"example_idx\"].isin(event_idxs)\n",
    "            non_event_acts_df = acts_df[non_event_mask].merge(\n",
    "                promising_features[[\"instance_idx\", \"feature_idx\"]],\n",
    "                on=[\"instance_idx\", \"feature_idx\"], how=\"inner\"\n",
    "            )\n",
    "\n",
    "            if not non_event_acts_df.empty:\n",
    "                non_event_features_df = non_event_acts_df.groupby([\"instance_idx\", \"feature_idx\"]).agg(\n",
    "                    activation_count=(\"activation_value\", \"count\")\n",
    "                ).reset_index()\n",
    "                n_non_event_examples = len(metadata_binned) - n_event_examples\n",
    "                non_event_features_df[\"activation_frac_non_event\"] = non_event_features_df[\"activation_count\"] / n_non_event_examples\n",
    "                ratio_df = promising_features.merge(\n",
    "                    non_event_features_df, on=[\"instance_idx\", \"feature_idx\"], how=\"left\"\n",
    "                )\n",
    "                ratio_df[\"activation_frac_non_event\"] = ratio_df[\"activation_frac_non_event\"].fillna(0.0)\n",
    "            else:\n",
    "                ratio_df = promising_features.copy()\n",
    "                ratio_df[\"activation_frac_non_event\"] = 0.0\n",
    "            \n",
    "            ratio_df[\"activation_ratio\"] = ratio_df[\"activation_frac_event\"] / (ratio_df[\"activation_frac_non_event\"] + 1e-9)\n",
    "            ratio_df[\"rate_proportion\"] = ratio_df[\"activation_frac_event\"] / (ratio_df[\"activation_frac_event\"] + ratio_df[\"activation_frac_non_event\"])\n",
    "\n",
    "            for _, row in ratio_df.iterrows():\n",
    "                results.append({\n",
    "                    'variable': variable, 'variable_type': 'discrete', 'value': value,\n",
    "                    'instance_idx': row['instance_idx'], 'feature_idx': row['feature_idx'],\n",
    "                    'activation_ratio': row['activation_ratio'],\n",
    "                    'activation_frac_during': row['activation_frac_event'],\n",
    "                    'activation_frac_outside': row['activation_frac_non_event'],\n",
    "                    'rate_proportion': row['rate_proportion']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Could not analyze {variable}={value}: {e}\")\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def analyze_continuous_variable(\n",
    "    acts_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    variable: str,\n",
    "    n_bins: int,\n",
    "    min_activation_frac: float\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyzes a continuous variable by binning it and then using the discrete analysis method.\n",
    "    \"\"\"\n",
    "    print(f\"  Binning '{variable}' into {n_bins} bins...\")\n",
    "    binned_col_name = f\"{variable}_binned\"\n",
    "\n",
    "    data_to_bin = metadata_binned[variable].dropna()\n",
    "    if data_to_bin.empty:\n",
    "        return []\n",
    "\n",
    "    if variable == 'movement_angle':\n",
    "        bins = np.linspace(-180, 180, n_bins + 1)\n",
    "        labels = [f\"({bins[i]:.0f}, {bins[i+1]:.0f}]\" for i in range(n_bins)]\n",
    "        metadata_binned[binned_col_name] = pd.cut(data_to_bin, bins=bins, labels=labels, include_lowest=True)\n",
    "    else:\n",
    "        metadata_binned[binned_col_name] = pd.qcut(data_to_bin, q=n_bins, labels=None, duplicates='drop')\n",
    "\n",
    "    results = analyze_discrete_variable(acts_df, metadata_binned, binned_col_name, min_activation_frac)\n",
    "\n",
    "    for res in results:\n",
    "        res['variable'] = variable\n",
    "        res['variable_type'] = 'continuous'\n",
    "\n",
    "    return results\n",
    "\n",
    "def map_features_to_metadata(\n",
    "    acts_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    discrete_vars: List[str] = None,\n",
    "    continuous_vars: List[str] = None,\n",
    "    exclude_columns: List[str] = None,\n",
    "    min_activation_frac: float = 0.1,\n",
    "    n_bins_continuous: int = 10,\n",
    "    top_n_features: int = 3\n",
    ") -> pd.DataFrame:  \n",
    "    \"\"\"\n",
    "    Automatically maps SAE features to metadata by finding top N features for each condition.\n",
    "    Returns a single DataFrame with both discrete and continuous results.\n",
    "    \"\"\"\n",
    "    if discrete_vars is None: discrete_vars = []\n",
    "    if continuous_vars is None: continuous_vars = []\n",
    "    if exclude_columns is None: exclude_columns = ['trial_idx', 'session']\n",
    "    \n",
    "    all_results = []\n",
    "    print(\"🚀 Starting automated feature-to-metadata mapping...\")\n",
    "    \n",
    "    for variable in metadata_binned.columns:\n",
    "        if variable in exclude_columns:\n",
    "            continue\n",
    "        print(f\"\\nAnalyzing variable: {variable}\")\n",
    "        \n",
    "        if variable in discrete_vars:\n",
    "            print(f\"  Treating as: discrete\")\n",
    "            results = analyze_discrete_variable(acts_df, metadata_binned, variable, min_activation_frac)\n",
    "            all_results.extend(results)\n",
    "        elif variable in continuous_vars:\n",
    "            print(f\"  Treating as: continuous\")\n",
    "            results = analyze_continuous_variable(acts_df, metadata_binned, variable, n_bins=n_bins_continuous, min_activation_frac=min_activation_frac)\n",
    "            all_results.extend(results)\n",
    "        else:\n",
    "            print(f\"  Skipping (not in discrete_vars or continuous_vars list)\")\n",
    "            continue\n",
    "        print(f\"  Found {len(results)} potential associations.\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"\\nNo associations found meeting the minimum activation fraction!\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame instead of tuple\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df['value'] = results_df['value'].astype(str)\n",
    "    \n",
    "    print(f\"\\nRanking features and selecting top {top_n_features} for each condition...\")\n",
    "    ranked_df = (\n",
    "        results_df.sort_values('activation_ratio', ascending=False)\n",
    "        .groupby(['variable', 'value', 'instance_idx'])\n",
    "        .head(top_n_features)\n",
    "    )\n",
    "    \n",
    "    # Sort the combined results\n",
    "    sort_order = ['variable_type', 'variable', 'value', 'instance_idx', 'activation_ratio']\n",
    "    ascending_order = [True, True, True, True, False]\n",
    "    \n",
    "    final_df = ranked_df.sort_values(by=sort_order, ascending=ascending_order).reset_index(drop=True)\n",
    "    \n",
    "    discrete_count = len(final_df[final_df['variable_type'] == 'discrete'])\n",
    "    continuous_count = len(final_df[final_df['variable_type'] == 'continuous'])\n",
    "    \n",
    "    print(f\"\\n✅ Found {discrete_count} top discrete associations.\")\n",
    "    print(f\"✅ Found {continuous_count} top continuous associations.\")\n",
    "    print(f\"✅ Total: {len(final_df)} associations returned in single DataFrame.\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# discrete_vars = ['event', 'maze_condition', 'barriers', 'targets', 'hit_position_x', 'hit_position_y', 'hit_position_angle']\n",
    "# continuous_vars = ['vel_magnitude', 'accel_magnitude', 'movement_angle']\n",
    "discrete_vars = ['event', 'maze_condition']\n",
    "continuous_vars = ['vel_magnitude', 'accel_magnitude']\n",
    "\n",
    "\n",
    "results = map_features_to_metadata(\n",
    "    acts_df, metadata_binned_subset,\n",
    "    discrete_vars=discrete_vars,\n",
    "    continuous_vars=continuous_vars,\n",
    "    min_activation_frac=0.5,\n",
    "    n_bins_continuous=12,\n",
    "    top_n_features=3\n",
    ")\n",
    "display(results)\n",
    "\n",
    "# # Optional filtering (ratio > 2.0, proportion > 0.5)\n",
    "# results = results[\n",
    "#     (results['activation_ratio'] > 2.0) & \n",
    "#     (results['rate_proportion'] > 0.5)\n",
    "# ].reset_index(drop=True)\n",
    "# display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate z-scores for spike counts across neurons.\"\"\"\n",
    "# Calculate mean and standard deviation for each neuron (column)\n",
    "neuron_means = spk_cts_df.mean(axis=0)\n",
    "neuron_stds = spk_cts_df.std(axis=0)\n",
    "\n",
    "# Calculate z-scores\n",
    "# Handle cases where standard deviation is zero to avoid division by zero\n",
    "spk_z_scores_df = spk_cts_df.sub(neuron_means, axis=1).div(neuron_stds, axis=1)\n",
    "spk_z_scores_df = spk_z_scores_df.replace([np.inf, -np.inf], np.nan) # Replace inf with NaN for clarity\n",
    "\n",
    "# Set z-score to 0 where standard deviation was 0 (and thus z-score would be NaN)\n",
    "spk_z_scores_df = spk_z_scores_df.fillna(0.0)\n",
    "\n",
    "display(spk_z_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualisation functions for event-feature associations\"\"\"\n",
    "\n",
    "# make sure these only get created once\n",
    "_canonical_cache = globals().setdefault('_canonical_cache', {})\n",
    "_warp_cache = globals().setdefault('_warp_cache', {})\n",
    "\n",
    "def create_canonical_timeline(combined_trials_df, maze_conditions=None, hit_target_positions=None):\n",
    "    \"\"\"\n",
    "    Create (or reuse) a canonical timeline based on average event durations.\n",
    "    Uses a simple cache so repeated calls with the same filters reuse results.\n",
    "    \"\"\"\n",
    "    # --- normalize filters BEFORE keying ---\n",
    "    # maze_conditions -> sorted tuple or None\n",
    "    if maze_conditions is not None:\n",
    "        maze_conditions = tuple(sorted(maze_conditions))\n",
    "    # hit_target_positions: coerce lists/ndarrays->tuples, then sort\n",
    "    if hit_target_positions is not None:\n",
    "        if len(hit_target_positions) > 0 and not isinstance(hit_target_positions[0], tuple):\n",
    "            hit_target_positions = [\n",
    "                tuple(pos) if isinstance(pos, (list, np.ndarray)) else pos\n",
    "                for pos in hit_target_positions\n",
    "            ]\n",
    "        hit_target_positions = tuple(sorted(hit_target_positions))\n",
    "\n",
    "    key = (maze_conditions, hit_target_positions)\n",
    "\n",
    "    if key in _canonical_cache:\n",
    "        print(f\"Reusing cached canonical timeline for key={key}\")\n",
    "        return _canonical_cache[key]\n",
    "\n",
    "    # --- filter trials ---\n",
    "    filtered_trials = combined_trials_df.copy()\n",
    "    if maze_conditions is not None:\n",
    "        filtered_trials = filtered_trials[filtered_trials['maze_condition'].isin(maze_conditions)]\n",
    "        print(f\"Filtered to maze conditions: {maze_conditions}\")\n",
    "    if hit_target_positions is not None:\n",
    "        filtered_trials = filtered_trials[filtered_trials['hit_target_position'].isin(hit_target_positions)]\n",
    "        print(f\"Filtered to target positions: {hit_target_positions}\")\n",
    "\n",
    "    print(f\"Using {len(filtered_trials)} trials (from {len(combined_trials_df)} total) to create canonical timeline\")\n",
    "    if len(filtered_trials) == 0:\n",
    "        print(\"No trials match the filtering criteria!\")\n",
    "        return None, None\n",
    "\n",
    "    # --- compute canonical ---\n",
    "    events_sequence = ['start', 'target_on_time', 'go_cue_time', 'move_begins_time', 'move_ends_time', 'end']\n",
    "    for event in events_sequence:\n",
    "        filtered_trials = filtered_trials[filtered_trials[event].notna()]\n",
    "    if len(filtered_trials) == 0:\n",
    "        print(\"No trials have all required events!\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Computing durations from {len(filtered_trials)} complete trials\")\n",
    "    durations = {}\n",
    "    for i in range(len(events_sequence) - 1):\n",
    "        e1, e2 = events_sequence[i], events_sequence[i + 1]\n",
    "        td = filtered_trials[e2] - filtered_trials[e1]\n",
    "        durations[f\"{e1}_to_{e2}\"] = td.mean()\n",
    "        print(f\"  {e1} to {e2}: {td.mean():.3f}s (±{td.std():.3f})\")\n",
    "\n",
    "    canonical_events = {'start': 0.0}\n",
    "    t = 0.0\n",
    "    for i in range(len(events_sequence) - 1):\n",
    "        k = f\"{events_sequence[i]}_to_{events_sequence[i+1]}\"\n",
    "        t += durations[k]\n",
    "        canonical_events[events_sequence[i+1]] = t\n",
    "\n",
    "    print(f\"\\nCanonical timeline: {canonical_events}\")\n",
    "    print(f\"Total canonical duration: {t:.3f}s\")\n",
    "\n",
    "    # cache & return\n",
    "    _canonical_cache[key] = (canonical_events, filtered_trials)\n",
    "    return canonical_events, filtered_trials\n",
    "\n",
    "def warp_trials_to_canonical_timeline(combined_trials_df, acts_df, spk_z_scores_df, metadata_binned,\n",
    "                                      instance_idx=0, feature_idx=None,\n",
    "                                      maze_conditions=None, hit_target_positions=None):\n",
    "    \"\"\"\n",
    "    Warp filtered trials to a (cached) canonical timeline. Vectorised for speed.\n",
    "    \"\"\"\n",
    "    # 1) Canonical (already cached by create_canonical_timeline)\n",
    "    canonical_events, filtered_trials = create_canonical_timeline(\n",
    "        combined_trials_df, maze_conditions, hit_target_positions\n",
    "    )\n",
    "    if canonical_events is None:\n",
    "        return None, None, None\n",
    "\n",
    "    # 2) Resolve feature\n",
    "    if feature_idx is None:\n",
    "        instance_acts = acts_df[acts_df['instance_idx'] == instance_idx]\n",
    "        if len(instance_acts) == 0:\n",
    "            print(f\"No activations found for instance {instance_idx}\")\n",
    "            return None, None, None\n",
    "        feature_idx = int(instance_acts['feature_idx'].value_counts().index[0])\n",
    "\n",
    "    # 3) Warp cache key\n",
    "    mc_key = tuple(sorted(maze_conditions)) if maze_conditions is not None else None\n",
    "    tp_key = hit_target_positions\n",
    "    if tp_key is not None:\n",
    "        if len(tp_key) > 0 and not isinstance(tp_key[0], tuple):\n",
    "            tp_key = [tuple(pos) if isinstance(pos, (list, np.ndarray)) else pos for pos in tp_key]\n",
    "        tp_key = tuple(sorted(tp_key))\n",
    "    warp_key = (instance_idx, feature_idx, mc_key, tp_key)\n",
    "\n",
    "    if warp_key in _warp_cache:\n",
    "        print(f\"Reusing cached warped data for key={warp_key}\")\n",
    "        return _warp_cache[warp_key]\n",
    "\n",
    "    # 4) Slice activations once\n",
    "    feature_acts = acts_df[\n",
    "        (acts_df['instance_idx'] == instance_idx) &\n",
    "        (acts_df['feature_idx'] == feature_idx)\n",
    "    ][['example_idx', 'activation_value']].copy()\n",
    "\n",
    "    # 5) Choose top unit (vectorised)\n",
    "    if len(feature_acts) > 0:\n",
    "        active_rows = feature_acts['example_idx'].values\n",
    "        unit_mean_zscores = spk_z_scores_df.iloc[active_rows].mean(axis=0)\n",
    "        top_unit = unit_mean_zscores.idxmax()\n",
    "    else:\n",
    "        top_unit = spk_z_scores_df.mean(axis=0).idxmax()\n",
    "\n",
    "    # 6) Canonical time axis\n",
    "    canonical_duration = float(max(canonical_events.values()))\n",
    "    n_bins = max(int(canonical_duration / 0.05), 1)\n",
    "    canonical_time_axis = np.linspace(0.0, canonical_duration, n_bins)\n",
    "    step = canonical_time_axis[1] - canonical_time_axis[0] if n_bins > 1 else 1.0\n",
    "\n",
    "    required_events = ['start', 'target_on_time', 'go_cue_time', 'move_begins_time', 'move_ends_time', 'end']\n",
    "\n",
    "    # Precompute a quick view for timestamps (numpy array for speed)\n",
    "    mb_index = metadata_binned.index.values\n",
    "\n",
    "    warped_feature_acts = []\n",
    "    warped_unit_acts = []\n",
    "    trial_info_list = []\n",
    "    valid_trials = 0\n",
    "\n",
    "    print(\"Warping trials...\")\n",
    "\n",
    "    # 7) (Tiny) helper for scattering with duplicate indices -> keep max\n",
    "    def scatter_max(dst_len, idx, vals):\n",
    "        dst = np.zeros(dst_len, dtype=float)\n",
    "        if idx.size:\n",
    "            # clamp indices to bounds\n",
    "            idx = np.clip(idx, 0, dst_len - 1)\n",
    "            # aggregate duplicates via max\n",
    "            # argsort, then group\n",
    "            order = np.argsort(idx)\n",
    "            idx_sorted = idx[order]\n",
    "            vals_sorted = vals[order]\n",
    "            # find boundaries\n",
    "            bounds = np.flatnonzero(np.diff(idx_sorted)) + 1\n",
    "            starts = np.concatenate(([0], bounds))\n",
    "            ends = np.concatenate((bounds, [idx_sorted.size]))\n",
    "            # reduce each run\n",
    "            for s, e in zip(starts, ends):\n",
    "                i = idx_sorted[s]\n",
    "                dst[i] = max(dst[i], np.max(vals_sorted[s:e]))\n",
    "        return dst\n",
    "\n",
    "    # 8) Pre-index feature acts by example_idx for fast per-trial slicing\n",
    "    #    (keeps it simple: per-trial merge on tiny arrays)\n",
    "    feature_acts_sorted = feature_acts.sort_values('example_idx', kind='mergesort')\n",
    "    fa_idx = feature_acts_sorted['example_idx'].values\n",
    "    fa_val = feature_acts_sorted['activation_value'].values\n",
    "\n",
    "    # 9) Iterate trials, but vectorise inside each trial\n",
    "    for trial_idx, trial in filtered_trials.iterrows():\n",
    "        # ensure required events present\n",
    "        if any(pd.isna(trial[ev]) for ev in required_events):\n",
    "            continue\n",
    "\n",
    "        # original & canonical event times\n",
    "        orig_ev = np.array([trial[ev] for ev in required_events], dtype=float)\n",
    "        cano_ev = np.array([canonical_events[ev] for ev in required_events], dtype=float)\n",
    "\n",
    "        # trial bin indices\n",
    "        t0, t1 = float(trial['start']), float(trial['end'])\n",
    "        trial_mask = (mb_index >= t0) & (mb_index <= t1)\n",
    "        if not trial_mask.any():\n",
    "            continue\n",
    "        trial_bins = np.nonzero(trial_mask)[0]\n",
    "\n",
    "        # warp all timestamps for this trial at once\n",
    "        warped_times = np.interp(mb_index[trial_bins], orig_ev, cano_ev)\n",
    "        time_idx = np.rint(warped_times / step).astype(int)\n",
    "        time_idx = np.clip(time_idx, 0, n_bins - 1)\n",
    "\n",
    "        # --- feature activations for these bins (vectorised) ---\n",
    "        # select rows of feature_acts that fall into trial_bins\n",
    "        # both arrays are sorted -> use searchsorted for fast intersection\n",
    "        pos_lo = np.searchsorted(fa_idx, trial_bins[0], side='left')\n",
    "        pos_hi = np.searchsorted(fa_idx, trial_bins[-1], side='right')\n",
    "        fa_slice_idx = fa_idx[pos_lo:pos_hi]\n",
    "        fa_slice_val = fa_val[pos_lo:pos_hi]\n",
    "        # filter to exact membership\n",
    "        mask_in = np.isin(fa_slice_idx, trial_bins, assume_unique=False)\n",
    "        fa_slice_idx = fa_slice_idx[mask_in]\n",
    "        fa_slice_val = fa_slice_val[mask_in]\n",
    "        if fa_slice_idx.size:\n",
    "            # map example_idx -> local position in trial_bins\n",
    "            loc = np.searchsorted(trial_bins, fa_slice_idx)\n",
    "            loc = np.clip(loc, 0, trial_bins.size - 1)\n",
    "            # scatter into canonical positions (use max if collisions)\n",
    "            feat_vec = scatter_max(n_bins, time_idx[loc], fa_slice_val.astype(float))\n",
    "        else:\n",
    "            feat_vec = np.zeros(n_bins, dtype=float)\n",
    "\n",
    "        # --- unit z-scores for these bins (vectorised) ---\n",
    "        unit_vals = spk_z_scores_df.iloc[trial_bins][top_unit].to_numpy(dtype=float, copy=False)\n",
    "        unit_vec = scatter_max(n_bins, time_idx, unit_vals)\n",
    "\n",
    "        warped_feature_acts.append(feat_vec)\n",
    "        warped_unit_acts.append(unit_vec)\n",
    "\n",
    "        tr = trial.copy()\n",
    "        tr['trial_idx'] = trial_idx\n",
    "        trial_info_list.append(tr)\n",
    "        valid_trials += 1\n",
    "\n",
    "    print(f\"Successfully warped {valid_trials} trials\")\n",
    "    if valid_trials == 0:\n",
    "        return None, feature_idx, top_unit\n",
    "\n",
    "    warped_feature_acts = np.stack(warped_feature_acts, axis=0)\n",
    "    warped_unit_acts = np.stack(warped_unit_acts, axis=0)\n",
    "    trial_info_df = pd.DataFrame(trial_info_list).reset_index(drop=True)\n",
    "\n",
    "    warped_data = {\n",
    "        'feature_activations': warped_feature_acts,\n",
    "        'unit_activations': warped_unit_acts,\n",
    "        'trial_info': trial_info_df,\n",
    "        'canonical_time_axis': canonical_time_axis,\n",
    "        'canonical_events': canonical_events\n",
    "    }\n",
    "\n",
    "    _warp_cache[warp_key] = (warped_data, feature_idx, top_unit)\n",
    "    return warped_data, feature_idx, top_unit\n",
    "\n",
    "def plot_warped_trials(warped_data, instance_idx, feature_idx, top_unit, top_unit_id,\n",
    "                      highlight_trials=None, max_individual_trials=10,\n",
    "                      smooth_window=None, show_event_regions=True):\n",
    "    \"\"\"\n",
    "    Plot trial-warped feature and unit activations on canonical timeline\n",
    "    \n",
    "    Parameters:\n",
    "    - warped_data: output from warp_trials_to_canonical_timeline\n",
    "    - instance_idx: SAE instance index being analyzed\n",
    "    - feature_idx: feature index being plotted\n",
    "    - top_unit: top unit index \n",
    "    - top_unit_id: location of top unit (PMd or M1)\n",
    "    - highlight_trials: list of trial indices to highlight, or None for random selection\n",
    "    - max_individual_trials: maximum number of individual trials to show\n",
    "    - smooth_window: optional smoothing window size in bins\n",
    "    - show_event_regions: whether to show colored regions for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    if warped_data is None:\n",
    "        print(\"No warped data available\")\n",
    "        return\n",
    "    \n",
    "    feature_acts = warped_data['feature_activations']\n",
    "    unit_acts = warped_data['unit_activations']\n",
    "    trial_info = warped_data['trial_info']\n",
    "    time_axis = warped_data['canonical_time_axis']\n",
    "    canonical_events = warped_data['canonical_events']\n",
    "    \n",
    "    print(f\"Plotting {len(feature_acts)} warped trials\")\n",
    "    \n",
    "    # Apply smoothing if requested\n",
    "    if smooth_window is not None and smooth_window > 1:\n",
    "        feature_acts = uniform_filter1d(feature_acts, size=smooth_window, axis=1)\n",
    "        unit_acts = uniform_filter1d(unit_acts, size=smooth_window, axis=1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    feature_mean = np.mean(feature_acts, axis=0)\n",
    "    feature_sem = np.std(feature_acts, axis=0) / np.sqrt(len(feature_acts))\n",
    "    \n",
    "    unit_mean = np.mean(unit_acts, axis=0)\n",
    "    unit_sem = np.std(unit_acts, axis=0) / np.sqrt(len(unit_acts))\n",
    "    \n",
    "    # Create single plot with two y-axes\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add background regions for different epochs if requested\n",
    "    if show_event_regions:\n",
    "        event_times = list(canonical_events.values())\n",
    "        event_names = list(canonical_events.keys())\n",
    "        colors = ['rgba(255,200,200,0.2)', 'rgba(200,255,200,0.2)', 'rgba(200,200,255,0.2)', \n",
    "                 'rgba(255,255,200,0.2)', 'rgba(255,200,255,0.2)']\n",
    "        \n",
    "        for i in range(len(event_times)-1):\n",
    "            fig.add_vrect(x0=event_times[i], x1=event_times[i+1], fillcolor=colors[i % len(colors)], layer=\"below\", line_width=0)\n",
    "    \n",
    "    # Plot feature activations (primary y-axis)\n",
    "    # SEM band\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=feature_mean + feature_sem, mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip', name='upper_bound'))\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=feature_mean - feature_sem, mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(0,100,80,0.3)', name='Feature ±SEM', showlegend=True))\n",
    "    \n",
    "    # Mean feature line\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=feature_mean, mode='lines', line=dict(color='darkgreen', width=4), name=f'Mean Feature {feature_idx}', yaxis='y'))\n",
    "    \n",
    "    # Individual feature trials\n",
    "    if highlight_trials is not None:\n",
    "        trial_indices = [idx for idx in highlight_trials if idx < len(feature_acts)][:max_individual_trials]\n",
    "        trial_label = \"Selected\"\n",
    "    else:\n",
    "        if len(feature_acts) <= max_individual_trials:\n",
    "            trial_indices = list(range(len(feature_acts)))\n",
    "        else:\n",
    "            trial_indices = np.random.choice(len(feature_acts), max_individual_trials, replace=False)\n",
    "        trial_label = \"Random\"\n",
    "    \n",
    "    for i, trial_idx in enumerate(trial_indices):\n",
    "        trial_info_str = \"\"\n",
    "        if 'maze_condition' in trial_info.columns:\n",
    "            maze_cond = trial_info.iloc[trial_idx]['maze_condition']\n",
    "            trial_info_str += f\"Maze: {maze_cond}\"\n",
    "        if 'hit_target_position' in trial_info.columns:\n",
    "            target_pos = trial_info.iloc[trial_idx]['hit_target_position']\n",
    "            trial_info_str += f\", Target: {target_pos}\"\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=time_axis, y=feature_acts[trial_idx], mode='lines', line=dict(color='rgba(0,150,100,0.5)', width=1), name=f'{trial_label} Feature Trials' if i == 0 else None, showlegend=i == 0, legendgroup='individual_feature_trials', hovertemplate=f'Trial {trial_idx}<br>{trial_info_str}<br>Time: %{{x}}<br>Feature: %{{y}}<extra></extra>', yaxis='y'))\n",
    "    \n",
    "    # Plot unit z-scores (secondary y-axis)\n",
    "    # SEM band for units\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=unit_mean + unit_sem, mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip', yaxis='y2'))\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=unit_mean - unit_sem, mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(0,80,150,0.3)', name='Unit Z-score ±SEM', showlegend=True, yaxis='y2'))\n",
    "    \n",
    "    # Mean unit line\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=unit_mean, mode='lines', line=dict(color='darkblue', width=4), name=f'Mean Unit {top_unit} Z-score', yaxis='y2'))\n",
    "    \n",
    "    # Individual unit trials\n",
    "    for i, trial_idx in enumerate(trial_indices):\n",
    "        trial_info_str = \"\"\n",
    "        if 'maze_condition' in trial_info.columns:\n",
    "            maze_cond = trial_info.iloc[trial_idx]['maze_condition']\n",
    "            trial_info_str += f\"Maze: {maze_cond}\"\n",
    "        if 'hit_target_position' in trial_info.columns:\n",
    "            target_pos = trial_info.iloc[trial_idx]['hit_target_position']\n",
    "            trial_info_str += f\", Target: {target_pos}\"\n",
    "            \n",
    "        fig.add_trace(go.Scatter(x=time_axis, y=unit_acts[trial_idx], mode='lines', line=dict(color='rgba(100,100,255,0.5)', width=1), name=f'{trial_label} Unit Trials' if i == 0 else None, showlegend=i == 0, legendgroup='individual_unit_trials', hovertemplate=f'Trial {trial_idx}<br>{trial_info_str}<br>Time: %{{x}}<br>Unit Z-score: %{{y}}<extra></extra>', yaxis='y2'))\n",
    "    \n",
    "    # Add event lines\n",
    "    for event_name, event_time in canonical_events.items():\n",
    "        fig.add_vline(x=event_time, line_dash=\"dash\", line_color=\"red\", line_width=2, annotation_text=event_name.replace('_', ' ').title(), annotation_position=\"top\")\n",
    "    \n",
    "    # Update layout with dual y-axes\n",
    "    fig.update_layout(\n",
    "        title=f\"Instance {instance_idx} Feature {feature_idx} & Top Unit {top_unit} ({top_unit_id})\",\n",
    "        xaxis_title=\"Canonical Time (s)\",\n",
    "        yaxis=dict(title=\"Feature Activation\", side=\"left\", color=\"darkgreen\"),\n",
    "        yaxis2=dict(title=\"Unit Z-score\", side=\"right\", overlaying=\"y\", color=\"darkblue\"),\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        legend=dict(x=0.02, y=0.98, bgcolor=\"rgba(255,255,255,0.8)\")\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def explore_trial_conditions(combined_trials_df):\n",
    "    \"\"\"Explore available maze conditions and target positions for filtering\"\"\"\n",
    "    print(\"\\nAvailable Trial Conditions\")\n",
    "    \n",
    "    print(\"\\nMaze Conditions:\")\n",
    "    maze_counts = combined_trials_df['maze_condition'].value_counts().sort_index()\n",
    "    for condition, count in maze_counts.items():\n",
    "        print(f\"  {condition}: {count} trials\")\n",
    "    \n",
    "    print(\"\\nHit Target Positions:\")\n",
    "    target_counts = combined_trials_df['hit_target_position'].value_counts()\n",
    "    for position, count in target_counts.items():\n",
    "        print(f\"  {position}: {count} trials\")\n",
    "    \n",
    "    print(f\"\\nTotal trials: {len(combined_trials_df)}\")\n",
    "    \n",
    "    return maze_counts, target_counts\n",
    "\n",
    "# Example usage\n",
    "print(\"Trial Warping Analysis\")\n",
    "# First, explore what conditions are available\n",
    "print(\"Exploring available trial conditions...\")\n",
    "maze_counts, target_counts = explore_trial_conditions(combined_trials_df)\n",
    "\n",
    "# Parameters to customize\n",
    "instance_to_analyze = 0\n",
    "feature_to_analyze = 183\n",
    "# FILTERING OPTIONS - Set these to filter trials\n",
    "maze_conditions_to_include = None\n",
    "target_positions_to_include = None\n",
    "# OTHER PARAMETERS\n",
    "num_example_trials = 8\n",
    "smooth_data = 3\n",
    "show_epochs = True\n",
    "\n",
    "print(f\"\\nRunning analysis with filters:\")\n",
    "print(f\"  Maze conditions: {maze_conditions_to_include}\")\n",
    "print(f\"  Target positions: {target_positions_to_include}\")\n",
    "\n",
    "# Run warping analysis (now including spk_z_scores_df parameter)\n",
    "warped_data, feature_idx, top_unit = warp_trials_to_canonical_timeline(\n",
    "    combined_trials_df, acts_df, spk_z_scores_df, metadata_binned_subset,\n",
    "    instance_idx=instance_to_analyze, \n",
    "    feature_idx=feature_to_analyze,\n",
    "    maze_conditions=maze_conditions_to_include,\n",
    "    hit_target_positions=target_positions_to_include\n",
    ")\n",
    "top_unit_id = session.units.id[top_unit]\n",
    "if isinstance(top_unit_id, (bytes, bytearray)):\n",
    "    top_unit_id = top_unit_id.decode(\"utf-8\")\n",
    "if \"group_1\" in top_unit_id:\n",
    "    top_unit_id = \"PMd\"\n",
    "elif \"group_2\" in top_unit_id:\n",
    "    top_unit_id = \"M1\"\n",
    "\n",
    "if warped_data is not None:\n",
    "    print(f\"\\nResults\")\n",
    "    print(f\"Analyzed Feature: {feature_idx}\")\n",
    "    print(f\"Top Co-active Unit: {top_unit} ({top_unit_id})\")\n",
    "    \n",
    "    # Create the warped trial plot\n",
    "    plot_warped_trials(\n",
    "        warped_data, instance_to_analyze, feature_idx, top_unit, top_unit_id,\n",
    "        highlight_trials=None,\n",
    "        max_individual_trials=num_example_trials,\n",
    "        smooth_window=smooth_data,\n",
    "        show_event_regions=show_epochs\n",
    "    )\n",
    "else:\n",
    "    print(\"Failed to warp trials - check your parameters and data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive UI\"\"\"\n",
    "\n",
    "# Helper to map a base variable and its type to the metadata_binned_subset column name\n",
    "def _bvar_name(var_name, var_type):\n",
    "    return f\"{var_name}_binned\" if var_type == 'continuous' else var_name\n",
    "\n",
    "# Mode selector: preset vs manual\n",
    "mode_radio = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Preset (from results table)', 'preset'),\n",
    "        ('Manual selection',       'manual')\n",
    "    ],\n",
    "    value='preset',\n",
    "    description=''\n",
    ")\n",
    "\n",
    "# Build the preset dropdown with full metrics\n",
    "preset_entries = []\n",
    "for _, r in results.iterrows():\n",
    "    bvar = _bvar_name(r.variable, r.variable_type)\n",
    "    if bvar not in metadata_binned_subset.columns:\n",
    "        continue\n",
    "    label = (\n",
    "        f\"Inst:{int(r.instance_idx)} | \"\n",
    "        f\"Feat:{int(r.feature_idx)} | \"\n",
    "        f\"Var:{bvar} | \"\n",
    "        f\"Val:{r['value']} | \"\n",
    "        f\"FracDuring:{r.activation_frac_during:.3f} | \"\n",
    "        f\"FracOutside:{r.activation_frac_outside:.3f} | \"\n",
    "        f\"ActRatio:{r.activation_ratio:.3f} | \"\n",
    "        f\"RateProp:{r.rate_proportion:.3f}\"\n",
    "    )\n",
    "    preset_entries.append((label, (int(r.instance_idx), int(r.feature_idx), bvar)))\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=preset_entries,\n",
    "    description='Select Result:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "preset_box = widgets.VBox([preset_dropdown])\n",
    "\n",
    "# Manual instance & feature selection\n",
    "instance_dropdown = widgets.Dropdown(\n",
    "    options=sorted(acts_df['instance_idx'].unique()),\n",
    "    description='Instance:'\n",
    ")\n",
    "\n",
    "feature_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Feature:'\n",
    ")\n",
    "\n",
    "def _on_instance_change(change):\n",
    "    inst = change['new']\n",
    "    feats = sorted(\n",
    "        acts_df.loc[acts_df['instance_idx'] == inst, 'feature_idx'].unique()\n",
    "    )\n",
    "    feature_dropdown.options = feats\n",
    "\n",
    "instance_dropdown.observe(_on_instance_change, names='value')\n",
    "_on_instance_change({'new': instance_dropdown.value})\n",
    "\n",
    "manual_box = widgets.VBox([instance_dropdown, feature_dropdown])\n",
    "manual_box.layout.display = 'none'\n",
    "\n",
    "# Maze condition and target position filters\n",
    "maze_options = sorted(combined_trials_df['maze_condition'].dropna().unique())\n",
    "maze_dropdown = widgets.Dropdown(\n",
    "    options=[None] + maze_options,\n",
    "    description='Maze cond:'\n",
    ")\n",
    "\n",
    "target_positions = combined_trials_df['hit_target_position'].dropna().unique()\n",
    "target_strs = [str(pos) for pos in target_positions]\n",
    "target_dropdown = widgets.Dropdown(\n",
    "    options=[None] + target_strs,\n",
    "    description='Target pos:'\n",
    ")\n",
    "\n",
    "# Toggle preset vs manual\n",
    "def _on_mode_change(change):\n",
    "    if change['new'] == 'preset':\n",
    "        preset_box.layout.display = ''\n",
    "        manual_box.layout.display = 'none'\n",
    "    else:\n",
    "        preset_box.layout.display = 'none'\n",
    "        manual_box.layout.display = ''\n",
    "\n",
    "mode_radio.observe(_on_mode_change, names='value')\n",
    "_on_mode_change({'new': mode_radio.value})\n",
    "\n",
    "# Generate button and output area\n",
    "generate_btn = widgets.Button(description='Generate Plot', button_style='info')\n",
    "out = widgets.Output()\n",
    "\n",
    "def _on_generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        if mode_radio.value == 'preset':\n",
    "            inst, feat, _ = preset_dropdown.value\n",
    "        else:\n",
    "            inst = instance_dropdown.value\n",
    "            feat = feature_dropdown.value\n",
    "\n",
    "        maze = [maze_dropdown.value] if maze_dropdown.value is not None else None\n",
    "        tgt = target_dropdown.value\n",
    "        hit_positions = [eval(tgt)] if tgt is not None else None\n",
    "\n",
    "        warped_data, used_feat, top_unit = warp_trials_to_canonical_timeline(\n",
    "            combined_trials_df,\n",
    "            acts_df,\n",
    "            spk_z_scores_df,\n",
    "            metadata_binned_subset,\n",
    "            instance_idx=inst,\n",
    "            feature_idx=feat,\n",
    "            maze_conditions=maze,\n",
    "            hit_target_positions=hit_positions\n",
    "        )\n",
    "        top_unit_id = session.units.id[top_unit]\n",
    "        if isinstance(top_unit_id, (bytes, bytearray)):\n",
    "            top_unit_id = top_unit_id.decode(\"utf-8\")\n",
    "        if \"group_1\" in top_unit_id:\n",
    "            top_unit_id = \"PMd\"\n",
    "        elif \"group_2\" in top_unit_id:\n",
    "            top_unit_id = \"M1\"\n",
    "\n",
    "        if warped_data is not None:\n",
    "            plot_warped_trials(warped_data, inst, used_feat, top_unit, top_unit_id)\n",
    "        else:\n",
    "            print(\"No data to display. Check your selections.\")\n",
    "\n",
    "generate_btn.on_click(_on_generate)\n",
    "\n",
    "# Assemble and display the UI\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Warp Trials Visualization</h2>\"),\n",
    "    mode_radio,\n",
    "    preset_box,\n",
    "    manual_box,\n",
    "    maze_dropdown,\n",
    "    target_dropdown,\n",
    "    generate_btn,\n",
    "    out\n",
    "])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualisation functions for all feature associations\"\"\"\n",
    "\n",
    "def plot_feature_tuning(\n",
    "    acts_df: pd.DataFrame,\n",
    "    spk_z_scores_df: pd.DataFrame,\n",
    "    metadata_binned: pd.DataFrame,\n",
    "    variable: str,\n",
    "    instance_idx: int,\n",
    "    feature_idx: int\n",
    "):\n",
    "    \"\"\"Visualizes SAE feature tuning to metadata variables.\"\"\"\n",
    "    # Get feature activations\n",
    "    feature_acts = acts_df[(acts_df['instance_idx'] == instance_idx) & (acts_df['feature_idx'] == feature_idx)]\n",
    "    \n",
    "    # Find top/bottom co-active units\n",
    "    if len(feature_acts) > 0:\n",
    "        feature_active_indices = feature_acts['example_idx'].values\n",
    "        neuron_mean_zscores = spk_z_scores_df.iloc[feature_active_indices].mean(axis=0)\n",
    "        top_unit = neuron_mean_zscores.idxmax()\n",
    "        bottom_unit = neuron_mean_zscores.idxmin()\n",
    "        top_zscore = neuron_mean_zscores[top_unit]\n",
    "        bottom_zscore = neuron_mean_zscores[bottom_unit]\n",
    "        print(f\"Top co-active unit: {top_unit} (z-score: {top_zscore:.3f})\")\n",
    "        print(f\"Bottom co-active unit: {bottom_unit} (z-score: {bottom_zscore:.3f})\")\n",
    "    else:\n",
    "        print(\"No feature activations found for this instance/feature.\")\n",
    "        return\n",
    "    \n",
    "    # Create complete dataset with zeros for inactive features\n",
    "    all_examples = pd.DataFrame({'example_idx': range(len(metadata_binned))})\n",
    "    all_examples = all_examples.merge(feature_acts[['example_idx', 'activation_value']], on='example_idx', how='left').fillna(0)\n",
    "    \n",
    "    if all_examples.empty:\n",
    "        print(f\"⚠️ No examples found for Instance {instance_idx}, Feature {feature_idx}. Cannot generate plot.\")\n",
    "        return\n",
    "    \n",
    "    # Get metadata and z-scores for all examples\n",
    "    metadata_slice = metadata_binned[[variable]].iloc[all_examples['example_idx']]\n",
    "    top_unit_slice = spk_z_scores_df[[top_unit]].iloc[all_examples['example_idx']]\n",
    "    bottom_unit_slice = spk_z_scores_df[[bottom_unit]].iloc[all_examples['example_idx']]\n",
    "    \n",
    "    # Create plotting dataframe\n",
    "    data_df = metadata_slice.reset_index(drop=True)\n",
    "    data_df['activation_value'] = all_examples['activation_value'].reset_index(drop=True)\n",
    "    data_df['top_unit_zscore'] = top_unit_slice[top_unit].reset_index(drop=True)\n",
    "    data_df['bottom_unit_zscore'] = bottom_unit_slice[bottom_unit].reset_index(drop=True)\n",
    "    data_df = data_df.dropna(subset=[variable])\n",
    "    \n",
    "    if data_df.empty:\n",
    "        print(f\"⚠️ No matching metadata found for feature bins. Cannot generate plot.\")\n",
    "        return\n",
    "    \n",
    "    # Check if data is interval type\n",
    "    try:\n",
    "        is_interval_data = pd.api.types.is_interval_dtype(data_df[variable].cat.categories)\n",
    "    except AttributeError:\n",
    "        is_interval_data = False\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    stats_df = data_df.groupby(variable).agg({\n",
    "        'activation_value': ['mean', 'sem'],\n",
    "        'top_unit_zscore': ['mean', 'sem'],\n",
    "        'bottom_unit_zscore': ['mean', 'sem']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    stats_df.columns = [variable, 'feature_mean', 'feature_sem', 'top_unit_mean', 'top_unit_sem', 'bottom_unit_mean', 'bottom_unit_sem']\n",
    "    \n",
    "    # Calculate rate proportions\n",
    "    if not feature_acts.empty:\n",
    "        condition_masks = {condition: metadata_binned[variable] == condition for condition in stats_df[variable]}\n",
    "        active_example_set = set(feature_acts['example_idx'])\n",
    "        \n",
    "        rate_props = []\n",
    "        for _, row in stats_df.iterrows():\n",
    "            condition = row[variable]\n",
    "            condition_mask = condition_masks[condition]\n",
    "            \n",
    "            condition_example_idxs = np.where(condition_mask)[0]\n",
    "            condition_activations = len(active_example_set.intersection(condition_example_idxs))\n",
    "            activation_frac_during = condition_activations / len(condition_example_idxs) if len(condition_example_idxs) > 0 else 0\n",
    "            \n",
    "            non_condition_example_idxs = np.where(~condition_mask)[0]\n",
    "            non_condition_activations = len(active_example_set.intersection(non_condition_example_idxs))\n",
    "            activation_frac_outside = non_condition_activations / len(non_condition_example_idxs) if len(non_condition_example_idxs) > 0 else 0\n",
    "            \n",
    "            rate_proportion = activation_frac_during / (activation_frac_during + activation_frac_outside) if (activation_frac_during + activation_frac_outside) > 0 else 0\n",
    "            rate_props.append(rate_proportion)\n",
    "        \n",
    "        stats_df['rate_proportion'] = rate_props\n",
    "    else:\n",
    "        stats_df['rate_proportion'] = 0\n",
    "    \n",
    "    # Calculate z-score stats for bar plot\n",
    "    if len(feature_acts) > 0:\n",
    "        zscore_stats = spk_z_scores_df.iloc[feature_active_indices].agg(['mean', 'sem']).T\n",
    "        zscore_stats.columns = ['mean_zscore', 'sem_zscore']\n",
    "        zscore_stats = zscore_stats.reset_index()\n",
    "        zscore_stats.columns = ['neuron', 'mean_zscore', 'sem_zscore']\n",
    "    else:\n",
    "        zscore_stats = pd.DataFrame({'neuron': spk_z_scores_df.columns, 'mean_zscore': 0, 'sem_zscore': 0})\n",
    "    \n",
    "    # Create plots based on variable type\n",
    "    if 'angle' in variable:  # Polar plot\n",
    "        stats_df['theta'] = stats_df[variable].apply(lambda x: x.mid if isinstance(x, pd.Interval) else x)\n",
    "        stats_df = stats_df.sort_values('theta')\n",
    "        plot_df = pd.concat([stats_df, stats_df.head(1)], ignore_index=True)\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=4,\n",
    "            specs=[[{\"type\": \"polar\"}, {\"type\": \"polar\"}, {\"type\": \"polar\"}, {\"type\": \"polar\"}],\n",
    "                   [{\"type\": \"xy\", \"colspan\": 4}, None, None, None]],\n",
    "            horizontal_spacing=0.1,\n",
    "            vertical_spacing=0.15,\n",
    "            subplot_titles=[\"Feature Activation\", \"Top Unit Z-score\", \"Bottom Unit Z-score\", \"Rate Proportion\", \"Mean Z-scores when Feature Active\"]\n",
    "        )\n",
    "        \n",
    "        # Feature activation polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['feature_mean'] + plot_df['feature_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['feature_mean'] - plot_df['feature_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), fill='tonext', fillcolor='rgba(220,20,60,0.2)', name='Feature ±SEM'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['feature_mean'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='crimson', width=3), name='Feature Activation'), row=1, col=1)\n",
    "        \n",
    "        # Top unit polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['top_unit_mean'] + plot_df['top_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), showlegend=False), row=1, col=2)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['top_unit_mean'] - plot_df['top_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), fill='tonext', fillcolor='rgba(0,0,139,0.2)', name='Top Unit ±SEM'), row=1, col=2)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['top_unit_mean'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='darkblue', width=3), name='Top Unit Z-score'), row=1, col=2)\n",
    "        \n",
    "        # Bottom unit polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['bottom_unit_mean'] + plot_df['bottom_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), showlegend=False), row=1, col=3)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['bottom_unit_mean'] - plot_df['bottom_unit_sem'], theta=plot_df['theta'], mode='lines', line=dict(width=0), fill='tonext', fillcolor='rgba(255,165,0,0.2)', name='Bottom Unit ±SEM'), row=1, col=3)\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['bottom_unit_mean'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='orange', width=3), name='Bottom Unit Z-score'), row=1, col=3)\n",
    "        \n",
    "        # Rate proportion polar plot\n",
    "        fig.add_trace(go.Scatterpolar(r=plot_df['rate_proportion'], theta=plot_df['theta'], mode='lines+markers', line=dict(color='green', width=3), name='Rate Proportion'), row=1, col=4)\n",
    "        \n",
    "        # Z-score bar plot\n",
    "        fig.add_trace(go.Bar(x=zscore_stats['neuron'], y=zscore_stats['mean_zscore'], error_y=dict(type='data', array=zscore_stats['sem_zscore']), marker_color='purple', marker_line_width=0, opacity=0.7, name='Mean Z-score'), row=2, col=1)\n",
    "        \n",
    "        # Create tick labels\n",
    "        if 'movement_angle' not in variable:\n",
    "            tick_labels = [f\"{int(round(theta))}°\" for theta in stats_df['theta']]\n",
    "        else:\n",
    "            tick_labels = [f\"{int(interval.mid)}°\" if hasattr(interval, 'mid') else str(interval) for interval in stats_df[variable]]    \n",
    "        \n",
    "        fig.update_layout(title=f\"Instance {instance_idx} Feature {feature_idx} & Top Unit {top_unit} & Bottom Unit {bottom_unit}\", showlegend=True, height=800, width=1600, margin=dict(t=80, b=60, l=50, r=50))\n",
    "        \n",
    "        # Update polar plots\n",
    "        rotation = 195 if 'movement_angle' in variable else 0\n",
    "        tickvals = stats_df['theta'].tolist() if 'movement_angle' in variable else [(angle % 360) for angle in stats_df['theta'].tolist()]\n",
    "        for i in range(1, 5):\n",
    "            polar_key = f'polar{i if i > 1 else \"\"}'\n",
    "            fig.update_layout(**{polar_key: dict(angularaxis=dict(direction=\"counterclockwise\", rotation=rotation, tickvals=tickvals, ticktext=tick_labels, tickfont=dict(size=10)), radialaxis=dict(range=[0, None]))})\n",
    "\n",
    "        fig.update_xaxes(title_text=\"Neuron\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Mean Z-score\", row=2, col=1)\n",
    "            \n",
    "    else:  # Linear plot\n",
    "        fig = make_subplots(rows=2, cols=4, specs=[[{}, {}, {}, {}], [{\"colspan\": 4}, None, None, None]], subplot_titles=[\"Feature Activation\", \"Top Unit Z-score\", \"Bottom Unit Z-score\", \"Rate Proportion\", \"Mean Z-scores when Feature Active\"], horizontal_spacing=0.1, vertical_spacing=0.3)\n",
    "        \n",
    "        # Setup x-axis labels\n",
    "        if is_interval_data:\n",
    "            x_axis_labels = stats_df[variable].apply(lambda x: str(x))\n",
    "            stats_df = stats_df.sort_values(by=variable)\n",
    "        else:\n",
    "            x_axis_labels = stats_df[variable].astype(str)\n",
    "        \n",
    "        # Plot based on data type\n",
    "        if is_interval_data:\n",
    "            # Line plots for continuous data\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['feature_mean'] + stats_df['feature_sem'], mode='lines', line_color='rgba(0,0,0,0)', showlegend=False), row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['feature_mean'] - stats_df['feature_sem'], mode='lines', line_color='rgba(0,0,0,0)', fill='tonexty', fillcolor='rgba(220,20,60,0.2)', name='Feature ±SEM'), row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['feature_mean'], mode='lines+markers', line_color='crimson', name='Feature Activation'), row=1, col=1)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['top_unit_mean'] + stats_df['top_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', showlegend=False), row=1, col=2)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['top_unit_mean'] - stats_df['top_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', fill='tonexty', fillcolor='rgba(0,0,139,0.2)', name='Top Unit ±SEM'), row=1, col=2)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['top_unit_mean'], mode='lines+markers', line_color='darkblue', name='Top Unit Z-score'), row=1, col=2)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['bottom_unit_mean'] + stats_df['bottom_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', showlegend=False), row=1, col=3)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['bottom_unit_mean'] - stats_df['bottom_unit_sem'], mode='lines', line_color='rgba(0,0,0,0)', fill='tonexty', fillcolor='rgba(255,165,0,0.2)', name='Bottom Unit ±SEM'), row=1, col=3)\n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['bottom_unit_mean'], mode='lines+markers', line_color='orange', name='Bottom Unit Z-score'), row=1, col=3)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(x=x_axis_labels, y=stats_df['rate_proportion'], mode='lines+markers', line=dict(color='green', width=3), name='Rate Proportion'), row=1, col=4)\n",
    "        else:\n",
    "            # Bar plots for categorical data\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['feature_mean'], error_y=dict(type='data', array=stats_df['feature_sem']), marker_color='crimson', marker_line_width=0, opacity=0.7, name='Feature Activation'), row=1, col=1)\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['top_unit_mean'], error_y=dict(type='data', array=stats_df['top_unit_sem']), marker_color='darkblue', marker_line_width=0, opacity=0.7, name='Top Unit Z-score'), row=1, col=2)\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['bottom_unit_mean'], error_y=dict(type='data', array=stats_df['bottom_unit_sem']), marker_color='orange', marker_line_width=0, opacity=0.7, name='Bottom Unit Z-score'), row=1, col=3)\n",
    "            fig.add_trace(go.Bar(x=x_axis_labels, y=stats_df['rate_proportion'], marker_color='green', marker_line_width=0, opacity=0.7, name='Rate Proportion'), row=1, col=4)\n",
    "        \n",
    "        # Z-score bar plot\n",
    "        fig.add_trace(go.Bar(x=zscore_stats['neuron'], y=zscore_stats['mean_zscore'], error_y=dict(type='data', array=zscore_stats['sem_zscore']), marker_color='purple', marker_line_width=0, opacity=0.7, name='Mean Z-score'), row=2, col=1)\n",
    "        \n",
    "        fig.update_layout(title=f\"Instance {instance_idx} Feature {feature_idx} & Top Unit {top_unit} & Bottom Unit {bottom_unit}\", height=800, width=1600, showlegend=True, margin=dict(t=80, b=60, l=50, r=50))\n",
    "        \n",
    "        # Update axes\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=1)\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=2)\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=3)\n",
    "        fig.update_xaxes(title_text=variable, tickangle=45, row=1, col=4)\n",
    "        fig.update_xaxes(title_text=\"Neuron\", row=2, col=1)\n",
    "        \n",
    "        fig.update_yaxes(title_text=\"Feature Activation\", color=\"crimson\", range=[0, None], row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Top Unit Z-score\", color=\"darkblue\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Bottom Unit Z-score\", color=\"orange\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"Rate Proportion\", range=[0, None], color=\"green\", row=1, col=4)\n",
    "        fig.update_yaxes(title_text=\"Mean Z-score\", row=2, col=1)\n",
    "        \n",
    "        fig.update_yaxes(rangemode='tozero', row=1, col=1)\n",
    "        fig.update_yaxes(rangemode='tozero', row=1, col=4)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# # Find go_cue feature and plot\n",
    "# event_feature = results[results['value'] == 'go_cue'].sort_values('activation_ratio', ascending=False).iloc[0]\n",
    "# display(event_feature)\n",
    "# plot_feature_tuning(acts_df=acts_df, spk_z_scores_df=spk_z_scores_df, metadata_binned=metadata_binned_subset, variable='event', instance_idx=int(event_feature['instance_idx']), feature_idx=int(event_feature['feature_idx']))\n",
    "\n",
    "# # Find movement_angle feature and plot  \n",
    "# move_angle_feature = results[results['variable'] == 'movement_angle'].sort_values('activation_ratio', ascending=False).iloc[0]\n",
    "# display(move_angle_feature)\n",
    "# plot_feature_tuning(acts_df=acts_df, spk_z_scores_df=spk_z_scores_df, metadata_binned=metadata_binned, variable='movement_angle_binned', instance_idx=int(move_angle_feature['instance_idx']), feature_idx=int(move_angle_feature['feature_idx']))\n",
    "\n",
    "# # Find vel_magnitude feature and plot\n",
    "# velocity_feature = results[results['variable'] == 'vel_magnitude'].sort_values('activation_ratio', ascending=False).iloc[0]\n",
    "# display(velocity_feature)\n",
    "# plot_feature_tuning(acts_df=acts_df, spk_z_scores_df=spk_z_scores_df, metadata_binned=metadata_binned_subset, variable='vel_magnitude_binned', instance_idx=int(velocity_feature['instance_idx']), feature_idx=int(velocity_feature['feature_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive UI\"\"\"\n",
    "\n",
    "# Helper to map a base variable and its type to the metadata_binned_subset column name\n",
    "def _bvar_name(var_name, var_type):\n",
    "    return f\"{var_name}_binned\" if var_type == 'continuous' else var_name\n",
    "\n",
    "# Selector for preset vs manual mode\n",
    "mode_radio = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Preset (from results table)', 'preset'),\n",
    "        ('Manual selection', 'manual')\n",
    "    ],\n",
    "    value='preset',\n",
    "    description=''\n",
    ")\n",
    "\n",
    "# Build the preset dropdown and store (instance, feature, variable) as the value\n",
    "preset_entries = []\n",
    "for _, r in results.iterrows():\n",
    "    bvar = _bvar_name(r.variable, r.variable_type)\n",
    "    if bvar not in metadata_binned_subset.columns:\n",
    "        continue  # Skip variables you haven’t binned\n",
    "    label = (\n",
    "        f\"Inst:{int(r.instance_idx)} | \"\n",
    "        f\"Feat:{int(r.feature_idx)} | \"\n",
    "        f\"Var:{bvar} | \"\n",
    "        f\"Val:{r['value']} | \"\n",
    "        f\"FracDuring:{r.activation_frac_during:.3f} | \"\n",
    "        f\"FracOutside:{r.activation_frac_outside:.3f} | \"\n",
    "        f\"ActRatio:{r.activation_ratio:.3f} | \"\n",
    "        f\"RateProp:{r.rate_proportion:.3f}\"\n",
    "    )\n",
    "    preset_entries.append((label, (int(r.instance_idx), int(r.feature_idx), bvar)))\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=preset_entries,\n",
    "    description='Select Result:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "preset_box = widgets.VBox([preset_dropdown])\n",
    "\n",
    "# ---- Manual selection (strict coupling) ----\n",
    "\n",
    "# Precompute bvar column on results for filtering; keep existing columns intact\n",
    "if 'bvar' not in results.columns:\n",
    "    results['bvar'] = results.apply(\n",
    "        lambda r: _bvar_name(r.variable, r.variable_type), axis=1\n",
    "    )\n",
    "\n",
    "instance_dropdown = widgets.Dropdown(\n",
    "    options=sorted(acts_df['instance_idx'].unique()),\n",
    "    description='Instance:'\n",
    ")\n",
    "\n",
    "# Start variable dropdown empty; we'll populate based on instance\n",
    "variable_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Variable:'\n",
    ")\n",
    "\n",
    "# Feature dropdown will depend on (instance, variable)\n",
    "feature_dropdown = widgets.Dropdown(\n",
    "    description='Feature:',\n",
    "    options=[]\n",
    ")\n",
    "\n",
    "def _refresh_variable_options(*_):\n",
    "    \"\"\"Restrict variables to those present for the selected instance and binned in metadata.\"\"\"\n",
    "    inst = instance_dropdown.value\n",
    "    if inst is None:\n",
    "        variable_dropdown.options = []\n",
    "        variable_dropdown.value = None\n",
    "        return\n",
    "\n",
    "    mask = (results['instance_idx'] == inst)\n",
    "    vars_for_inst = sorted(results.loc[mask, 'bvar'].unique())\n",
    "    vars_for_inst = [v for v in vars_for_inst if v in metadata_binned_subset.columns]\n",
    "\n",
    "    # Keep label==value pairing as before\n",
    "    manual_var_options = [(v, v) for v in vars_for_inst]\n",
    "    prev = variable_dropdown.value\n",
    "    variable_dropdown.options = manual_var_options\n",
    "    # Preserve selection if still valid; else pick first or None\n",
    "    valid_vals = [v for _, v in manual_var_options]\n",
    "    variable_dropdown.value = prev if prev in valid_vals else (valid_vals[0] if valid_vals else None)\n",
    "\n",
    "def _refresh_feature_options(*_):\n",
    "    \"\"\"Restrict features to those present for (instance, variable).\"\"\"\n",
    "    inst = instance_dropdown.value\n",
    "    sel_var = variable_dropdown.value\n",
    "    if (inst is None) or (sel_var is None):\n",
    "        feature_dropdown.options = []\n",
    "        feature_dropdown.value = None\n",
    "        feature_dropdown.disabled = True\n",
    "        return\n",
    "\n",
    "    mask = (results['instance_idx'] == inst) & (results['bvar'] == sel_var)\n",
    "    feats = sorted(results.loc[mask, 'feature_idx'].unique())\n",
    "\n",
    "    feature_dropdown.options = feats\n",
    "    feature_dropdown.value = (feats[0] if feats else None)\n",
    "    feature_dropdown.disabled = (len(feats) == 0)\n",
    "\n",
    "# Wire up dependencies: instance → variables, and (instance or variable) → features\n",
    "instance_dropdown.observe(_refresh_variable_options, names='value')\n",
    "instance_dropdown.observe(_refresh_feature_options, names='value')\n",
    "variable_dropdown.observe(_refresh_feature_options, names='value')\n",
    "\n",
    "# Prime once after widgets are created\n",
    "_refresh_variable_options()\n",
    "_refresh_feature_options()\n",
    "\n",
    "manual_box = widgets.VBox([\n",
    "    instance_dropdown,\n",
    "    variable_dropdown,\n",
    "    feature_dropdown\n",
    "])\n",
    "manual_box.layout.display = 'none'  # Start hidden\n",
    "\n",
    "# Buttons for generating or clearing the plot, and an output area\n",
    "generate_btn = widgets.Button(description='Generate Plot', button_style='info')\n",
    "clear_btn    = widgets.Button(description='Clear',         button_style='warning')\n",
    "button_box   = widgets.HBox([generate_btn, clear_btn])\n",
    "out = widgets.Output()\n",
    "\n",
    "# Toggle between preset and manual views\n",
    "def _on_mode_change(change):\n",
    "    if change['new'] == 'preset':\n",
    "        preset_box.layout.display = ''\n",
    "        manual_box.layout.display = 'none'\n",
    "    else:\n",
    "        preset_box.layout.display = 'none'\n",
    "        manual_box.layout.display = ''\n",
    "        # ensure dropdowns are in a valid state when switching\n",
    "        _refresh_variable_options()\n",
    "        _refresh_feature_options()\n",
    "\n",
    "mode_radio.observe(_on_mode_change, names='value')\n",
    "\n",
    "# Callback to generate the tuning plot\n",
    "def _on_generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        if mode_radio.value == 'preset':\n",
    "            inst, feat, var = preset_dropdown.value\n",
    "        else:\n",
    "            inst = instance_dropdown.value\n",
    "            var  = variable_dropdown.value\n",
    "            feat = feature_dropdown.value\n",
    "            if (inst is None) or (var is None) or (feat is None):\n",
    "                print(\"No matching (instance, variable, feature) for the current selection.\")\n",
    "                return\n",
    "\n",
    "        plot_feature_tuning(\n",
    "            acts_df=acts_df,\n",
    "            spk_z_scores_df=spk_z_scores_df,\n",
    "            metadata_binned=metadata_binned_subset,\n",
    "            variable=var,\n",
    "            instance_idx=inst,\n",
    "            feature_idx=feat\n",
    "        )\n",
    "\n",
    "# Callback to clear the output\n",
    "def _on_clear(_):\n",
    "    out.clear_output()\n",
    "\n",
    "generate_btn.on_click(_on_generate)\n",
    "clear_btn.on_click(_on_clear)\n",
    "\n",
    "# Assemble and display the UI\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>SAE Feature Visualization</h2>\"),\n",
    "    mode_radio,\n",
    "    preset_box,\n",
    "    manual_box,\n",
    "    button_box,\n",
    "    out\n",
    "])\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive UI\"\"\"\n",
    "\n",
    "# Colour-by selector\n",
    "colour_by = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\"Maze condition\", \"maze_condition\"),\n",
    "        (\"Hit position\", \"hit_position\"),\n",
    "        (\"Number of barriers\", \"barriers\"),\n",
    "        (\"Number of targets\", \"targets\")\n",
    "    ],\n",
    "    value=\"maze_condition\",\n",
    "    description=\"Colour by:\"\n",
    ")\n",
    "\n",
    "# Preset dropdown (same style as earlier UI)\n",
    "preset_entries = []\n",
    "for _, r in results.iterrows():\n",
    "    bvar = _bvar_name(r.variable, r.variable_type)\n",
    "    if bvar not in metadata_binned_subset.columns:\n",
    "        continue\n",
    "    label = (\n",
    "        f\"Inst:{int(r.instance_idx)} | \"\n",
    "        f\"Feat:{int(r.feature_idx)} | \"\n",
    "        f\"Var:{bvar} | \"\n",
    "        f\"Val:{r['value']} | \"\n",
    "        f\"FracDuring:{r.activation_frac_during:.3f} | \"\n",
    "        f\"FracOutside:{r.activation_frac_outside:.3f} | \"\n",
    "        f\"ActRatio:{r.activation_ratio:.3f} | \"\n",
    "        f\"RateProp:{r.rate_proportion:.3f}\"\n",
    "    )\n",
    "    preset_entries.append((label, (int(r.instance_idx), int(r.feature_idx), bvar)))\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=preset_entries,\n",
    "    description='Select Result:',\n",
    "    layout=widgets.Layout(width='90%')\n",
    ")\n",
    "preset_box = widgets.VBox([preset_dropdown])\n",
    "\n",
    "# Manual selection (instance → variable → feature dependent dropdowns)\n",
    "if 'bvar' not in results.columns:\n",
    "    results['bvar'] = results.apply(lambda r: _bvar_name(r.variable, r.variable_type), axis=1)\n",
    "\n",
    "instance_dropdown = widgets.Dropdown(\n",
    "    options=sorted(acts_df['instance_idx'].unique()),\n",
    "    description='Instance:'\n",
    ")\n",
    "variable_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Variable:'\n",
    ")\n",
    "feature_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Feature:'\n",
    ")\n",
    "\n",
    "def _refresh_variable_options(*_):\n",
    "    inst = instance_dropdown.value\n",
    "    if inst is None:\n",
    "        variable_dropdown.options = []\n",
    "        variable_dropdown.value = None\n",
    "        return\n",
    "    mask = (results['instance_idx'] == inst)\n",
    "    vars_for_inst = sorted(results.loc[mask, 'bvar'].unique())\n",
    "    vars_for_inst = [v for v in vars_for_inst if v in metadata_binned_subset.columns]\n",
    "    variable_dropdown.options = vars_for_inst\n",
    "    variable_dropdown.value = (vars_for_inst[0] if vars_for_inst else None)\n",
    "\n",
    "def _refresh_feature_options(*_):\n",
    "    inst = instance_dropdown.value\n",
    "    sel_var = variable_dropdown.value\n",
    "    if (inst is None) or (sel_var is None):\n",
    "        feature_dropdown.options = []\n",
    "        feature_dropdown.value = None\n",
    "        return\n",
    "    mask = (results['instance_idx'] == inst) & (results['bvar'] == sel_var)\n",
    "    feats = sorted(results.loc[mask, 'feature_idx'].unique())\n",
    "    feature_dropdown.options = feats\n",
    "    feature_dropdown.value = (feats[0] if feats else None)\n",
    "\n",
    "instance_dropdown.observe(_refresh_variable_options, names='value')\n",
    "instance_dropdown.observe(_refresh_feature_options, names='value')\n",
    "variable_dropdown.observe(_refresh_feature_options, names='value')\n",
    "\n",
    "_refresh_variable_options()\n",
    "_refresh_feature_options()\n",
    "\n",
    "manual_box = widgets.VBox([instance_dropdown, variable_dropdown, feature_dropdown])\n",
    "manual_box.layout.display = 'none'\n",
    "\n",
    "# Buttons and output\n",
    "generate_btn = widgets.Button(description='Generate Plot', button_style='info')\n",
    "clear_btn    = widgets.Button(description='Clear',         button_style='warning')\n",
    "button_box   = widgets.HBox([generate_btn, clear_btn])\n",
    "out = widgets.Output()\n",
    "\n",
    "# Mode toggle\n",
    "def _on_mode_change(change):\n",
    "    if change['new'] == 'preset':\n",
    "        preset_box.layout.display = ''\n",
    "        manual_box.layout.display = 'none'\n",
    "    else:\n",
    "        preset_box.layout.display = 'none'\n",
    "        manual_box.layout.display = ''\n",
    "        _refresh_variable_options()\n",
    "        _refresh_feature_options()\n",
    "mode_radio.observe(_on_mode_change, names='value')\n",
    "\n",
    "# --- Plot function ---\n",
    "def plot_trial_avg_scatter(instance_idx, feature_idx, variable, colour_var):\n",
    "    acts = acts_df[(acts_df.instance_idx==instance_idx) & (acts_df.feature_idx==feature_idx)]\n",
    "    if acts.empty:\n",
    "        print(\"No activations found\")\n",
    "        return\n",
    "\n",
    "    merged = acts.merge(metadata_binned_subset.reset_index(), left_on=\"example_idx\", right_index=True, how=\"left\")\n",
    "\n",
    "    trial_avg = merged.groupby(\"trial_idx\").agg(\n",
    "        avg_activation=(\"activation_value\", \"mean\"),\n",
    "        timestamp=(\"timestamp\", \"first\"),\n",
    "        maze_condition=(\"maze_condition\", \"first\"),\n",
    "        barriers=(\"barriers\", \"first\"),\n",
    "        targets=(\"targets\", \"first\"),\n",
    "        hit_position_x=(\"hit_position_x\", \"first\"),\n",
    "        hit_position_y=(\"hit_position_y\", \"first\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    if colour_var == \"hit_position\":\n",
    "        def pos_label(x, y):\n",
    "            if y > 0 and x < 0: return \"Top-Left\"\n",
    "            if y > 0 and x > 0: return \"Top-Right\"\n",
    "            if y < 0 and x < 0: return \"Bottom-Left\"\n",
    "            if y < 0 and x > 0: return \"Bottom-Right\"\n",
    "            return \"Other\"\n",
    "        trial_avg[\"hit_position\"] = trial_avg.apply(lambda r: pos_label(r.hit_position_x, r.hit_position_y), axis=1)\n",
    "\n",
    "    import plotly.express as px\n",
    "    fig = px.scatter(\n",
    "        trial_avg,\n",
    "        x=\"timestamp\",\n",
    "        y=\"avg_activation\",\n",
    "        color=colour_var,\n",
    "        title=f\"Inst {instance_idx}, Feat {feature_idx} | Avg activation per trial\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Alphabet * 10\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        legend=dict(orientation=\"v\", x=1.02, y=1),\n",
    "        margin=dict(r=200)\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Button callbacks\n",
    "def _on_generate(_):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        if mode_radio.value == 'preset':\n",
    "            inst, feat, var = preset_dropdown.value\n",
    "        else:\n",
    "            inst = instance_dropdown.value\n",
    "            var  = variable_dropdown.value\n",
    "            feat = feature_dropdown.value\n",
    "            if (inst is None) or (var is None) or (feat is None):\n",
    "                print(\"No matching (instance, variable, feature)\")\n",
    "                return\n",
    "        plot_trial_avg_scatter(inst, feat, var, colour_by.value)\n",
    "\n",
    "def _on_clear(_):\n",
    "    out.clear_output()\n",
    "\n",
    "generate_btn.on_click(_on_generate)\n",
    "clear_btn.on_click(_on_clear)\n",
    "\n",
    "# Assemble UI\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Trial-average Feature Activations</h2>\"),\n",
    "    mode_radio,\n",
    "    preset_box,\n",
    "    manual_box,\n",
    "    colour_by,\n",
    "    button_box,\n",
    "    out\n",
    "])\n",
    "display(ui)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
