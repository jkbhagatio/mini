\section{Methods}

\subsection{Model Architecture}

\subsubsection{Sparse Autoencoders (SAEs)}

We begin with the standard Sparse Autoencoder formulation. Given neural data $\mathbf{X} \in \mathbb{R}^{T \times N}$ where $T$ is the number of time points and $N$ is the number of neurons, an SAE learns an encoder function $f_{\text{enc}}: \mathbb{R}^N \rightarrow \mathbb{R}^D$ and decoder function $f_{\text{dec}}: \mathbb{R}^D \rightarrow \mathbb{R}^N$ where $D$ is the latent dimension.

The encoder projects input neural activity to a sparse latent representation:
\begin{equation}
\mathbf{z}_t = f_{\text{enc}}(\mathbf{x}_t) = \text{ReLU}(\mathbf{W}_{\text{enc}} \mathbf{x}_t + \mathbf{b}_{\text{enc}})
\end{equation}

The decoder reconstructs the original activity:
\begin{equation}
\hat{\mathbf{x}}_t = f_{\text{dec}}(\mathbf{z}_t) = \mathbf{W}_{\text{dec}} \mathbf{z}_t + \mathbf{b}_{\text{dec}}
\end{equation}

The total loss function combines reconstruction error with sparsity regularization:
\begin{equation}
\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \|\mathbf{x}_t - \hat{\mathbf{x}}_t\|_2^2 + \lambda \sum_{t=1}^{T} \|\mathbf{z}_t\|_1
\end{equation}

where $\lambda$ controls the sparsity-reconstruction trade-off.

\subsubsection{Multi-Scale Sparse Autoencoders (MSAEs)}

To capture features across multiple temporal scales, we extend the SAE architecture to process neural data at different temporal resolutions simultaneously. Our MSAE consists of multiple parallel encoding pathways, each operating at a different temporal scale.

For temporal scales $s \in \{s_1, s_2, \ldots, s_K\}$, we create downsampled versions of the input data:
\begin{equation}
\mathbf{X}^{(s)} = \text{Downsample}(\mathbf{X}, s)
\end{equation}

Each scale has its own encoder-decoder pair:
\begin{align}
\mathbf{z}_t^{(s)} &= f_{\text{enc}}^{(s)}(\mathbf{x}_t^{(s)}) \\
\hat{\mathbf{x}}_t^{(s)} &= f_{\text{dec}}^{(s)}(\mathbf{z}_t^{(s)})
\end{align}

The multi-scale loss combines reconstruction errors across all scales:
\begin{equation}
\mathcal{L}_{\text{MSAE}} = \sum_{s} \alpha_s \left[ \frac{1}{T_s} \sum_{t=1}^{T_s} \|\mathbf{x}_t^{(s)} - \hat{\mathbf{x}}_t^{(s)}\|_2^2 + \lambda_s \sum_{t=1}^{T_s} \|\mathbf{z}_t^{(s)}\|_1 \right]
\end{equation}

where $\alpha_s$ weights the contribution of each scale and $T_s$ is the number of time points at scale $s$.

\subsection{Neural Data Pipeline}

\subsubsection{Data Preprocessing}

Our preprocessing pipeline standardizes neural data across different recording modalities and experimental paradigms:

\begin{enumerate}
\item \textbf{Spike detection and sorting}: For raw extracellular recordings, we apply spike detection and sorting algorithms to extract single-unit activity.

\item \textbf{Quality control}: We filter units based on signal-to-noise ratio, isolation quality, and firing rate criteria.

\item \textbf{Binning}: Spike trains are binned at multiple temporal resolutions (e.g., 1ms, 10ms, 100ms) to capture different aspects of neural dynamics.

\item \textbf{Normalization}: Neural activity is z-scored across time to account for differences in baseline firing rates across neurons.

\item \textbf{Artifact removal}: We identify and remove periods with electrical artifacts or recording instabilities.
\end{enumerate}

\subsubsection{Training Procedure}

Model training follows a multi-stage approach:

\begin{enumerate}
\item \textbf{Hyperparameter optimization}: We perform systematic sweeps over key hyperparameters including sparsity weights ($\lambda$), latent dimensions ($d_{\text{sae}}$), and top-k sparsity constraints.

\item \textbf{Multi-scale training}: Models are trained jointly across all temporal scales with adaptive learning rates for each scale.

\item \textbf{Regularization}: We apply dropout, weight decay, and early stopping to prevent overfitting.

\item \textbf{Validation}: Training is monitored using held-out validation data to ensure generalization.
\end{enumerate}

\subsubsection{Hyperparameter Selection}

The choice of hyperparameters significantly affects the interpretability-reconstruction trade-off:

\begin{itemize}
\item \textbf{Latent dimension ($d_{\text{sae}}$)}: We systematically vary the number of latent features and evaluate the resulting interpretability and reconstruction quality.

\item \textbf{Sparsity constraint (top-k)}: We implement top-k sparsity, keeping only the k most active features at each time point, and study how k affects feature interpretability.

\item \textbf{Scale weights ($\alpha_s$)}: The relative importance of different temporal scales is optimized based on downstream task performance.
\end{itemize}

\subsection{Model Evaluation}

\subsubsection{Reconstruction Quality}

We assess reconstruction quality using multiple metrics:
\begin{itemize}
\item Mean squared error (MSE) between original and reconstructed activity
\item Pearson correlation between original and reconstructed neural trajectories
\item Preservation of trial-to-trial variability structure
\end{itemize}

\subsubsection{Feature Interpretability}

Interpretability is evaluated through:
\begin{itemize}
\item Visual inspection of learned features and their temporal dynamics
\item Correlation with known behavioral and stimulus variables
\item Consistency of features across experimental sessions
\item Biological plausibility of discovered patterns
\end{itemize}

\subsubsection{Downstream Task Performance}

We validate the utility of discovered features through:
\begin{itemize}
\item Decoding of behavioral variables (movement direction, choice, etc.)
\item Stimulus decoding (visual scenes, auditory features, etc.)
\item Cross-session and cross-animal generalization
\end{itemize}

\subsection{Dashboard for Feature Exploration}

To facilitate interpretation of discovered features, we develop an interactive dashboard that allows researchers to:
\begin{itemize}
\item Visualize feature activation patterns across time and trials
\item Correlate features with behavioral and stimulus variables
\item Compare features across different model configurations
\item Export features for further analysis
\end{itemize}

The dashboard integrates with common neuroscience analysis workflows and provides both statistical summaries and detailed visualizations of individual features.
