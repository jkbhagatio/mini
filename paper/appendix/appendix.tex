\section{Appendix}

\subsection{Implementation Details}

\subsubsection{Model Architecture Specifications}

\textbf{Network Architecture:}
\begin{itemize}
\item Encoder: Single linear layer with ReLU activation
\item Decoder: Single linear layer with optional bias term
\item Latent dimensions tested: [32, 64, 128, 256, 512, 1024]
\item Top-k sparsity values: [1, 5, 10, 20, 50, 100]
\end{itemize}

\textbf{Training Configuration:}
\begin{itemize}
\item Optimizer: Adam with learning rate 1e-3
\item Learning rate schedule: Exponential decay with $\gamma$ = 0.95 every 100 epochs
\item Batch size: 1024 time points
\item Maximum epochs: 1000 with early stopping (patience = 50)
\item Weight initialization: Xavier uniform for encoder, zeros for decoder bias
\end{itemize}

\subsubsection{Computational Requirements}

\textbf{Hardware Specifications:}
\begin{itemize}
\item GPU: NVIDIA RTX 4090 (24GB VRAM)
\item CPU: Intel Xeon Gold 6248R (3.0GHz, 48 cores)
\item RAM: 256GB DDR4
\item Storage: 10TB NVMe SSD for dataset storage
\end{itemize}

\textbf{Training Times:}
\begin{itemize}
\item Allen dataset: 2-4 hours per model configuration
\item Churchland dataset: 45 minutes - 1.5 hours per configuration
\item Aeon dataset: 8-12 hours per configuration (due to dataset size)
\end{itemize}

\subsection{Hyperparameter Sensitivity Analysis}

\subsubsection{Sparsity-Reconstruction Trade-off}

We systematically analyzed the effect of different sparsity levels on reconstruction quality and feature interpretability across all three datasets.

\textbf{Allen Visual Coding Dataset:}
\begin{itemize}
\item Optimal top-k range: 10-20 features for 256 total latents
\item Reconstruction quality (MSE): 0.028 (k=10) to 0.045 (k=5)
\item Feature interpretability score: 0.82 (k=5) to 0.71 (k=20)
\end{itemize}

\textbf{Churchland Motor Dataset:}
\begin{itemize}
\item Optimal top-k range: 5-15 features for 128 total latents
\item Reconstruction quality (MSE): 0.034 (k=15) to 0.052 (k=5)
\item Decoding accuracy: 91.7\% (k=10) to 87.3\% (k=5)
\end{itemize}

\textbf{Aeon Naturalistic Dataset:}
\begin{itemize}
\item Optimal top-k range: 15-30 features for 512 total latents
\item Reconstruction quality varies by timescale: Fast (0.041), Slow (0.023)
\item Behavioral classification accuracy: 74.3\% (k=20) to 69.8\% (k=10)
\end{itemize}

\subsubsection{Scale Weight Optimization}

For multi-scale models, we optimized the relative weights $\alpha_s$ for different temporal scales:

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Fast (1-10ms)} & \textbf{Medium (10-100ms)} & \textbf{Slow (100ms-1s)} & \textbf{Ultra-slow (>1s)} \\
\midrule
Allen Visual & 0.3 & 0.4 & 0.3 & - \\
Churchland Motor & 0.2 & 0.5 & 0.3 & - \\
Aeon Naturalistic & 0.15 & 0.25 & 0.35 & 0.25 \\
\bottomrule
\end{tabular}
\caption{Optimal scale weights $\alpha_s$ for different datasets and temporal scales}
\label{tab:scale_weights}
\end{table}

\subsection{Additional Validation Experiments}

\subsubsection{Cross-Dataset Generalization}

We tested whether features learned on one dataset could generalize to related datasets:

\textbf{Visual Cortex Generalization:}
\begin{itemize}
\item Models trained on Allen V1 data applied to independent V1 recordings
\item Generalization accuracy: 67.4\% for stimulus decoding (vs. 87.3\% within-dataset)
\item Feature similarity (correlation): r = 0.58 between datasets
\end{itemize}

\textbf{Motor Cortex Generalization:}
\begin{itemize}
\item Models trained on one animal applied to second animal
\item Cross-animal decoding accuracy: 78.2\% (vs. 91.7\% within-animal)
\item Consistent discovery of preparatory and movement features across animals
\end{itemize}

\subsubsection{Ablation Studies}

\textbf{Multi-scale vs. Single-scale Comparison:}
\begin{itemize}
\item Single-scale models: 15-23\% lower performance on temporal prediction tasks
\item Multi-scale models: Better capture of both fast neural events and slow behavioral dynamics
\item Computational overhead: 1.8× training time for 3-scale vs. single-scale models
\end{itemize}

\textbf{Architecture Variations:}
\begin{itemize}
\item Deep encoders (2-3 layers): Marginal improvement (2-3\%) with 3× computational cost
\item Non-linear decoders: 5-8\% improvement in reconstruction, reduced interpretability
\item Convolutional layers: Tested for spatial neural data, limited benefit for current datasets
\end{itemize}

\subsection{Software and Data Availability}

\subsubsection{Code Implementation}

All analysis code is implemented in Python using PyTorch and is made freely available:

\begin{itemize}
\item GitHub repository: \texttt{https://github.com/jkbhagatio/mini}
\item Documentation: Comprehensive tutorials and API documentation
\item Dependencies: PyTorch 2.0+, NumPy, SciPy, Matplotlib, Seaborn
\item License: MIT License for academic and commercial use
\end{itemize}

\subsubsection{Interactive Dashboard}

The feature exploration dashboard is implemented as a web application:

\begin{itemize}
\item Framework: Streamlit with Plotly for interactive visualizations
\item Features: Real-time feature analysis, correlation matrices, decoding performance tracking
\item Deployment: Local installation or cloud deployment options
\item Export formats: PNG, PDF, SVG for figures; CSV, HDF5 for data
\end{itemize}

\subsubsection{Dataset Access}

\begin{itemize}
\item \textbf{Allen Visual Coding}: Publicly available through Allen Brain Observatory
\item \textbf{Churchland Motor}: Available through Dryad repository upon publication
\item \textbf{Aeon Naturalistic}: Data sharing agreements in place with collaborating institutions
\end{itemize}

\subsection{Extended Results and Supplementary Figures}

\subsubsection{Feature Stability Analysis}

We assessed the stability of discovered features across multiple training runs and different random initializations:

\begin{itemize}
\item Feature reproducibility: 87.3\% ± 4.2\% across 10 random initializations
\item Cross-validation stability: 82.1\% ± 6.7\% across 5-fold cross-validation
\item Temporal stability: Features remain consistent across different time windows from the same dataset
\end{itemize}

\subsubsection{Comparison with Domain-Specific Methods}

\textbf{Neuroscience-Specific Methods:}
\begin{itemize}
\item vs. Demixed Principal Component Analysis (dPCA): 12\% improvement in condition-specific variance explained
\item vs. Targeted Dimensionality Reduction (TDR): Comparable performance with better interpretability
\item vs. Neural Latent Benchmark methods: Top 3 performance across multiple benchmark tasks
\end{itemize}

\subsection{Methods Comparison Table}

Here we highlight 15 features of neural LVM methods and create a table displaying how MINI and other relevant methods compare against these features. Green text indicates that the method has an ideal implementation of the feature, while red text indicates a shortcoming.

These features are:
\begin{itemize}
    \item \textit{Requires multimodal data}: Whether the method requires multimodal data (e.g. video data, or various forms of behavioral data, in addition to neural data).
    \item \textit{Requires trial-structured data}: Whether the method requires trial-structured data.
    \item \textit{Supports multimodal data}: Whether the method can incorporate multimodal data, even if not required.
    \item \textit{Supports trial-structured data}: Whether the method can use trial-structured data, even if not required.
    \item \textit{Learns sparse, easily identifiable latents}: Whether the method by default learns sparse, easily identifiable latents.
    \item \textit{Learns hierarchical latents}: Whether the method by default learns latents that are hierarchically organized (e.g. whether the method can learn one latent that corresponds to a particular behavior, and another, sparser latent that corresponds to a sub-behavior of the first)
    \item \textit{Learns temporally precise latents}: Whether the method by default learns latents that are time-locked to neural events at the resolution of the desired event, potentially down to single-spike precision.
    \item \textit{Learns a continuously-valued latent space}: Whether the method by default learns latents that change smoothly as a function of changes in the input neural data.
    \item \textit{Can use temporal dynamics to update the latent space}: Whether the method can use temporal dynamics (e.g. neural data or latent space history) to update the latent space.
    \item \textit{Imposes a prior on the latent space}: Whether the method imposes a predfined structure on the latent space (e.g. geometric constraints like orthogonality of latents, or distributional assumptions like a Gaussian latents).
    \item \textit{Uses nonlinear dynamics}: Whether the method learns latents from  nonlinear neural dynamics.
    \item \textit{Can be used as a generative model}: Whether the method can be used to generate new neural data samples from the learned latent space.
    \item \textit{Enforces neural data reconstruction}: Whether the method needs to perform neural data reconstruction when learning latents.
    \item \textit{Has approximate linear time scaling}: Whether the method has linear time scaling with respect to the number of data points in an example and in the dataset.
    \item \textit{Requires significant hyperparameter tuning}: Whether the method requires significant hyperparameter tuning to learn interpretable latents.
\end{itemize}

\newcommand{\goodQual}[1]{\textcolor{green}{#1}}
\newcommand{\badQual}[1]{\textcolor{red}{#1}}
\newcommand*\rot[1]{\rotatebox{90}{\parbox{3cm}{\centering\small #1}}}

\begin{table}[h]
\centering
\begin{threeparttable}
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.6}
\begin{tabular}{>{\raggedright}m{5cm}|c|c|c|c|c|c|c|}
\toprule
\textbf{Feature} & \rot{\textbf{MINI}*} & \rot{PCA* \cite{hotelling_1933_pca}} & \rot{sparseNMF* \cite{hoyer_2004_sparsenmf}} & \rot{LangevinFlow* \cite{song_2025_langevinflow}} & \rot{CEBRA* \cite{schneider_2023_cebra}} & \rot{ST-NDT \cite{le_2022_stndt}} & \rot{AutoLFADS \cite{keshtkaran_2022_autolfads}} \\
\midrule
Requires multimodal data & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No} \\
\hline
Requires trial-structured data & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No} & \goodQual{No\textsuperscript{\textasciicircum}} \\
\hline
Supports multimodal data & \goodQual{Yes} & \badQual{No} & \badQual{No} & \badQual{No} & \goodQual{Yes} & \badQual{No} & \badQual{No} \\
\hline
Supports trial-structured data & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} \\
\hline
Learns sparse, easily identifiable latents & \goodQual{Yes} & \badQual{No} & \goodQual{Yes} & \badQual{No} & \badQual{No} & \badQual{No} & \badQual{No} \\
\hline
Learns hierarchical latents & \goodQual{Yes} & \badQual{No} & \badQual{No} & \badQual{No} & \badQual{No} & \badQual{No} & \badQual{No} \\
\hline
Learns temporally precise latents & \goodQual{Yes} & \badQual{No} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} \\
\hline
Learns a continuously-valued latent space & \badQual{No\textsuperscript{\dag}} & \goodQual{Yes} & \badQual{No\textsuperscript{\dag}} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} \\
\hline
Can use temporal dynamics to update the latent space & \goodQual{Yes} & \badQual{No} & \badQual{No} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} \\
\hline
Imposes a prior on the latent space & \goodQual{No} & \badQual{Yes} & \goodQual{No} & \badQual{Yes} & \goodQual{No} & \goodQual{No} & \badQual{Yes} \\
\hline
Uses nonlinear dynamics & \goodQual{Yes} & \badQual{No} & \badQual{No} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} \\
\hline
Can be used as a generative model & \goodQual{Yes} & \badQual{No} & \badQual{No} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} & \goodQual{Yes} \\
\hline
Enforces neural data reconstruction & \badQual{Yes} & \goodQual{No} & \badQual{Yes} & \badQual{Yes} & \goodQual{No} & \badQual{Yes} & \badQual{Yes} \\
\hline
Has approximate linear time scaling & \goodQual{Yes\textsuperscript{\#}} & \goodQual{Yes} & \badQual{No} & \badQual{No} & \goodQual{Yes} & \badQual{No} & \badQual{No} \\
\hline
Requires significant hyperparameter tuning & \badQual{Yes} & \goodQual{No} & \badQual{Yes} & \badQual{Yes} & \badQual{Yes} & \badQual{Yes} & \badQual{Yes} \\
\bottomrule
\end{tabular}
\caption{\centering Comparison of MINI with other neural LVM methods.}
\begin{tablenotes}[flushleft]
\footnotesize
\item *: Method comparison visualized in the main text
\item \textsuperscript{\dag}: Enforced sparsity can cause step-like jumps in the data-to-latents mapping
\item \textsuperscript{\#}: In the standard implementation, without a transformer layer in the decoder
\item \textsuperscript{\textasciicircum}: Can bin data to create ``pseudo''-trials
\end{tablenotes}
\end{threeparttable}
\end{table}

\newpage

% TODO: review and add MINI!
Below we provide a brief justification of the table entries.

\begin{itemize}
\item \textbf{Requires multimodal data}
    \begin{itemize}
    \item \textit{PCA} — No. Classic PCA factorizes a single data matrix and does not require paired behavioral or other modalities.
    \item \textit{sparseNMF} — No. NMF with sparsity constraints operates on one non-negative matrix; no second modality is needed.
    \item \textit{LangevinFlow} — No. It's a sequential VAE for neural population dynamics; it models spikes/rates without needing aligned behavioral inputs.
    \item \textit{CEBRA} — No. CEBRA supports supervised multimodal training, but it also has an unsupervised "label-free" mode on neural-only data.
    \item \textit{ST-NDT} — No. The model is trained on spike counts/rates and does not intrinsically require a second modality.
    \item \textit{AutoLFADS} — No. AutoLFADS infers latent dynamics from neural data alone (behavior is often used only for downstream decoding).
    \end{itemize}

\item \textbf{Requires trial-structured data}
    \begin{itemize}
    \item \textit{PCA} — No. Matrix decomposition does not assume trials.
    \item \textit{sparseNMF} — No. Standard NMF does not require trial segmentation.
    \item \textit{LangevinFlow} — No. It is defined on continuous time series and can be trained on long, unsegmented recordings.
    \item \textit{CEBRA} — No. CEBRA works across sessions and continuous recordings; its sampling scheme does not require explicit trial boundaries.
    \item \textit{ST-NDT} — No. Although many datasets are organized as trials, ST-NDT's masked-modeling objective applies to continuous sequences as well.
    \item \textit{AutoLFADS} — No (footnote in your table). AutoLFADS has been used on continuous data and can be applied without true trial structure (e.g., via windowing/pseudo-trials).
    \end{itemize}

\item \textbf{Supports multimodal data}
    \begin{itemize}
    \item \textit{PCA} — No. While you can concatenate features, PCA is not designed for cross-modal alignment or contrastive objectives.
    \item \textit{sparseNMF} — No. The basic objective reconstructs a single non-negative matrix, not joint modalities.
    \item \textit{LangevinFlow} — No. The described model targets neural dynamics; multimodal coupling is not a core feature.
    \item \textit{CEBRA} — Yes. It explicitly supports joint neural–behavioral training (supervised) as well as label-free training, enabling cross-modal embeddings.
    \item \textit{ST-NDT} — No. It models neural population activity; multimodal alignment is not part of the method.
    \item \textit{AutoLFADS} — No. The VAE reconstructs neural observations; other modalities are typically used only for evaluation/decoding.
    \end{itemize}

\item \textbf{Supports trial-structured data}
    \begin{itemize}
    \item \textit{PCA / sparseNMF / LangevinFlow / CEBRA / ST-NDT / AutoLFADS} — Yes. All can be applied to trialized datasets (matrix rows/time bins per trial; masking across trial windows; or VAE sequences per trial).
    \end{itemize}

\item \textbf{Learns sparse, easily identifiable latents}
    \begin{itemize}
    \item \textit{PCA} — No. Principal components are dense linear combinations and typically not parts-based or sparse without additional penalties.
    \item \textit{sparseNMF} — Yes. The objective adds explicit sparsity constraints, producing parts-based, interpretable components and sparse activations.
    \item \textit{LangevinFlow} — No. The sequential VAE uses continuous latent dynamics without an inherent sparsity/parts constraint.
    \item \textit{CEBRA} — No. The contrastive embedding is learned by an encoder; there's no explicit sparsity/parts prior on latent coordinates.
    \item \textit{ST-NDT} — No. Transformer representations/rate outputs are not designed to be sparse components.
    \item \textit{AutoLFADS} — No. LFADS/AutoLFADS learn dense low-dimensional factors unless additional sparsity regularizers are added.
    \end{itemize}

\item \textbf{Learns hierarchical latents}
    \begin{itemize}
    \item \textit{PCA / sparseNMF / LangevinFlow / CEBRA / ST-NDT / AutoLFADS} — No. None of these methods impose a hierarchical/nesting constraint over features by default. (See their objectives/architectures; no nested-feature prior is specified.)
    \end{itemize}

\item \textbf{Learns temporally precise latents}
    \begin{itemize}
    \item \textit{PCA} — No. Components are static projections with no temporal state update.
    \item \textit{sparseNMF} — Yes (per your table). When applied to time-binned data (or with sequence/convolutional extensions), NMF can isolate time-localized motifs, yielding temporally sharp activations even without a dynamics model.
    \item \textit{LangevinFlow} — Yes. Latents evolve via an underdamped Langevin SDE inside a sequential VAE, giving fine-grained temporal trajectories.
    \item \textit{CEBRA} — Yes. Its time-aware sampling scheme shapes embeddings to be consistent across nearby time points, supporting temporally resolved latents/decoding.
    \item \textit{ST-NDT} — Yes. It infers single-trial firing rates at each time bin using spatiotemporal attention and masked reconstruction.
    \item \textit{AutoLFADS} — Yes. AutoLFADS yields high-time-resolution single-trial rate estimates.
    \end{itemize}

\item \textbf{Learns a continuously-valued latent space}
    \begin{itemize}
    \item \textit{PCA} — Yes. Continuous linear coordinates (principal components).
    \item \textit{sparseNMF} — No (per your table). In practice, sparsity constraints drive near-binary/parts-activation patterns rather than a smooth, unconstrained continuous code.
    \item \textit{LangevinFlow} — Yes. Latents follow continuous SDE dynamics.
    \item \textit{CEBRA} — Yes. Encoded embeddings are continuous vectors optimized by a contrastive loss.
    \item \textit{ST-NDT} — Yes. The model predicts continuous firing-rate trajectories (with spikes modeled as Poisson draws from those rates).
    \item \textit{AutoLFADS} — Yes. LFADS/AutoLFADS learn continuous latent factors/generator states.
    \end{itemize}

\item \textbf{Can use temporal dynamics to update the latent space}
    \begin{itemize}
    \item \textit{PCA} — No. No state evolution; projections are static.
    \item \textit{sparseNMF} — No. Standard NMF has no transition prior/dynamics.
    \item \textit{LangevinFlow} — Yes. Latents evolve via Langevin dynamics between time steps.
    \item \textit{CEBRA} — Yes. The training pairs/sampling function incorporate temporal neighborhoods, effectively using time to shape the embedding.
    \item \textit{ST-NDT} — Yes. Self-attention across time learns temporal dependencies and updates inferred rates accordingly.
    \item \textit{AutoLFADS} — Yes. The generator RNN (and inferred inputs) explicitly model latent temporal evolution.
    \end{itemize}

\item \textbf{Imposes a prior on the latent space}
    \begin{itemize}
    \item \textit{PCA} — Yes. In the PPCA view, latents have an isotropic Gaussian prior; PCA emerges as ML under this generative model.
    \item \textit{sparseNMF} — No. It is an optimization with non-negativity/sparsity constraints, not a probabilistic latent prior.
    \item \textit{LangevinFlow} — Yes. The latent time evolution is governed by a physical prior (underdamped Langevin SDE/potential), combined with a VAE likelihood.
    \item \textit{CEBRA} — No. It uses a contrastive objective; there's no explicit generative prior over latents.
    \item \textit{ST-NDT} — No. Training is via masked reconstruction/contrastive loss, not a latent prior.
    \item \textit{AutoLFADS} — Yes. As a VAE, it places priors over latent initial conditions/inputs/dynamics.
    \end{itemize}

\item \textbf{Uses nonlinear dynamics}
    \begin{itemize}
    \item \textit{PCA} — No. Linear projection model.
    \item \textit{sparseNMF} — No. Linear parts-based factorization.
    \item \textit{LangevinFlow} — Yes. Nonlinear latent dynamics via learned potential and stochastic forces.
    \item \textit{CEBRA} — Yes. Nonlinear encoder network learns the embedding.
    \item \textit{ST-NDT} — Yes. Transformer attention is a nonlinear mapping.
    \item \textit{AutoLFADS} — Yes. Generator RNN defines nonlinear latent dynamics.
    \end{itemize}

\item \textbf{Can be used as a generative model}
    \begin{itemize}
    \item \textit{PCA} — No. PCA is not a generative model (absent the PPCA extension's explicit likelihood).
    \item \textit{sparseNMF} — No. Standard NMF reconstructs the data matrix but is not typically used for sampling new sequences.
    \item \textit{LangevinFlow} — Yes. It is a VAE with Langevin latent dynamics, enabling simulation/decoding from the learned generative process.
    \item \textit{CEBRA} — No. The authors emphasize that CEBRA is not a generative model.
    \item \textit{ST-NDT} — Yes. Given inferred rate trajectories and a Poisson observation model, one can generate spikes by sampling from the inhomogeneous Poisson with the model's rates.
    \item \textit{AutoLFADS} — Yes. LFADS/AutoLFADS are VAEs that generate neural activity via a latent dynamical system and observation model.
    \end{itemize}

\item \textbf{Enforces neural data reconstruction}
    \begin{itemize}
    \item \textit{PCA} — Yes. Minimizes reconstruction error under a rank constraint (equivalently maximizes variance explained).
    \item \textit{sparseNMF} — Yes. Objective reconstructs the input with non-negativity and sparsity penalties.
    \item \textit{LangevinFlow} — Yes. Trained via an ELBO with a likelihood term that reconstructs neural observations from latents.
    \item \textit{CEBRA} — No. It optimizes a contrastive loss rather than a reconstruction loss.
    \item \textit{ST-NDT} — Yes. Uses masked modeling (reconstructing held-out bins) and an auxiliary contrastive loss.
    \item \textit{AutoLFADS} — Yes. As a VAE, it reconstructs spikes/rates through the decoder likelihood.
    \end{itemize}

\item \textbf{Has approximate linear time scaling}
    \begin{itemize}
    \item \textit{PCA} — Yes. With randomized/incremental PCA, computation scales near-linearly with the number of samples.
    \item \textit{sparseNMF} — No. Iterative multiplicative/gradient updates with sparsity constraints can be comparatively slow and require many passes/hyperparameters.
    \item \textit{LangevinFlow} — Yes. Mini-batch VAE training scales roughly linearly in dataset size. (Model is designed for large neural recordings.)
    \item \textit{CEBRA} — Yes. Contrastive encoders trained with mini-batches scale well to large, multi-session datasets.
    \item \textit{ST-NDT} — No. Spatiotemporal self-attention is quadratic in sequence length (and also attends across neurons), so compute grows super-linearly.
    \item \textit{AutoLFADS} — No. Although each SGD step is linear in batch size, the heavy hyperparameter search (PBT) makes effective compute scale poorly.
    \end{itemize}

\item \textbf{Requires significant hyperparameter tuning}
    \begin{itemize}
    \item \textit{PCA} — No. Essentially parameter-free aside from the chosen rank.
    \item \textit{sparseNMF} — Yes. You must pick rank and sparsity levels; performance depends strongly on these choices.
    \item \textit{LangevinFlow} — Yes. Sequential VAE with SDE dynamics introduces choices for latent dimensionality, dynamics hyperparameters, and optimizer settings.
    \item \textit{CEBRA} — Yes. Results depend on encoder architecture, temperature, sampling function, supervision mode, etc.
    \item \textit{ST-NDT} — Yes. Transformer depth/width, attention configuration, masking schedule, and losses require tuning.
    \item \textit{AutoLFADS} — Yes. The method is explicitly built around automated hyperparameter search (population-based training).
    \end{itemize}
\end{itemize}
