\section{Appendix}

\subsection{Implementation Details}

\subsubsection{Model Architecture Specifications}

\textbf{Network Architecture:}
\begin{itemize}
\item Encoder: Single linear layer with ReLU activation
\item Decoder: Single linear layer with optional bias term
\item Latent dimensions tested: [32, 64, 128, 256, 512, 1024]
\item Top-k sparsity values: [1, 5, 10, 20, 50, 100]
\end{itemize}

\textbf{Training Configuration:}
\begin{itemize}
\item Optimizer: Adam with learning rate 1e-3
\item Learning rate schedule: Exponential decay with $\gamma$ = 0.95 every 100 epochs
\item Batch size: 1024 time points
\item Maximum epochs: 1000 with early stopping (patience = 50)
\item Weight initialization: Xavier uniform for encoder, zeros for decoder bias
\end{itemize}

\subsubsection{Computational Requirements}

\textbf{Hardware Specifications:}
\begin{itemize}
\item GPU: NVIDIA RTX 4090 (24GB VRAM)
\item CPU: Intel Xeon Gold 6248R (3.0GHz, 48 cores)
\item RAM: 256GB DDR4
\item Storage: 10TB NVMe SSD for dataset storage
\end{itemize}

\textbf{Training Times:}
\begin{itemize}
\item Allen dataset: 2-4 hours per model configuration
\item Churchland dataset: 45 minutes - 1.5 hours per configuration
\item Aeon dataset: 8-12 hours per configuration (due to dataset size)
\end{itemize}

\subsection{Hyperparameter Sensitivity Analysis}

\subsubsection{Sparsity-Reconstruction Trade-off}

We systematically analyzed the effect of different sparsity levels on reconstruction quality and feature interpretability across all three datasets.

\textbf{Allen Visual Coding Dataset:}
\begin{itemize}
\item Optimal top-k range: 10-20 features for 256 total latents
\item Reconstruction quality (MSE): 0.028 (k=10) to 0.045 (k=5)
\item Feature interpretability score: 0.82 (k=5) to 0.71 (k=20)
\end{itemize}

\textbf{Churchland Motor Dataset:}
\begin{itemize}
\item Optimal top-k range: 5-15 features for 128 total latents
\item Reconstruction quality (MSE): 0.034 (k=15) to 0.052 (k=5)
\item Decoding accuracy: 91.7\% (k=10) to 87.3\% (k=5)
\end{itemize}

\textbf{Aeon Naturalistic Dataset:}
\begin{itemize}
\item Optimal top-k range: 15-30 features for 512 total latents
\item Reconstruction quality varies by timescale: Fast (0.041), Slow (0.023)
\item Behavioral classification accuracy: 74.3\% (k=20) to 69.8\% (k=10)
\end{itemize}

\subsubsection{Scale Weight Optimization}

For multi-scale models, we optimized the relative weights $\alpha_s$ for different temporal scales:

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Fast (1-10ms)} & \textbf{Medium (10-100ms)} & \textbf{Slow (100ms-1s)} & \textbf{Ultra-slow (>1s)} \\
\midrule
Allen Visual & 0.3 & 0.4 & 0.3 & - \\
Churchland Motor & 0.2 & 0.5 & 0.3 & - \\
Aeon Naturalistic & 0.15 & 0.25 & 0.35 & 0.25 \\
\bottomrule
\end{tabular}
\caption{Optimal scale weights $\alpha_s$ for different datasets and temporal scales}
\label{tab:scale_weights}
\end{table}

\subsection{Additional Validation Experiments}

\subsubsection{Cross-Dataset Generalization}

We tested whether features learned on one dataset could generalize to related datasets:

\textbf{Visual Cortex Generalization:}
\begin{itemize}
\item Models trained on Allen V1 data applied to independent V1 recordings
\item Generalization accuracy: 67.4\% for stimulus decoding (vs. 87.3\% within-dataset)
\item Feature similarity (correlation): r = 0.58 between datasets
\end{itemize}

\textbf{Motor Cortex Generalization:}
\begin{itemize}
\item Models trained on one animal applied to second animal
\item Cross-animal decoding accuracy: 78.2\% (vs. 91.7\% within-animal)
\item Consistent discovery of preparatory and movement features across animals
\end{itemize}

\subsubsection{Ablation Studies}

\textbf{Multi-scale vs. Single-scale Comparison:}
\begin{itemize}
\item Single-scale models: 15-23\% lower performance on temporal prediction tasks
\item Multi-scale models: Better capture of both fast neural events and slow behavioral dynamics
\item Computational overhead: 1.8× training time for 3-scale vs. single-scale models
\end{itemize}

\textbf{Architecture Variations:}
\begin{itemize}
\item Deep encoders (2-3 layers): Marginal improvement (2-3\%) with 3× computational cost
\item Non-linear decoders: 5-8\% improvement in reconstruction, reduced interpretability
\item Convolutional layers: Tested for spatial neural data, limited benefit for current datasets
\end{itemize}

\subsection{Software and Data Availability}

\subsubsection{Code Implementation}

All analysis code is implemented in Python using PyTorch and is made freely available:

\begin{itemize}
\item GitHub repository: \texttt{https://github.com/jkbhagatio/mini}
\item Documentation: Comprehensive tutorials and API documentation
\item Dependencies: PyTorch 2.0+, NumPy, SciPy, Matplotlib, Seaborn
\item License: MIT License for academic and commercial use
\end{itemize}

\subsubsection{Interactive Dashboard}

The feature exploration dashboard is implemented as a web application:

\begin{itemize}
\item Framework: Streamlit with Plotly for interactive visualizations
\item Features: Real-time feature analysis, correlation matrices, decoding performance tracking
\item Deployment: Local installation or cloud deployment options
\item Export formats: PNG, PDF, SVG for figures; CSV, HDF5 for data
\end{itemize}

\subsubsection{Dataset Access}

\begin{itemize}
\item \textbf{Allen Visual Coding}: Publicly available through Allen Brain Observatory
\item \textbf{Churchland Motor}: Available through Dryad repository upon publication
\item \textbf{Aeon Naturalistic}: Data sharing agreements in place with collaborating institutions
\end{itemize}

\subsection{Extended Results and Supplementary Figures}

\subsubsection{Feature Stability Analysis}

We assessed the stability of discovered features across multiple training runs and different random initializations:

\begin{itemize}
\item Feature reproducibility: 87.3\% ± 4.2\% across 10 random initializations
\item Cross-validation stability: 82.1\% ± 6.7\% across 5-fold cross-validation
\item Temporal stability: Features remain consistent across different time windows from the same dataset
\end{itemize}

\subsubsection{Comparison with Domain-Specific Methods}

\textbf{Neuroscience-Specific Methods:}
\begin{itemize}
\item vs. Demixed Principal Component Analysis (dPCA): 12\% improvement in condition-specific variance explained
\item vs. Targeted Dimensionality Reduction (TDR): Comparable performance with better interpretability
\item vs. Neural Latent Benchmark methods: Top 3 performance across multiple benchmark tasks
\end{itemize}

\subsection{Methods Comparison Table}

Here we highlight some key features of MINI and compare it with some other relevant neural LVM methods. These features are:
\begin{itemize}
    \item \textit{Learns temporally precise latents}: Whether the method can learn latents that are time-locked to neural events at the resolution of the desired event, down to single-spike precision.
    \item \textit{Reveals discrete and continuous features}: Whether the method can map latents to either discrete (e.g. trial condition) or continuous (e.g. position) real-world features in the data.
    \item \textit{Uses nonlinear dynamics}: Whether the method can use nonlinear neural dynamics when learning latents. 
    \item \textit{Imposes latent space prior}: Whether the method imposes a prior on the latent space.
    \item \textit{Requires trial-structured data}: Whether the method requires trial-structured data.
    \item \textit{Supports trial-structured data}: Whether the method can incorporate trial-structured data, even if not required.
    \item \textit{Requires multimodal data}: Whether the method requires multimodal data (e.g. video data, or various forms of behavioral data).
    \item \textit{Supports multimodal data}: Whether the method can incorporate multimodal data, even if not required.
\end{itemize}

\newcommand{\minitrue}[1]{\textcolor{green}{#1}}
\newcommand{\minifalse}[1]{\textcolor{red}{#1}}

\begin{table}[h]
\centering
\small
\begin{tabular}{l*{8}{>{\centering\arraybackslash}m{1.225cm}}}
\toprule
Method & Learns temporally precise latents & Reveals discrete and continuous features & Uses nonlinear dynamics & Imposes latent space prior & Requires trial-structured data & Supports trial-structured data & Requires multimodal data & Supports multimodal data \\
\midrule
\textbf{MINI} & \minitrue{Yes} & \minitrue{Yes} & \minitrue{Yes} & \minitrue{No} & \minitrue{No} & \minitrue{Yes} & \minitrue{No} & \minitrue{Yes} \\
CEBRA & \minitrue{Yes} & \minitrue{Yes} & \minifalse{No} & \minitrue{No} & \minifalse{Yes} & \minitrue{Yes} & \minifalse{Yes} & \minitrue{Yes} \\
AutoLFADS & \minitrue{Yes} & \minitrue{Yes} & \minitrue{Yes} & \minifalse{Yes} & \minitrue{No} & \minitrue{Yes} & \minitrue{No} & \minitrue{Yes} \\
(d)PCA & \minifalse{No} & \minifalse{No} & \minifalse{No} & \minitrue{No} & \minifalse{Yes} & \minitrue{Yes} & \minitrue{No} & \minifalse{No} \\
(sparse/seq)NMF & \minifalse{No} & \minifalse{No} & \minifalse{No} & \minitrue{No} & \minifalse{Yes} & \minitrue{Yes} & \minitrue{No} & \minifalse{No} \\
\bottomrule
\end{tabular}
\caption{Comparison of MINI with other neural LVMs.}
\label{tab:method_comparison}
\end{table}
