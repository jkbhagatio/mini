\subsection{Model Architecture}

\subsubsection{Sparse Autoencoders (SAEs)}

We begin with the standard Sparse Autoencoder formulation. Given neural data $\mathbf{X} \in \mathbb{R}^{T \times N}$ where $T$ is the number of time points and $N$ is the number of neurons, an SAE learns an encoder function $f_{\text{enc}}: \mathbb{R}^N \rightarrow \mathbb{R}^D$ and decoder function $f_{\text{dec}}: \mathbb{R}^D \rightarrow \mathbb{R}^N$ where $D$ is the latent dimension.

The encoder projects input neural activity to a sparse latent representation:
\begin{equation}
\mathbf{z}_t = f_{\text{enc}}(\mathbf{x}_t) = \text{ReLU}(\mathbf{W}_{\text{enc}} \mathbf{x}_t + \mathbf{b}_{\text{enc}})
\end{equation}

The decoder reconstructs the original activity:
\begin{equation}
\hat{\mathbf{x}}_t = f_{\text{dec}}(\mathbf{z}_t) = \mathbf{W}_{\text{dec}} \mathbf{z}_t + \mathbf{b}_{\text{dec}}
\end{equation}

The total loss function combines reconstruction error with sparsity regularization:
\begin{equation}
\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \|\mathbf{x}_t - \hat{\mathbf{x}}_t\|_2^2 + \lambda \sum_{t=1}^{T} \|\mathbf{z}_t\|_1
\end{equation}

where $\lambda$ controls the sparsity-reconstruction trade-off.

\subsubsection{Multi-Scale Sparse Autoencoders (MSAEs)}

To capture features across multiple temporal scales, we extend the SAE architecture to process neural data at different temporal resolutions simultaneously. Our MSAE consists of multiple parallel encoding pathways, each operating at a different temporal scale.

For temporal scales $s \in \{s_1, s_2, \ldots, s_K\}$, we create downsampled versions of the input data:
\begin{equation}
\mathbf{X}^{(s)} = \text{Downsample}(\mathbf{X}, s)
\end{equation}

Each scale has its own encoder-decoder pair:
\begin{align}
\mathbf{z}_t^{(s)} &= f_{\text{enc}}^{(s)}(\mathbf{x}_t^{(s)}) \\
\hat{\mathbf{x}}_t^{(s)} &= f_{\text{dec}}^{(s)}(\mathbf{z}_t^{(s)})
\end{align}

The multi-scale loss combines reconstruction errors across all scales:
\begin{equation}
\mathcal{L}_{\text{MSAE}} = \sum_{s} \alpha_s \left[ \frac{1}{T_s} \sum_{t=1}^{T_s} \|\mathbf{x}_t^{(s)} - \hat{\mathbf{x}}_t^{(s)}\|_2^2 + \lambda_s \sum_{t=1}^{T_s} \|\mathbf{z}_t^{(s)}\|_1 \right]
\end{equation}

where $\alpha_s$ weights the contribution of each scale and $T_s$ is the number of time points at scale $s$.

\subsubsection{Hyperparameter Selection}

The choice of hyperparameters significantly affects the interpretability-reconstruction trade-off:

\begin{itemize}
\item \textbf{Latent dimension ($d_{\text{sae}}$)}: We systematically vary the number of latent features and evaluate the resulting interpretability and reconstruction quality. Smaller dimensions promote interpretability but may limit representational capacity, while larger dimensions provide better reconstruction at the cost of feature clarity.

\item \textbf{Sparsity constraint (top-k)}: We implement top-k sparsity, keeping only the k most active features at each time point. This ensures a controlled level of sparsity while allowing for flexible feature activation patterns across different neural states.

\item \textbf{Scale weights ($\alpha_s$)}: The relative importance of different temporal scales is optimized based on downstream task performance and biological relevance of discovered features.
\end{itemize}
